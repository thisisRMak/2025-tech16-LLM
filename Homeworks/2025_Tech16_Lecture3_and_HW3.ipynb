{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO07/453SNFwa9lnlH2e0Ju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thisisRMak/2025-tech16-LLM/blob/main/Tech16_Lecture_3_class_code_and_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMhTGwoYYG1D",
        "outputId": "64d19653-1cfb-4721-e25e-d9839689b7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.19)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.19 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.19)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.7)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.20)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.5)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.61.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.11.12)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.12)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.8->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index\n",
        "%pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "client = OpenAI(api_key=open_ai_key)"
      ],
      "metadata": {
        "id": "AqGcZPkTYRbF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://privatewealth.goldmansachs.com/outlook/2025-isg-outlook.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn-GEEQiYsLH",
        "outputId": "86a527e5-e080-4a68-8202-e34802657bd2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-20 01:36:43--  https://privatewealth.goldmansachs.com/outlook/2025-isg-outlook.pdf\n",
            "Resolving privatewealth.goldmansachs.com (privatewealth.goldmansachs.com)... 23.192.230.149, 23.192.230.134\n",
            "Connecting to privatewealth.goldmansachs.com (privatewealth.goldmansachs.com)|23.192.230.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/pdf]\n",
            "Saving to: ‘2025-isg-outlook.pdf.3’\n",
            "\n",
            "2025-isg-outlook.pd     [  <=>               ]   6.88M  22.6MB/s    in 0.3s    \n",
            "\n",
            "2025-02-20 01:36:44 (22.6 MB/s) - ‘2025-isg-outlook.pdf.3’ saved [7209986]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = open_ai_key"
      ],
      "metadata": {
        "id": "uCkp7ibBY_8k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
      ],
      "metadata": {
        "id": "wPbGA8eLZk_1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('./').load_data('2025-isg-outlook.pdf')\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcrPudNuZwZU",
        "outputId": "cee92540-d380-4244-80c6-05bb6c44251b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|██████████| 1/1 [00:08<00:00,  8.08s/file]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='eca796fa-787b-42c3-ae7f-0f01eb7f63a5', embedding=None, metadata={'page_label': 'i', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Outlook\\nKeep On Truckin’\\nInvestment Strategy Group | January 2025\\nWealth Management\\nCan’t nothin’ hold me back, nothin’\\nI’ll keep right on, right on truckin’\\n – Lyrics from “Keep On Truckin’” by Eddie Kendricks \\n#1 on Billboard Hot 100 and R&B Singles Chart (1973)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='adb52bf1-b495-4f00-ac93-9970b60951f2', embedding=None, metadata={'page_label': 'ii', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='This material represents the views of the Investment Strategy Group in the Wealth Management Business of Goldman Sachs. \\nIt is not a product of Goldman Sachs Global Investment Research. The views and opinions expressed herein may differ from \\nthose expressed by other groups of Goldman Sachs.\\nSharmin Mossavar-Rahmani  \\nChief Investment Officer  \\nInvestment Strategy Group  \\nGoldman Sachs \\nBrett Nelson  \\nHead of Tactical Asset Allocation \\nInvestment Strategy Group  \\nGoldman Sachs \\n \\nThe co-authors give special thanks to: \\nMatheus Dibo  \\nManaging Director\\nMatthew Weir  \\nManaging Director\\nNicola Gifford  \\nVice President\\nOussama Fatri \\nManaging Director\\nHarm Zebregs  \\nManaging Director\\nKelly Han \\nVice President\\nAdditional contributors from the  \\nInvestment Strategy Group:\\nVenkatesh Balasubramanian  \\nManaging Director\\nPeter Foley \\nVice President\\nJames Gilbert  \\nVice President\\nRob Hunter \\nVice President\\nFabian Mertes  \\nVice President\\nMichael Murdoch  \\nVice President\\nJan Niessen \\nVice President\\nEziz Jumayev \\nAssociate\\nGeorge Milton  \\nAssociate\\nGrant Nelson  \\nAssociate\\nJonas Schmitten  \\nAssociate', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='953b273a-8049-46eb-abb0-28da5ef927c9', embedding=None, metadata={'page_label': '1', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1Outlook Investment Strategy Group\\n2025 OUTLOOK\\nDear Clients, \\nThe two key themes that have underpinned our investment recommendations \\nsince the trough of the global financial crisis (GFC)—US Preeminence and Stay \\nInvested—remain intact. In fact, in 2024, the gap between the US and the rest of \\nthe world across key economic and financial market metrics widened.  \\n US equities, as measured by the S&P 500, outperformed all other major equity \\nmarkets in 2024, with a total return of 25%, \\ncompared to 12% for non-US developed \\nmarket equities (as measured by the MSCI \\nEAFE index, in local currency) and 14% for \\nemerging market equities (as measured by the \\nMSCI EM index, in local currency as well). \\n For US investors, the S&P 500’s \\noutperformance was even greater, as the \\nUS dollar appreciated by 7% (as measured \\nby the US Dollar Index, or DXY). Non-\\nUS developed equities returned just 4% for \\ndollar-based investors, and emerging markets \\nreturned 8%—underperforming the US by \\n21 and 17 percentage points, respectively. \\n Overweighting US equities and staying \\ninvested have served our clients well over \\nthe past 15 years. An Investment Strategy \\nGroup (ISG) moderate risk portfolio designed \\nfor taxable investors and a similar portfolio \\ndesigned for tax-exempt investors each \\nreturned about 9% annualized or about \\n280% cumulatively between March 2009 \\nand year-end 2024.1 \\nFor Private Wealth  \\nManagement Clients\\nOutlook\\nInvestment  \\nStrategy Group  \\nJanuary 2010\\nTake Stock of America\\nWe believe that 2010 will see the continuing emergence of fast-growing economies \\nsuch as China and India, but we don’t think their success will cost the US its \\nleadership position. The underlying strength and influence of America is intact.\\nFor Private Wealth  \\nManagement Clients\\nOutlook\\nInvestment  \\nStrategy Group  \\nJanuary 2011\\nStay the Course\\nThe American Evolution: Much like George Washington crossing the Delaware \\nRiver in the winter of 1776-77, America’s structural resilience, fortitude and  \\ningenuity will carry the economy and financial markets in 2011 – and beyond.\\nOutlook\\nOur six-year investment theme endures.\\nInvestment Management Division\\nInvestment Strategy Group  |  January 2015\\nUS Preeminence\\nOutlook\\nAmerican Preeminence  \\nin a Rattled World\\nInvestment Strategy Group | January 2019\\nConsumer and Investment Management Division\\nEmma Lazarus’ Colossus\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8c7bd143-b9cc-461c-9c5e-83ab8627a64b', embedding=None, metadata={'page_label': '2', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 Goldman Sachs january 2025\\n US economic growth outpaced that of other developed economies, achieving \\nan estimated 2.8% growth in 2024, compared to a meager 0.8% in the rest of \\nthe developed economies. US GDP increased by $1.4 trillion, compared to $619 \\nbillion in the Eurozone and $937 billion in China. Some of the outperformance \\nis driven by the productivity boom in the US, where productivity (as measured by \\ngrowth in real GDP per hour worked) increased by 2.3%, compared to 0.6% in \\nthe UK, 0% in the Eurozone and a marginal decline in Japan. \\n China experienced a notable increase in productivity growth as well; however, \\nas highlighted throughout our 2025 Outlook, the impact of higher growth rates \\non China’s lower base number is dwarfed by the impact of lower growth rates \\non the US’s much higher base number. The effect of high overall GDP, GDP per \\ncapita, productivity and other economic metrics in the US, on its large base, \\nmakes it virtually impossible for other economies to catapult ahead of the US. \\n Financial market participants have recognized US preeminence. As shown in Exhibit \\n1, US equities have outperformed non-US developed markets by eight percentage \\npoints on an annualized basis and emerging market equities by nine percentage points \\nsince the trough of the GFC. For illustrative purposes, a $100 million investment in US \\nequities, compounded over nearly 16 years, would have grown to nearly $1.2 billion. \\nBy comparison, a $100 million investment in non-US developed market equities \\nwould have grown to $418 million, and \\nto $344 million if invested in emerging \\nmarket equities. A similar investment in \\nChinese equities would have grown to \\nonly $273 million. \\n Inevitably, after such a long run \\nof US equity outperformance, our \\nclients are asking questions about \\nour strategic and tactical asset \\nallocation views: \\n• US clients are asking whether \\nthey should allocate all their \\npublic equities exclusively to the \\nUS, especially since they can get \\nglobal exposure through S&P 500 \\nExhibit 1: Annualized and Cumulative Equity \\nReturns Since March 2009 \\nUS equities have significantly outperformed since the trough \\nof the GFC. \\n17\\n13\\n9 9\\n8\\n7\\n0\\n3\\n6\\n9\\n12\\n15\\n18\\nUS India Non-US\\nDeveloped\\nEurozone EM China\\nAnnualized Total Return (%)\\nCumulative:\\n1,079%\\nTotal Value:\\n$1,179M\\nCumulative:\\n584%\\nTotal Value:\\n$684M\\nCumulative:\\n304%\\nTotal Value:\\n$404M \\nCumulative:\\n318%\\nTotal Value:\\n$418M \\nCumulative:\\n244%\\nTotal Value:\\n$344M \\nCumulative:\\n173%\\nTotal Value:\\n$273M\\nTotal Value of US$100 Million Invested on Trough of GFC\\nData as of December 31, 2024.  \\nNote: All non-US equity returns are measured by MSCI indices in US dollars. \\nSource: Investment Strategy Group, Bloomberg.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5b5dc765-6392-4915-8d46-95ed496aaa20', embedding=None, metadata={'page_label': '3', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3Outlook Investment Strategy Group\\ncompanies. About 28% of these companies’ total revenues comes from outside \\nthe US, according to our colleagues in Global Investment Research.2\\n• Many of our European clients are asking whether they should underweight US \\nassets in favor of European equities, given the relative cheapness of European \\nequities. They are also asking whether they should favor Indian equities, given \\nthe expectation that India will benefit from the West’s de-risking from China. \\n• After a cumulative return of 1,079% since the trough of the GFC, some clients \\nare asking whether they should tactically underweight US equities with an \\nopportunistic allocation to cash or bonds. \\n• Given the 27% increase in the spot price of gold and the 123% increase in \\nthe price of bitcoin in 2024, clients are asking whether they should allocate a \\nportion of their portfolios to gold, bitcoin or both. \\nOur answer to all these questions is a resounding no:\\n• Our strategic overweight to US assets compared to the weights of the MSCI All \\nCountry World Index (ACWI) stood at 23 percentage points at the trough of \\nthe GFC. The outperformance of US equities narrowed that overweight to seven \\npercentage points by the end of 2024. We do not recommend a zero allocation \\nto non-US equities. However, we have implicitly increased the overweight to US \\nequities through an allocation to private assets funded out of non-US equities. \\n In our base case scenario, we do not expect US equities to meaningfully \\noutperform non-US equities over the next five years—and most definitely \\nnot by the magnitude we have seen over the last 15 years. In addition, many \\nworld-class companies in the consumer discretionary, health care, utilities \\nand materials sectors are located outside the US, and these companies are \\nattractively valued: they should not be categorically excluded from a portfolio. \\n• We do not recommend a tactical allocation to non-US equities funded out of US \\nequities, either. The valuations of non-US developed equities are at historic lows. \\nHowever, we believe that current valuations are justified based on slower trend \\neconomic growth, lower earnings per share growth, weaker demographics and \\nmore geopolitical vulnerabilities in non-US developed economies. \\n• Tactically, we do not recommend an overweight to US bonds and cash funded \\nout of US equities. We agree that US equities are expensive, and more so than \\nthey were at the end of 2023:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7ed7292c-edb6-4fcc-bd1f-da7c413e6e3e', embedding=None, metadata={'page_label': '4', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4 Goldman Sachs january 2025\\n –  An ISG metric that combines five different short- and long-term valuation \\nmetrics is firmly in the 10th decile, meaning equities have been cheaper \\nmore than 90% of the time in the post-WWII period. Still, the S&P 500 has \\nreturned 202% since entering the 10th decile in December 2016.\\n –  The S&P 500 price relative to operating earnings over the last 12 months is also \\nin the 10th decile and is 52% above its long-term median. It was 34% above \\nthe long-term median at the end of 2023, yet the S&P 500 still returned \\n25% in 2024. \\n –  The S&P 500 price relative to expected operating earnings over the next \\n12 months is in the ninth decile. The measure is 32% above its long-term \\nmedian, compared to 21% at the end of 2023. \\n –  The equity risk premium, which compares the earnings yield of the S&P 500 \\nto the yield of the 10-year Treasury, has increased to the ninth decile from \\nthe eighth decile at the end of 2023. \\nAs we have often highlighted in our reports and client calls, high valuations \\nalone are not an effective timing signal. We expect US equities to outperform \\nboth intermediate-duration US bonds and cash based on our economic growth \\nforecast of 2.3%. US equities would be most likely to underperform in the case of \\na recession, yet we assign just a 20% \\nprobability to a recession in 2025. \\n Hence our recommendation to stay \\ninvested. Since January 2010, when \\nour clients first asked if they should \\nunderweight US equities following the \\nstrong returns of the S&P 500 after the \\nGFC, we have recommended they stay \\ninvested on 128 separate occasions in \\nour various ISG publications and client \\ncalls, as shown in Exhibit 2. \\n With respect to gold and bitcoin, \\nwe recognize that the 27% return of \\ngold has attracted client attention. In \\n2024, gold outperformed the S&P \\n500 by two percentage points, with \\nExhibit 2: S&P 500 vs. ISG Recommendation to \\nStay Invested in US Equities\\nWe have recommended clients stay invested on 128 \\nseparate occasions since 2010.\\nMentions of “Stay Invested in US Equities”S&P 500S&P 500\\n800\\n1,600\\n2,400\\n3,200\\n4,000\\n4,800\\n5,600\\n6,400\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024\\nData as of December 31, 2024.  \\nSource: Investment Strategy Group, Datastream.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='23c5396d-466d-4cf7-9d3c-ed9cad4280ac', embedding=None, metadata={'page_label': '5', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5Outlook Investment Strategy Group\\nsimilar levels of volatility. China, including the People’s Bank of China, has \\nbeen the largest buyer of gold since November 2022, following the sanctions \\nagainst Russia related to the invasion of Ukraine. China’s purchases are likely to \\ncontinue, given its increasing geopolitical tensions with the US. However, we do \\nnot believe gold is a strategic asset class for a prudently diversified portfolio, as \\ndiscussed in detail later in this report and in our 2010 Insight, Commodities: A \\nSolution in Search of a Strategy. Nor do we tactically recommend investing in gold \\nsince the recent price increases have been driven by the purchases made by China \\nand (to a lesser extent) those made by other central banks. We are agnostic with \\nrespect to the short-term upside or downside of gold, but do not expect gold to \\noutperform the S&P 500 over the next five years. \\n The 123% return of bitcoin is even more tantalizing to some investors. \\nAlthough its volatility has decreased from an average of 125% during the \\n2010–14 period to a post-2014 average volatility of 63%, it is still more than \\nfour times as volatile as US equities. Therefore, for every $1 invested in bitcoin, a \\nclient can invest over $4 in the S&P 500. Based on this adjustment, the S&P 500 \\nlagged bitcoin by about 30 percentage points in 2024. \\n Bitcoin prices benefited from a nearly 40% boost after the November 5 US \\nelection, driven by expectations of a more favorable cryptocurrency regulatory \\nenvironment. We have long argued that bitcoin is a speculative trading asset \\nand the behavior of it and other cryptocurrencies in recent months only \\nreinforces that view. To provide some perspective on the election’s impact \\non cryptocurrencies’ performance, consider the performance of memecoins. \\nMemecoins, described by the Financial Times as “joke-based tokens with \\nbafflingly enduring appeal,”3 are digital coins that originate from an internet \\nmeme. They have rallied significantly and beyond reason since the election. \\nDogecoin, the largest memecoin, with a market capitalization of $47 billion, has \\nrallied about 90% since the election. Shiba Inu, the second-largest by market \\ncapitalization, has rallied 25%. \\n We address these questions in greater detail in Section I. We show that the gap \\nbetween the US and other developed and emerging market countries continues \\nto widen across most metrics. No major country, including China, will catch up \\nto the US across most of these metrics for the foreseeable future—if ever. The \\nstrengths of the US economy, which sustain steady and reliable economic and \\nearnings growth, are unparalleled. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f556fa0c-b5e7-439c-9d70-226af985f683', embedding=None, metadata={'page_label': '6', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6 Goldman Sachs january 2025\\n Importantly, we argue that the recent barrage of commentary from the media \\nciting the end of American exceptionalism is misplaced. We believe the incoming \\nTrump administration will not derail US preeminence—the system of checks and \\nbalances is alive and well. \\n We also explain why the record cheapness of most non-US equity markets does \\nnot warrant a tactical shift away from US equities. \\n Next, we put forth our one- and five-year expected returns, which underpin \\nour view not to underweight US equities in favor of bonds or cash. We also \\nreview our opportunistic tactical tilts going into 2025.\\n Along the way, we dispel several key myths that have become common lore. \\nWe show that:\\n• There is no evidence of mean reversion in equity valuations.\\n• Valuations are not an effective timing signal to exit the markets.\\n• Equity concentration is not an effective timing signal for forward returns.\\nMyth Busters', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9326dc3d-a7a4-4273-bb4d-476e7608d38c', embedding=None, metadata={'page_label': '7', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7Outlook Investment Strategy Group\\nWe also show why China cannot rely on exports to the US, Europe and, \\nimportantly, the Global South to maintain its current target growth rates. We \\nbelieve China’s economy will, at best, follow the path of Japan’s and decline to a \\n2% growth rate. \\n We conclude Section I with key risks to our outlook. We first address the issue \\nof the US debt trajectory and show why it is not a risk to our outlook for at least \\nthe next decade. While no one knows the exact tipping point when the level of \\nUS debt-to-GDP will lead to a financial crisis, we show that the level is likely \\nmuch higher than the current 99%. \\n From our perspective, the greater risks emanate from heightened geopolitical \\ntensions and emboldened risk-taking by Russia, North Korea and, most \\nnotably, China. \\n In Section II, we review our economic outlook for key developed and emerging \\nmarket countries. \\n Section III details our financial market outlook for these countries as well as \\nour outlook on the US dollar. \\n As usual, we present our annual outlook and our investment themes following \\nextensive and rigorous analysis performed by our team, including consultations \\nwith leading experts. Still, we publish this annual report as we have every \\nyear, with an appropriate dose of humility, especially at this time of heightened \\ngeopolitical uncertainty and tensions around the globe. \\n In closing, we wish you a healthy, happy, prosperous and safe 2025.\\nThe Investment Strategy Group', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='cacc32f2-4b61-4e8b-a713-a34d900b37f8', embedding=None, metadata={'page_label': '8', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8 Goldman Sachs january 2025\\nContents\\n SECTION I\\n10 US Preeminence: The Gap Widens  \\n  The gap between the US and the rest of the \\nworld across a number of economic and \\nfinancial market metrics continues to widen.\\n11 Economic Factors\\n18 Structural Factors\\n20 Strategic Allocation: Minor Adjustments\\n22  Staying Invested in US Equities Versus \\nNon-US Equities\\n34  Staying Invested in US Equities Versus \\nBonds or Cash\\n38  Strategic and T actical Assessment of Gold \\nand Bitcoin\\n43  Our One- and Five-Y ear Expected \\nT otal Returns\\n44 Our T actical Tilts \\n48   Risks to Our 2025 Economic and Financial \\nMarket Outlook\\n56  Key Takeaways\\n  Our investment themes of US Preeminence \\nand Stay Invested remain valid, but we do not \\nexpect US equities to continue to outperform \\nby the same magnitude that they have over the \\nlast nearly 16 years.\\n2025 OUTLOOK', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='956e8a91-4464-4f1d-9e66-1f5e0814776b', embedding=None, metadata={'page_label': '9', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9Outlook Investment Strategy Group\\n SECTION II: DIVERGING PATHS\\n58  2025 Global  \\nEconomic Outlook\\n  Despite divergent paths, most economies will \\nascend toward higher elevations this year.  \\n60  United States\\n64  Eurozone\\n66  United Kingdom\\n67 Japan\\n68  Emerging Markets\\n SECTION III: EXTENDING THE SEASON\\n74  2025 Financial \\nMarkets Outlook\\n  Ongoing economic growth will lift markets this \\nyear, albeit with a few icy patches along the way.\\n76  US Equities\\n82  Non-US Developed Market Equities\\n83  Eurozone Equities\\n84 UK Equities\\n84  Japanese Equities\\n85  Emerging Market Equities\\n86  Global Currencies\\n91  Global Fixed Income\\n104 Global Commodities', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='360a2759-2525-4a63-b7ff-079b0d27a208', embedding=None, metadata={'page_label': '10', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10 Goldman Sachs january 2025\\nUS Preeminence:  \\nThe Gap Widens \\nour view of us preeminence is predicated upon several \\nfactors that have helped create the largest and most diverse, \\ninnovative and resilient economy in the world. Some of the \\nfactors are economic, like the country’s wealth and research and \\ndevelopment (R&D) budget; some are cultural, like a tendency \\ntoward risk-taking and entrepreneurship; some are structural, \\nlike good governance and a system of checks and balances; and \\nothers are simply based on geography and geology, such as the \\nadvantages of having oceans on two sides and an abundance of \\nnatural resources. These factors have underpinned our strategic \\noverweight to US assets and our tactical view of staying \\ninvested in US equities rather than reallocating assets to non-US \\nequities or bonds and cash. Below, we look more closely at two \\ncategories of factors: economic and structural.\\nSECTION I', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d6305a6d-a2b5-4f53-8933-d5679d4cf524', embedding=None, metadata={'page_label': '11', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='11Outlook Investment Strategy Group\\nEconomic Factors\\nWe begin with the economic factors. The economic \\nwealth of the US allows for significantly greater \\nallocation of resources to R&D, innovation, health \\ncare, education, the military and other areas. Its \\nenormous wealth also affords the country its unique \\nstatus as the issuer of the world’s reserve currency. \\nSize of Economy\\nThe US is the largest economy in the world and \\naccounts for more than a quarter of world GDP \\n(see Exhibit 3). In fact, the US has been the world’s \\nlargest economy since the 1890s and will remain so \\nfor the foreseeable future, as the gap between the \\nUS and the rest of the world continues to widen—\\njust as it did in 2024. \\n Last year, the US added $1.4 trillion to its \\nGDP, whereas China, the second-largest economy, \\nadded $937 billion and the entire Eurozone added \\n$619 billion (see Exhibit 4). As another point of \\ncomparison, US GDP grew by slightly more than \\nthe entire GDP of the Netherlands.\\n Some might say that the 2024 GDP data is \\nskewed by the unexpectedly strong performance \\nof the US economy. We therefore examine the data \\nsince 2019 to include the impact of COVID-19, \\nbecause the US—by most measures—fared \\nparticularly poorly during the pandemic. According \\nto the Pandemic Center at the Brown University \\nSchool of Public Health, “the US stands out as a \\nclear outlier: although among the highest prepared, \\nExhibit 3: Top 10 Countries Ranked by 2024 \\nNominal GDP\\nThe US has the highest GDP in the world.  \\n29,168\\n18,273\\n16,370\\n4,710 4,070 3,889 3,588 3,174 2,377 2,215 2,188\\n0\\n5,000\\n10,000\\n15,000\\n20,000\\n25,000\\n30,000\\nUS\\nChina\\nEurozone\\nGermany\\nJapan\\nIndia\\nUK\\nFrance\\nItaly\\nCanada\\nBrazil\\nUS$ Billions\\nData as of 2024. \\nNote: The exhibit uses IMF full-year estimates for 2024 and shows top 10 countries plus \\nthe Eurozone.  \\nSource: Investment Strategy Group, IMF.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 4: Change in Nominal GDP Since Q4 2023\\nThe US added $1.4 trillion to its GDP over the past year. \\n1,404\\n937\\n619\\n404\\n291\\n59\\n0\\n300\\n600\\n900\\n1,200\\n1,500\\nUS China Eurozone India UK Japan\\nUS$ Billions\\nData as of Q4 2024.  \\nNote: Q4 2024 is based on ISG estimates.  \\nSource: Investment Strategy Group, Haver Analytics, Bloomberg.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.  \\n \\nExhibit 5: Change in Nominal GDP Since Q4 2019\\nSince year-end 2019, US GDP has grown by $7.8 trillion—\\nmore than the entire GDP of any country except for China.  \\n7,768\\n4,258\\n3,007\\n1,145\\n784\\n-961\\n-2,000\\n-1,000\\n0\\n1,000\\n2,000\\n3,000\\n4,000\\n5,000\\n6,000\\n7,00 0\\n8,000\\nUS China Eurozone India UK Japan\\nUS$ Billions\\nData as of Q4 2024.  \\nNote: Q4 2024 is based on ISG estimates. \\nSource: Investment Strategy Group, Haver Analytics, Bloomberg.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ca2dd254-f490-4d4c-8b6f-3447afed6183', embedding=None, metadata={'page_label': '12', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12 Goldman Sachs january 2025\\nit experienced the third-highest age-adjusted \\nmortality—more than eight times higher than what \\nwould have been expected.”4\\n Despite this public health care failure, US GDP \\nhas grown by $7.8 trillion—more than the entire \\nGDP of any country except China—since year-end \\n2019, as shown in Exhibit 5.\\n Given this growing gap, even China does not \\ncatch up to the GDP of the US—ever. China’s \\neconomy peaked at 75% of the US economy in \\n2021; at that time, the US economy had been \\nnegatively affected by COVID-19 while China \\nhad escaped relatively unscathed and the Chinese \\nrenminbi had appreciated against the US dollar. \\n We do not expect China’s economy to exceed that \\npeak. Its economy currently stands at 63% of the US \\neconomy. We expect the ratio to reach a high of 70% \\nby 2034 and gradually level off (see Exhibit 6). \\n The data presented in the charts is based on \\nnominal GDP at current exchange rates. There are \\nsome economists, political scientists and market \\nparticipants who argue that purchasing power parity \\n(PPP) is a better measure for comparing GDP across \\ncountries because it incorporates the lower cost of \\ngoods and services in emerging market countries. \\nBased on that measure, China is a larger economy, \\nstanding at $37 trillion versus $29 trillion in the US. \\n We believe it is misleading to use PPP to \\ncompare China’s economy to that of the US when \\ndiscussing a country’s preeminence on the global \\nstage. As an International Monetary Fund (IMF) \\npaper notes, “market exchange rates are the logical \\nchoice when financial flows are involved.”5 The \\nIMF uses both market exchange rates and PPP, \\ndepending “on the issue being considered.”\\n Given China’s financial flows, including its \\nextensive dollar-based imports, our team (which \\nincludes a former IMF China desk specialist) \\nbelieves current market exchange rates are more \\nappropriate for some cross-country comparisons. \\n China, for example, imports 18% of its GDP—\\nmuch of it in commodities that are quoted globally \\nin US dollars. China imports 74% of the crude oil \\nand refined products it consumes, 36% of its natural \\ngas consumed, 81% of its iron ore consumed, 92% \\nof its soybeans consumed, 33% of its beef consumed \\nand 23% of its refined copper consumed. \\n Another example of global financial flows in \\nChina that justify the use of market exchange rates \\nis the Belt and Road Initiative (BRI). Estimated at \\nabout $1 trillion in size, the program has provided \\nhard-currency-denominated loans to many low- and \\nmiddle-income countries. In the early years of the \\nBRI, between 2014 and 2017, as little as 50% and \\nas much as 80% of the loans were denominated in \\nUS dollars. In later years, dollar-denominated loans \\naccounted for over 40% of loans.6\\nExhibit 6: China’s GDP as a Share of US GDP\\nWe do not expect China’s economy to exceed 2021’s peak of \\n75% of US GDP. \\n68\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n2000 2005 2010 2015 2020 2025 2030\\n75\\n2035 2040 2045 2050\\nChina Nominal GDP (% of US GDP)\\nForecast\\n63\\nData through 2024. Forecast through 2050. \\nSource: Investment Strategy Group, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved. \\n \\nExhibit 7: Nominal GDP per Capita\\nThe US also has the highest GDP per capita of any major \\neconomy.  \\n86.6\\n55.5 52.4\\n48.0\\n32.9\\n13.0\\n2.7\\n0\\n20\\n40\\n60\\n80\\n100\\nUS Germany UK France Japan China India\\nUS$ Thousands\\nData as of 2024. \\nNote: The exhibit shows IMF full-year estimates for 2024. \\nSource: Investment Strategy Group, IMF.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9afe0a27-de05-44ca-8d7c-9a49d0f7b816', embedding=None, metadata={'page_label': '13', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='13Outlook Investment Strategy Group\\nGDP per Capita\\nThe US also has the highest GDP per capita of any \\nmajor country, at about $86,600 (see Exhibit 7). \\nThe handful of countries that have a higher GDP \\nper capita are either tax havens or oil and natural \\ngas producers—all with populations of less than \\n10 million.\\n The US GDP per capita increases in 2024 and \\nsince year-end 2019 also dwarf those of other \\ncountries: the gap continues to widen. \\n As shown in Exhibit 8, the $3,600 increase \\nin US GDP per capita in 2024 was double the \\nincrease in the Eurozone, and five times as great \\nas the increase in China. The only major country \\nwith a greater increase than the US in 2024 is the \\nUK, but that is explained by the appreciation of \\nsterling. Adjusted for the currency impact, UK GDP \\nper capita increased by $1,664. \\n Similarly, since 2019, US GDP per capita has \\nincreased by $21,000: that is twice the increase in \\nthe UK, 2.5 times as great as the increase in the \\nEurozone, and nearly seven times as great as the \\nincrease in China. As we showed in our December \\n2022 Insight report, Middle Kingdom: Middle \\nIncome, China’s GDP per capita will not catch up \\nto that of the US in the 21st century; its GDP per \\ncapita is below that of the poverty level in the US, \\nso its growth rates are applied to a very low base.7\\nProductivity\\nOne of the key drivers of such steady economic \\ngrowth is US labor productivity. The US labor force \\nis the most productive in the world (see Exhibit \\n9). A US employee generates $171,000 of GDP; \\nthe next-highest levels are produced in Taiwan at \\n$154,000, while Germany is at $120,000 and China \\nat $47,000—or 27% of US productivity levels. \\nExhibit 8 Change in Nominal GDP per Capita \\nSince Q4 2023\\nThe $3,600 increase in US GDP per capita in 2024 was \\ntwice as large as the increase in the Eurozone.  \\n3,887\\n1,664\\n3,600\\n1,779\\n689 641\\n257\\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n3,500\\n4,000\\n4,500\\nUK UK\\n(Constant FX)\\nUS Eurozone China Japan India\\nUS$ \\nData as of Q4 2024.  \\nNote: Q4 2024 is based on ISG estimates.  \\nSource: Investment Strategy Group, Haver Analytics, Bloomberg.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 9: Labor Productivity per Person Employed\\nThe US labor force is the most productive in the world, while \\nChina and India lag meaningfully behind. \\n171\\n154\\n127 120 118\\n96\\n47\\n25\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\n180\\nUS Taiwan France Germany UK Japan China India\\nUS$ Thousands (2022 PPP*)\\nData as of 2024. \\nSource: Investment Strategy Group, Total Economy Database™, Conference Board. \\n* Purchasing power parity. \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='514d35ff-b212-4e85-9bd3-eb374ed399eb', embedding=None, metadata={'page_label': '14', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='14 Goldman Sachs january 2025\\n In 2024, US labor productivity surged ahead, \\ngrowing by $9,000 and further widening the gap \\nbetween the US and the rest of the world (see \\nExhibit 10). Labor productivity in the US has been \\nthe highest among all countries since 2003. \\n In our 2016 Outlook: The Last Innings, \\nwe argued against the pessimism put forth by \\nRobert J. Gordon, professor of economics at \\nNorthwestern University and author of The Rise \\nand Fall of American Growth (2016). In the book, \\nhe contended that US productivity growth had \\nalready “slowed to a crawl” and would be “further \\nheld back.”8\\n The optimists have been proven right, given \\nthe continued innovation in robotics, 3D printing, \\ngenetics, biotechnology, artificial intelligence, the \\ncloud and big data—just to name a few—and further \\ndiffusion of information technology through society. \\n We note that labor productivity can be \\nmeasured by productivity per person employed \\nbut also by productivity per hour worked. The US \\nranks the highest by both measures. The data on \\nthe absolute level of productivity and magnitude of \\nproductivity growth in the US is incontrovertible. \\nIf our clients read or hear observations to the \\ncontrary, we encourage them to question the \\nreliability of the source. \\n We also note that productivity is measured \\nacross countries using PPP; as discussed earlier, we \\nuse nominal exchange rates for financial capital \\nflows across countries, but PPP is the standard \\nform of measurement for productivity. The \\nPPP standard raises the level of productivity of \\nemerging market countries such as China and India \\nand has minimal impact on developed economies. \\nEducation, Human Capital and \\nHours Worked\\nThe drivers of US productivity include \\nhigh levels of education, experience \\nand knowledge. Together they drive the \\naccumulation of human capital. The \\noutput of a worker each year is also \\ndetermined by hours worked. \\n As shown in Exhibits 11, 12 and \\n13, the US has the highest average level \\nExhibit 10: Change in Labor Productivity\\nUS labor productivity grew by $9,000 in 2024, further \\nwidening the gap with the rest of the world. \\n9,073\\n6,088\\n3,684 3,658\\n3,120\\n2,012\\n708 673\\n-2,773\\n-4,000\\n-2,000\\n0\\n2,000\\n4,000\\n6,000\\n8,000\\n10,000\\nUS UK Germany France Eurozone Taiwan China India Japan\\nUS$ \\nData as of 2024.  \\nNote: Productivity in 2024 is based on ISG and IMF estimates. \\nSource: Investment Strategy Group, Total Economy Database™, Conference Board, Haver \\nAnalytics, IMF.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 11: Average Years of Schooling\\nThe US has the highest average number of years of \\nschooling. \\n13.3 13.2 13.1 13.1\\n12.5\\n10.6\\n9.2\\n7.8\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\nUS Korea UK Japan Germany France China India\\nYears\\nData as of 2020. \\nNote: Average years of schooling data is estimated using original growth assumptions between \\n2015 and 2020 from Barro-Lee (2015) and the most recent years of schooling data for 2015 as \\nreported by Barro-Lee (2021).  \\nSource: Investment Strategy Group, UNESCO, United Nations, Barro-Lee. \\n \\nThe data on the absolute level of \\nproductivity and magnitude of \\nproductivity growth in the US is \\nincontrovertible.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='21728930-ecb2-4c7c-aed2-8a32d5c6f7fa', embedding=None, metadata={'page_label': '15', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='15Outlook Investment Strategy Group\\nof schooling as measured in years. Its share of \\nworking-age people who have completed tertiary \\neducation is the second-highest, and its level of \\nhuman capital is the third-highest. \\n According to data from the Times Higher \\nEducation World University Rankings 2025, the \\nUS accounts for 17 of the top 30 universities in the \\nworld. The rankings are based on 18 metrics across \\nfive categories: teaching, research environment, \\nresearch quality, international outlook and \\nindustry, which includes industry income and \\npatents. Europe (including the UK) is home to \\nseven of the top universities and China has two. \\nExhibit 14 lists the top 10 universities. \\n Finally, hours worked also contribute to labor \\nproductivity when measured by output per person \\nemployed. For example, US workers work about \\n18% longer than workers in the Eurozone. More \\nspecifically, they work 34% more than German \\nworkers and 20% more than French workers (see \\nExhibit 15). South Korean, Taiwanese, Chinese \\nand Indian workers work even longer hours \\nthan US workers, but they are not as productive \\nas US workers because their output per hour is \\nsubstantially lower.\\nR&D and Innovation \\nThe US also leads in R&D and innovation. In \\nR&D, which is easier to measure than innovation, \\nthe US spent over $800 billion in 2021, the latest \\nyear for which data is available. That compares to \\n$434 billion for China and $166 billion for Japan \\n(see Exhibit 16). \\n The higher R&D spend has translated into \\nhigher US rankings for innovation. Understanding \\nthat no measure of innovation is perfect, we \\npresent three metrics:\\nExhibit 14: Times Higher Education  2025 \\nRanking—Top 10 Universities in the World\\nThe US accounts for the majority of the top universities in \\nthe world. \\nRank University Country \\n1 University of Oxford UK\\n2 Massachusetts Institute of Technology US\\n3 Harvard University US\\n4 Princeton University US\\n5 University of Cambridge UK\\n6 Stanford University US\\n7 California Institute of Technology US\\n8 University of California, Berkeley US\\n9 Imperial College London UK\\n10 Yale University US\\nData as of 2025. \\nSource: Investment Strategy Group, Times Higher Education . \\nExhibit 13: Human Capital Index \\nThe quality of the US labor force is among the highest in \\nthe world. \\n3.8 3.8 3.7 3.7 3.6\\n3.2\\n2.7\\n2.2\\n0\\n1\\n2\\n3\\n4\\n UK  Korea  US  Germany  Japan  France  China  India\\nHuman Capital Index\\nData as of 2019.  \\nNote: The index measures the human capital that a child born today can expect to attain by her \\n18th birthday, highlighting how current health and education outcomes shape the productivity of \\nthe next generation of workers.  \\nSource: Investment Strategy Group, World Bank. \\nExhibit 12: Share of Working-Age Population With \\nCompleted Tertiary Education\\nThe working-age population in the US has a high level of \\ncompleted tertiary education relative to other regions.\\n32.1\\n29.3\\n27.7\\n24.0\\n16.4\\n15.1\\n6.0 5.8\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\nUS Japan UK Germany France India China\\n%\\nKorea\\nData as of 2020. \\nNote: Share of population that has completed tertiary education is estimated using original \\ngrowth assumptions between 2015 and 2020 from Barro-Lee (2015) and the most recent \\ncompletion rate data for 2015 as reported by Barro-Lee (2021).  \\nSource: Investment Strategy Group, UNESCO, United Nations, Barro-Lee.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='481e6084-94eb-4fb6-aab1-a74e89811575', embedding=None, metadata={'page_label': '16', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='16 Goldman Sachs january 2025\\n1. Number of patents. Japan and the US are world \\nleaders, with 16,000 and 14,000 triadic patent \\nfamilies, respectively, registered as of 2021. A \\ntriadic patent family is a set of patents filed at \\nthree major patent offices: the US Patent and \\nTrademark Office, the European Patent Office \\nand the Japan Patent Office (see Exhibit 17). \\n2. The Modern Innovation System Composite \\nIndex. Designed by the Atlantic Council \\nGeoEconomics Center and Rhodium Group’s \\nChina Pathfinder, this metric is comprehensive \\nand captures national R&D spending as a \\nshare of GDP, venture capital attractiveness, \\nprivate-sector versus state-funded innovation, \\ntriadic patent families filed, international \\nattractiveness of a nation’s intellectual \\nproperty, and strength of the intellectual \\nproperty regime. According to this metric, \\nthe US is ranked third, after South Korea \\nand Japan. China stands at 2.5, which is well \\nExhibit 15: Annual Hours Worked per Worker\\nWorkers in the US work 34% more than workers in Germany \\nand 20% more than French workers.\\n2.4 2.3\\n2.0 1.9 1.8\\n1.6\\n1.5 1.5 1.5\\n1.3\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\nIndia China Taiwan Korea US Japan Eurozone UK France Germany\\nThousands\\nData as of 2024. \\nSource: Investment Strategy Group, Total Economy Database™, Conference Board. \\n \\nExhibit 17: Triadic Patent Families Registered \\nJapan and the US have the most triadic patent families \\nregistered.\\n16,102\\n14,341\\n6,106\\n4,364\\n1,991 1,868\\n0\\n2,000\\n4,000\\n6,000\\n8,000\\n10,000\\n12,000\\n14,000\\n16,000\\n18,000\\nJapan US China Germany France UK\\nTriadic Patent Families Registered\\nData as of 2021. \\nNote: Triadic patent families are a set of patents filed at the European Patent Office, the United \\nStates Patent and Trademark Office, and the Japan Patent Office. \\nSource: Investment Strategy Group, OECD.\\nExhibit 16: Top 10 Countries Ranked by \\nR&D Spending\\nThe US spends the most on R&D globally.\\n819\\n434\\n166 137\\n92 90 66 34 32 17\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n900\\nUS China Japan Germany UK Korea France Canada Italy India\\nUS$ Billions\\nData as of 2021. \\nNote: India’s R&D spending is as of 2020. \\nSource: Investment Strategy Group, Haver Analytics. \\nExhibit 18: Modern Innovation Composite Index\\nThe US is ranked third, after South Korea and Japan, while \\nChina ranks well below the average open economy. \\n6.5\\n5.5\\n4.8 4.8 4.6\\n4.0\\n3.5\\n2.5\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n Japan Korea  US  UK  Germany Avg. Open\\nEconomy\\n France  China\\nModern Innovation System Composite Index\\nData as of 2023. \\nSource: Investment Strategy Group, China Pathfinder, Atlantic Council, Rhodium Group.  ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='da4c7268-0e8f-407d-8061-3ac75578b181', embedding=None, metadata={'page_label': '17', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='17Outlook Investment Strategy Group\\nbelow the average of the 10 largest open \\nmarket economies (see Exhibit 18). \\n3. Number of technology companies with \\nmeaningful net income. The US has 48 listed \\ntechnology companies with a net income of \\nover $1 billion, compared to Japan at eight, \\nthe Eurozone and China at seven, Taiwan at \\nfive, and Korea at four (see Exhibit 19).\\nCapital Markets\\nFinally, a dynamic, innovative and growing \\neconomy depends on available financing and open \\ncapital markets. The US has the largest public and \\nprivate financial markets. \\n The market capitalization of the United States’ \\npublic equity and bond markets stands at $79 \\ntrillion and is eight times as large as that of the \\nnext country, which is Japan, at $10 trillion. US \\npublic equity markets account for 67% of the \\nMSCI All-Country World Index (ACWI). \\nUS investment grade bonds account for \\n41% of the Bloomberg Global Aggregate \\nBond Index. \\n The US also dominates the private \\ncapital markets. Of an estimated $13.3 \\ntrillion in private assets, North America \\naccounts for $8.2 trillion, or 61%.9 \\nThose assets are invested in private \\nequity, venture capital, private debt, \\nprivate real estate, infrastructure and \\nnatural resources. Europe accounts for 23%, at \\n$3.1 trillion, and the Asia-Pacific region including \\nChina accounts for 12%, with the rest of the world \\naccounting for 3%.\\n North America also accounts for 61% of \\nventure capital funds, which is an additional \\nfactor in the higher rate of innovation in the US. \\nMost of the capital for risk-taking innovators and \\nentrepreneurs is found in the US. \\n Another indicator of US preeminence reflected \\nin the financial markets is the predominance of \\nUS companies in the MSCI ACWI. US companies \\naccount for seven of the top 10, based on the \\naverage of their net income over the last three years \\n(see Exhibit 20). This exhibit understates the role \\nof US companies in generating profits, because the \\nthree that are not based in the US are state-owned \\nenterprises. The primary stakeholders in Saudi \\nArabian Oil Company, Industrial and Commercial \\nExhibit 19: Number of Technology Companies \\nWith Net Income Exceeding $1 billion\\nThe US has the highest number of large technology \\ncompanies globally. \\n48\\n8 7 7\\n5\\n22\\n4\\n0\\n0\\n10\\n20\\n30\\n40\\n50\\nUS Japan ChinaEurozone KoreaTaiwan France UKGermany\\nNumber of Companies \\nData as of December 2024.  \\nNote: Numbers are based on MSCI ACWI constituents. \\nSource: Investment Strategy Group, Datastream. \\nExhibit 20: Top 10 Companies in MSCI ACWI by \\nNet Income\\nUS companies comprise 7 of the top 10 companies globally \\nbased on net income.\\nRank Company Country \\n3-Year \\nAverage Net \\nProfit (US$ \\nBillions)\\n1 Saudi Arabian Oil Co. Saudi Arabia 129\\n2 Apple Inc. US 101\\n3 Microsoft Corp. US 82\\n4 Alphabet Inc. US 79\\n5 Industrial and Commercial Bank of China Ltd. China 52\\n6 Amazon.com Inc. US 49\\n7 China Construction Bank Corp. China 47\\n8 JPMorgan Chase & Co. US 46\\n9 Exxon Mobil Corp. US 44\\n10 Meta Platforms Inc. US 41\\nData as of December 2024. \\nNote: Net income is measured on adjusted basis to exclude one-off items. The 2024 3-year \\naverage is calculated using 2022, 2023 and 2024 calendarized company figures. 2024 net profit is \\nbased on Bloomberg consensus.  \\nSource: Investment Strategy Group, Bloomberg, MSCI. \\nThe market capitalization of the \\nUnited States’ public equity and bond \\nmarkets stands at $79 trillion and is \\neight times as large as that of the next \\ncountry, which is Japan.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2120a269-8b84-4405-a974-37b96d32615d', embedding=None, metadata={'page_label': '18', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='18 Goldman Sachs january 2025\\nBank of China, and China Construction Bank are \\ntheir governments, which own 97%, 82%, and \\n82%, respectively, of these companies.\\n Given the outperformance of US assets relative \\nto non-US assets, we expect US capital markets to \\ngrow faster than those of other large economies \\nas more capital flows into the US. As shown in \\nExhibit 21, net portfolio inflows from foreign \\ninvestors have increased to their highest level since \\nthe GFC, at $1.4 trillion over the last four quarters. \\nThis level is more than double the average since \\nthe GFC. Net foreign direct investment (FDI) has \\nincreased to $382 billion, which is 20% more than \\nthe average since the GFC (see Exhibit 22).\\n Net portfolio flows into Europe in the same \\nperiod have been much smaller, at $743 billion, \\nand net FDI has been negative, at $372 billion. In \\nChina, net portfolio flows have turned positive, at \\n$139 billion, compared to a post-GFC average of \\n$76 billion; net FDI has dropped precipitously to \\nnearly zero, compared to a post-GFC average of \\n$208 billion.\\n Given the performance of Chinese equities since \\nthe GFC and the current increased uncertainty \\nin Chinese economic policies, we believe foreign \\nportfolio and FDI flows to China will be limited \\nfor the next several years. \\nStructural Factors\\nISG has consistently stated that one of the \\nfoundations of US preeminence is the strength \\nof its institutions and its system of checks and \\nbalances. \\n Naysayers continue to question the viability of \\nAmerica’s system of governance, despite it having \\nbeen tested over nearly 250 years. The election of \\nDonald Trump to a second term as president has \\nonce again brought to the fore the concerns that \\nwere raised during his first term:\\n• Nobel Laureate and Institute Professor of \\nEconomics at MIT Daron Acemoglu: “As an \\nobvious threat to US democracy, he will erode \\nmany critical institutional norms over the next \\nfour years.”10 \\n• The Harvard Kennedy School’s Paul F. McGuire \\nLecturer in Comparative Politics, Pippa \\nNorris: “The longer-term risks of democratic \\nbacksliding have risen exponentially, given \\nthe lack of checks on the aggrandizement of \\nPresidential power after Republicans gained \\ncontrol of the White House, the Senate, and … \\nthe House of Representatives.”11 \\n• Cornell University professor and senior fellow \\nat the Brookings Institution and former IMF \\neconomist Eswar Prasad: “Trump’s actions \\n… will undercut key elements of the US \\ninstitutional framework … Washington’s system \\nExhibit 21: Net Portfolio Flows From Abroad\\nUS portfolio inflows have increased to the highest level \\nsince the GFC at $1.4 trillion over the past four quarters. \\nUS China Eurozone\\n1,396\\n139\\n743\\n-400\\n-200\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\n1,600\\n2001 2004 2007 2010 2013 2016 2019 2022\\n4-Quarter Rolling Sum (US$ Billions)\\nData through Q3 2024.  \\nSource: Investment Strategy Group, Haver Analytics.\\nExhibit 22: Net Foreign Direct Investment Flows \\nFrom Abroad \\nEurozone FDI inflows have been negative, while inflows into \\nChina have dropped to nearly zero.  \\nUS\\n4-Quarter Rolling Sum (US$ Billions)\\n382\\n5\\n-372\\n-600\\n-400\\n-200\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n2001 2004 2007 2010 2013 2016 2019 2022\\nChina\\nEurozone\\nData through Q3 2024.  \\nSource: Investment Strategy Group, Haver Analytics.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='61c760fa-03e5-44ca-809e-ce81e6f56e0e', embedding=None, metadata={'page_label': '19', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='19Outlook Investment Strategy Group\\nof checks and balances will be substantially \\nweaker in the next few years.”12 \\n• Columnist, senior advisor at the Atlantic \\nCouncil and businessman Harlan Ullman: \\n“The Constitution’s system of checks and \\nbalances and divided government could soon \\nend. President-elect Donald Trump not only \\ntransformed the Republican Party into the \\n‘Make-America-Great-Again’ party but also \\ntransformed the Constitution, bypassing it to \\nform a government ruled by what looks like an \\nincipient American Politburo.”13 \\nWe addressed these concerns in our 2019 Outlook: \\nAmerican Preeminence in a Rattled World, and \\non two client calls in 2024 before the US election \\nwith Karl Rove, senior advisor (2000–04) and \\ndeputy chief of staff (2004–07) to then-President \\nGeorge W. Bush. In the client calls, he shared his \\nview that the “president’s power is constrained by \\nlaw and by the courts and by the Congress,” and \\nhe believes that the court system will serve as “a \\nbrake on any president.” He highlighted that “there \\nare guardrails in place that will keep the country \\nmoving forward and provide us the opportunity to \\ndo what we have historically done.” \\n Post-election, Rove pointed to several recent \\nevents that show the guardrails are holding:\\n• The failed nomination of Representative Matthew \\nGaetz to serve as attorney general, which resulted \\nin an immediate, overwhelmingly negative \\nreaction from Republican senators and Gaetz’s \\nsubsequent withdrawal from consideration\\n• The election of Senator John Thune of South \\nDakota as Senate majority leader even though \\nhe was not the candidate favored by many \\nincoming Trump administration supporters\\n• The passage of a funding bill that averted a \\ngovernment shutdown without meeting Trump’s \\ndemand for the removal or increase in the \\ngovernment’s borrowing limit\\n• Thune’s plans to put forth two separate \\nreconciliation bills, the first focusing \\non border security, defense and energy \\nand the second dealing with the 2017 \\ntax cuts that expire at the end of 2025, \\nwithout Trump’s prior approval \\nThe late Charles Krauthammer, a \\npolitical journalist who won the Pulitzer \\nPrize in 1987 for his columns in the \\nWashington Post, wrote in the early years of the \\nTrump presidency, “Our checks and balances \\nhave turned out to be quite vibrant.”14 America’s \\nguardrails, he posited, had held.15  \\n As we wrote in last year’s Outlook, we believe \\nthe system of checks and balances will continue \\nto hold. Our view is bolstered by the words \\nof James Baker III, former secretary of state \\n(1989–92), secretary of the Treasury (1985–88) \\nand White House chief of staff (1981–85 and \\n1992–93), who said: “We are a country of laws, \\nlimited by bureaucracy and the power structure in \\nWashington. Presidents are not unilateral rulers.”16 \\n Over the years, we have often quoted Alexis \\nde Tocqueville, French political scientist and \\nphilosopher and author of Democracy in America, \\nin ISG Outlook reports as a reminder to our \\nclients of the great resilience of the US. We heed his \\ninsight again: “The greatness of America lies not in \\nbeing more enlightened than any other nation, but \\nrather in her ability to repair her faults.”17 \\nOutlook\\nAmerican Preeminence  \\nin a Rattled World\\nInvestment Strategy Group | January 2019\\nConsumer and Investment Management Division\\nEmma Lazarus’ Colossus\\nThe greatness of America lies not \\nin being more enlightened than any \\nother nation, but rather in her ability \\nto repair her faults.”\\n— Alexis de Tocqueville', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='dba73e99-b81a-4f81-89ec-2965c6566441', embedding=None, metadata={'page_label': '20', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='20 Goldman Sachs january 2025\\nStrategic Allocation: Minor Adjustments\\nInvestors have clearly discounted US preeminence, \\nas measured by the outperformance of US equities \\n(see Exhibit 23) and the US dollar since the GFC. \\nThe US dollar has appreciated 52% since its trough \\nin April 2008. \\n 2024 was another strong year\\n, with US equities \\noutperforming non-US developed and emerging \\nmarket equities (see Exhibit 24). The US dollar \\ncontinued its upward climb and appreciated 7%. \\n Given our view of US preeminence and the \\noutperformance of US equities,\\n many clients—\\nespecially those in the US—are asking why they \\nshould allocate any public market assets to non-US \\ndeveloped and emerging market equities. \\n Let’\\ns begin with a review of ISG’s current \\nallocation in a moderate-risk taxable portfolio \\nfor US-based investors. While we have 72 model \\nportfolios for different risk profiles, tax statuses, \\nand geographic and currency preferences, and \\nwhile we recommend portfolios be customized \\nfor every client, we use the moderate-risk model \\nportfolio as an appropriate representation. \\n Our maximum overweight to US assets relative \\nto the MSCI \\nACWI was 23 percentage points in \\n2009 (see Exhibit 25). As US equities outperformed \\nnon-US equities, the overweight narrowed to seven \\npercentage points by year-end 2024. Including \\nthis overweight, US equities account for 74% of \\nthe public equity allocation in the current model \\nportfolio. \\n W\\ne do not believe it is appropriate to increase \\nthe overweight to match our high point of 23 \\npercentage points, nor do we plan to eliminate \\nnon-US equities altogether. However, we are \\nmaking some adjustments to our strategic asset \\nallocation. \\nExhibit 23: Annualized and Cumulative Equity \\nReturns Since March 2009 \\nUS equities have significantly outperformed since the trough \\nof the GFC. \\n17\\n13\\n9 9\\n8\\n7\\n0\\n3\\n6\\n9\\n12\\n15\\n18\\nUS India Non-US\\nDeveloped\\nEurozone EM China\\nAnnualized Total Return (%)\\nCumulative:\\n1,079%\\nTotal Value:\\n$1,179M\\nCumulative:\\n584%\\nTotal Value:\\n$684M\\nCumulative:\\n304%\\nTotal Value:\\n$404M \\nCumulative:\\n318%\\nTotal Value:\\n$418M \\nCumulative:\\n244%\\nTotal Value:\\n$344M \\nCumulative:\\n173%\\nTotal Value:\\n$273M\\nTotal Value of US$100 Million Invested on Trough of GFC\\nData as of December 31, 2024.  \\nNote: All non-US equity returns are measured by MSCI indices in US dollars. \\nSource: Investment Strategy Group, Bloomberg.\\nExhibit 24: 2024 Total Equity Returns \\nUS equities outperformed both non-US developed and \\nemerging market equities again in 2024.  \\n25\\n20\\n12\\n8\\n5\\n3\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nUS China India EM Non-US\\nDeveloped\\nEurozone\\nTotal Return (%)\\nData as of December 31, 2024.  \\nNote: All non-US equity returns are measured by MSCI indices in US dollars. \\nSource: Investment Strategy Group, Bloomberg.\\nExhibit 25: ISG Strategic Asset Allocation to US, \\nNon-US Developed and EM Public Equity\\nWe have increased our US overweight to 12 percentage \\npoints by reducing the exposure to public non-US equities. \\n42\\n65 67 74 79\\n45\\n30 23\\n24 20\\n13 5 10\\n3\\n7 12\\n1\\n0\\n20\\n40\\n60\\n80\\n100\\nMSCI ACWI ISG Model\\nPortfolio\\nMSCI ACWI Current ISG\\nModel Portfolio\\nNew ISG Model\\nPortfolio\\nAs of 2009 As of December 2024\\nUS Non-US Developed EM%\\nData as of December 2024. \\nNote: ISG model portfolio reflects the public equity breakdown of the ISG taxable moderate portfolio.  \\nSource: Investment Strategy Group, MSCI, Datastream.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d6b3c88b-ddf4-42e3-adf1-54966fd7f3ad', embedding=None, metadata={'page_label': '21', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='21Outlook Investment Strategy Group\\n We are reducing the exposure to public non-\\nU\\nS equities, in both developed and emerging \\nmarkets, and reallocating those investments to \\nprivate assets, which are predominantly composed \\nof US assets. This reallocation implicitly increases \\nthe US equity overweight from seven to 12 \\npercentage points.\\n This allocation improves the risk/return profile \\nof the portfolio as measured by the Sharpe ratio, \\nand it marginally improves the expected return.\\n \\nGiven our view that there is more alpha potential \\nin private assets, the potential for adding value \\nthrough manager selection has also increased. \\n The impact on a moderate-risk taxable \\nportfolio is shown in Exhibit 26. \\nThis reallocation \\nwill become effective at the end of the first \\nquarter of 2025.\\n Now,\\n some may ask why we don’t eliminate \\nall non-US equities, given that US companies are \\nthe best managed in the world (see Exhibit 27) \\nand have significant sales outside the US. As our \\ncolleague David Kostin, US equity strategist in \\nGlobal Investment Research, reports in the 2024 \\nPortfolio Passport, international sales account \\nfor 28% of S&P 500 revenues, with Asia-Pacific \\naccounting for 8% (of which China represents \\nonly 2%); Europe, Middle East and Africa making \\nup 11%; and the rest coming from Canada, Latin \\nAmerica and other regions.18\\nOur response is fivefold:\\n First, appropriate diversification is one of the \\nfive pillars of our investment philosophy\\n. Some \\nhave referred to diversification as one of the few \\nfree lunches in portfolio management. (Another \\nis compounding.) An appropriately diversified \\nportfolio provides clients with a better risk/return \\nprofile. Given that the correlation between US and \\nnon-US developed market equities since the GFC \\nis just 0.88, and the correlation between US and \\nemerging market equities is 0.75, both asset classes \\nprovide some nominal diversification benefits. The \\nuncertainty band around the expected return of a \\nportfolio is decreased. \\nSecond, while we are duly humble when putting \\nforth our annual economic and financial market \\noutlook,\\n we are certain that US equities and the \\ndollar will not repeat the outperformance of \\n2024—or that of the past nearly 16 years—over \\nthe next five years. As we show later in Exhibit 66, \\nwhen reviewing our one- and five-year expected \\nreturns, US and non-US developed market equities \\nare likely to have nearly identical returns, and \\nExhibit 26: ISG Model Portfolio\\nThe new allocation improves the risk/return profile of the \\nportfolio as measured by the Sharpe ratio.\\nUS Taxable Moderate Portfolio\\nCurrent Model  \\nPortfolio\\nNew Model \\nPortfolio\\nInvestment Grade Fixed Income 32.5% 32.5%\\nUS Investment Grade Municipal Bonds 32.5 32.5\\nOther Fixed Income 5.0% 5.0%\\nUS High Yield Municipal Bonds 5 5\\nPublic Equity 38.0% 35.5%\\nUS All-Cap Equity 28 28\\nNon-US Developed Equity 9 7 -2\\nEmerging Market Equity 1 0.5 -0.5\\nHedge Funds 3.0% 3.0%\\nEvent Driven 1 1\\nEquity Long/Short 1 1\\nTactical Trading 1 1\\nPrivate Equity 15.5% 18.0%\\nBuyout 11 13 +2\\nGrowth 3.5 4 +0.5\\nVenture 1 1\\nOther Private Assets 6.0% 6.0%\\nPrivate Credit 2 2\\nCore Private Real Estate and Infrastructure 4 4\\nTotal 100.0% 100.0%\\nAfter-Tax Estimated Mean Return \\nAssuming 3.0% Risk-Free Rate 6.3% 6.4%\\nSharpe Ratio 0.53 0.54\\nVolatility 8.6% 8.6%\\nData as of December 2024. \\nSource: Investment Strategy Group. \\nExhibit 27: Average Management Scores\\nUS companies are the best managed in the world.\\n3.3 3.2 3.2\\n3.0 3.0\\n2.7 2.6\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\nUS Germany Japan France UK China India\\nScore\\nData as of 2024. \\nSource: Investment Strategy Group, World Management Survey.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='009d0807-77a1-449d-8b4c-351d4a52b9e4', embedding=None, metadata={'page_label': '22', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='22 Goldman Sachs january 2025\\nwe expect the dollar to modestly depreciate \\nover the next five years from its currently high \\nvaluations. Some non-US exposure will provide \\ndiversification without requiring investors to forgo \\nincremental returns. \\n Third, inevitably we will have periods of \\nUS underperformance sometime in the future. \\nAs shown in Exhibit 28, while the US has \\noutperformed over the long run, there have been \\nseveral multiyear periods when non-US developed \\nmarket equities (for which there is much longer \\nhistory than for emerging markets) outperformed \\nUS equities, including between January 2002 and \\nJune 2008. Non-US developed market equities \\noutperformed US equities by 89 percentage points \\ncumulatively, or nine percentage points annualized, \\nduring that period. \\n Another example of US equity underperformance \\nis the period between October 2001 and August \\n2008, when Indian equities outperformed US \\nequities by 470 percentage points cumulatively, or \\n26 percentage points annualized. India accounts \\nfor only 2% of the MSCI ACWI, compared to \\n24% for non-US developed market equities. \\nNevertheless, it provides an example of periods \\nwhen US equities have lagged another market by a \\nsignificant amount. \\n Fourth, there are numerous world-class \\ncompanies with significant market share globally \\nthat are outside the US. Most are concentrated in \\nhealth care, consumer discretionary, consumer \\nstaples, energy and materials. Below, we provide \\none high-name-recognition example from each of \\nthese sectors so clients can better understand why \\nwe do not think it is appropriate to categorically \\nand indefinitely eliminate such companies \\nfrom their portfolios. These companies were \\nselected from a list of the top 10 in each sector \\nbased on average net income over the past three \\nyears. ISG does not make any individual stock \\nrecommendations. \\n Examples:\\n• Novo Nordisk of Denmark, the maker of \\nweight-loss drugs Ozempic and Wegovy\\n• LVMH of France, the owner of brands such \\nas Louis Vuitton, Dior, Bulgari, Tiffany and \\nDom Perignon\\n• Nestlé of Switzerland, with cereal brands like \\nCheerios, chocolate brands such as KitKat and \\nBaci, water brands such as Perrier, ice cream \\nbrands such as Haagen-Dazs, and pet care \\nbrands such as Purina\\n• Shell, an energy exploration and production \\ncompany headquartered in London, with more \\ngas stations in the US than Exxon Mobil\\n• BHP Group of Australia, the largest metals and \\nmining company in the world \\nFinally, US equities are expensive relative to most \\nnon-US equities, as we discuss in more detail \\nnext. While we believe the valuation differential \\nis justified, we also do not think an increased \\nallocation to public US equities is warranted at this \\ntime. Recently, an amusing Financial Times article \\nreferred to US equities in the context of “Tina”—\\n“there is no alternative” but US equities.19 Much of \\nthe good news has been priced in.\\n We now turn to US and non-US equity \\nvaluations. \\nStaying Invested in US Equities Versus \\nNon-US Equities\\nOur non-US clients are asking us a different \\nquestion than our US clients—the exact opposite. \\nThey want to know why, given the relative \\ncheapness of non-US equities, isn’t ISG shifting \\naway from US equities to non-US equities—even if \\nonly on a tactical basis? \\nExhibit 28: Total Return of US vs. Non-US \\nDeveloped Equities\\nThere have been several multiyear periods when non-US \\ndeveloped equities outperformed.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n1925 1933 1941 1949 1957 1965 1973 1981 1989 1997 2005 2013 2021\\nRelative Performance\\nUS OutperformsUS Underperforms\\nData through December 31, 2024. \\nNote: We use non-US developed equities in the comparison since MSCI emerging markets indices \\nbegin only in 1988. US equities are based on the S&P 500 index and non-US developed equities \\nare based on the MSCI World ex-US index.  \\nSource: Investment Strategy Group, Global Financial Data, Datastream.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='59eb245e-fc25-4ab3-9e0a-f3b83c015eb1', embedding=None, metadata={'page_label': '23', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='23Outlook Investment Strategy Group\\n Non-US equities are indeed cheaper than US \\nequities. \\n We aggregate six different valuation metrics \\nto make long-term historical comparisons. Non-\\nUS developed market equities are trading at a \\nnear historic discount of 54% to US equities (see \\nExhibit 29). Among non-US developed market \\nequities, the UK is the cheapest major equity \\nmarket, at a 62% discount. Emerging market \\nequities are trading at a near historic discount of \\n61% (see Exhibit 30). Among emerging market \\nequities, China is the cheapest major equity \\nmarket, at a 63% discount. \\n This cheapness is broad-based across nearly \\nall the major equity markets. The one exception is \\nIndia, whose equities trade at a 6% discount to US \\nequities based on this combined metric. \\n The question we address below is whether \\nthese extreme discounts reflect a tactical \\ninvestment opportunity. First, we show that these \\ndiscounts do not accurately reflect the cheapness \\nin each country or region. Second, we explain why \\nwe believe that these discounts are justified based \\non each country’s or region’s economic prospects. \\nAssessing the Valuation Metrics\\nWhen we examine individual countries and regions \\nfor forward-looking investment decisions, we use \\nprice-to-forward earnings (one of the more widely \\nused valuation metrics). Based on this metric, the \\ndiscounts range from 33% for Japanese equities \\nto 53% for Chinese equities. India is at a slight \\npremium based on this one metric (see Exhibit 31).\\n However, each equity market has very different \\nexposures to different sectors. For example, the \\nbroad technology sector (information technology- \\nand technology-driven companies like Amazon, \\nGoogle and Alibaba)20 accounts for 30% of \\nS&P 500 earnings, as much as 69% in Taiwan \\nand as little as 1% in the UK. Broad technology \\nExhibit 31: Price-to-Forward Earnings Ratios\\nMost countries and regions trade at a discount to the US.\\n23\\n21\\n14\\n13\\n12 11\\n10\\n0\\n5\\n10\\n15\\n20\\n25\\nIndia US Japan Eurozone EM UK China\\nPrice/Forward Earnings Ratio (x)\\nData as of December 31, 2024.  \\nSource: Investment Strategy Group, FactSet.\\nExhibit 30: Emerging Market Equity Valuation \\nDiscount to US Equities\\nEmerging market equities are trading at a near historic \\ndiscount of 61% to US equities.\\n-67\\n13\\n-61\\n-35\\n-80\\n-70\\n-60\\n-50\\n-40\\n-30\\n-20\\n-10\\n0\\n10\\n20\\n1995 1999 2003 2007 2011 2015 2019 2023\\nPremium/Discount vs. US Equities (%)\\nEmerging Market Equities\\nLong-Term Average\\nData through December 31, 2024.  \\nNote: Valuation discount is based on price/trend earnings, price/peak earnings, price/trailing \\nearnings, Shiller CAPE and price/10-year average earnings. \\nSource: Investment Strategy Group, Datastream.\\nExhibit 29: Non-US Developed Equity Valuation \\nDiscount to US Equities\\nNon-US developed equities are trading at a discount of 54% \\nto US equities.\\n7\\n-35\\n-11\\n-54\\n-28\\n-60\\n-50\\n-40\\n-30\\n-20\\n-10\\n0\\n10\\n1992 1996 2000 2004 2008 2012 2016 2020 2024\\nPremium/Discount vs. US Equities (%)\\nNon-US Developed Equities\\nLong-Term Average\\nData through December 31, 2024.  \\nNote: Valuation discount is based on price/trend earnings, price/peak earnings, price/trailing \\nearnings, Shiller CAPE and price/10-year average earnings.  \\nSource: Investment Strategy Group, Datastream.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c081b7e4-4403-4667-977a-32c5f4737494', embedding=None, metadata={'page_label': '24', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='24 Goldman Sachs january 2025\\nis considered a growth sector and is the most \\nexpensive sector in the S&P 500, trading at a price-\\nto-forward earnings of 28. \\n In contrast, the energy sector is a slow-growing \\nsector of the US economy and is the cheapest \\nsector in the S&P 500. Energy accounts for 5% \\nof S&P 500 earnings, as much as 19% of MSCI \\nUK and as little as 1% of MSCI Japan. The energy \\nsector in the S&P 500 trades at a price-to-forward \\nearnings of 13. \\n As a result, equity markets with greater \\ntechnology exposure will be more expensive than \\nequity markets with greater energy exposure. To \\nevaluate cheapness in search of a tactical trading \\nopportunity, one needs to equalize the sector \\nexposures of different markets. After such an \\nadjustment, the non-US equity markets are not as \\ncheap as they first appear. \\n The market multiple for every country and \\nregion increases after a sector weight adjustment \\n(see Exhibit 32). For example, the Eurozone, which \\ntrades at a 39% discount to the US before any \\nsector weight adjustment, trades at a 23% discount \\nafter the adjustments are made. Similarly, India, \\nwhich trades at a slight premium to the US, carries \\na 29% premium after sector weight adjustments. \\n Non-US developed and emerging market \\nequities are not as cheap as they appear relative \\nto US equities after adjusting for significant \\ndifferences in the key sectors of each market. \\nThe Discounts Are Justified \\nWe believe that the overall level of discounts \\nis justified. Non-US equities should trade at a \\ndiscount to US equities: \\n• US economic trend growth is higher than that \\nof developed economies and some emerging \\nmarket economies.\\n• The US is likely to get a limited but still positive \\nboost to GDP from an increase in AI-driven \\nproductivity sooner than any other country, and \\na larger or equal boost compared with other \\ncountries over time. \\n• Earnings per share (EPS) growth in the US \\nhas exceeded that of all other major countries \\nexcept India; adjusted for India’s substantially \\nhigher inflation, US EPS growth has exceeded \\nthat of India as well. \\n• EPS growth rates have been less volatile in the \\nUS than EPS growth elsewhere. \\n• The US has the largest exposure to the broad \\ntechnology sector and other growth industries \\nsuch as biotechnology and health-care \\nequipment for robotics surgery, so we expect \\nUS EPS growth rates to exceed those of most \\nother countries. \\n• US companies have less exposure to China’s \\neconomic slowdown. \\n• US equity markets provide more downside \\nprotection during market downdrafts.\\n \\nExhibit 32: Price-to-Forward Earnings Ratios\\nValuations for every country and region increase after adjusting for differences in sector weights.\\n21\\n23\\n28\\n14\\n17\\n13\\n16\\n12\\n15\\n11\\n16\\n10\\n15\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nUS India India with\\nS&P 500\\nEarnings Weights\\nJapan Japan with\\nS&P 500\\nEarnings Weights\\nEurozone Eurozone with\\nS&P 500\\nEarnings Weights\\nEM EM with\\nS&P 500\\nEarnings Weights\\nUK UK with\\nS&P 500\\nEarnings Weights\\nChina China with\\nS&P 500\\nEarnings Weights\\nPrice/Forward Earnings Ratio (x)\\nData as of December 31, 2024.  \\nSource: Investment Strategy Group, FactSet.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0d0fe3fa-3d08-4011-9a2b-68577a84bbee', embedding=None, metadata={'page_label': '25', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='25Outlook Investment Strategy Group\\nEconomic Trend Growth: The US trend growth \\nrate is estimated at 1.9%, higher than that of any \\nother major developed economy. UK trend growth \\nis 1.4%, the Eurozone is 1.2% and Japan is 0.6%. \\nSurprisingly, US growth is also favorable relative to \\nthat of many emerging market countries. China’s \\ntrend growth is on a downward trajectory, as \\nwe discuss below. Russia, ensnared in war with \\nUkraine, has a trend growth estimated at 1.2%. \\nBrazil’s trend growth rate is higher, at 2.5%.\\n The US is also the only major country whose \\neconomy has recovered from the pandemic hit \\nand returned to growth at its pre-pandemic trend \\nlevel (see Exhibit 33). We do not expect any of the \\ncountries shown in Exhibit 33 to close the gap to \\ntheir pre-COVID trend levels. \\nImpact of AI on Economic Trend Growth: \\nAccording to our colleague Joseph Briggs of \\nGoldman Sachs Economics Research, US trend \\ngrowth will increase by 0.1% from the impact of \\nAI starting in 2027. That growth boost will reach \\n0.4% by 2032. Briggs and the Economics Research \\nteam are forecasting US GDP growth of 2.3% \\nbetween 2031 and 2035, helped by the AI boost to \\nproductivity.21\\n The boost from AI will increase GDP in the \\nEurozone, UK and Japan starting in 2028. It will \\nreach 0.3% by 2034 in the Eurozone and 2033 in \\nthe UK and Japan. AI’s boost to China’s growth will \\nstart at 0.1% in 2028 and reach 0.2% by 2034. \\n Some AI observers argue that AI is unlikely \\nto have this level of impact on economies for at \\nExhibit 33: GDP vs. Pre-Pandemic Trend \\nThe US is the only major country whose economy has resumed growth to its pre-pandemic trend level.\\n70\\n75\\n80\\n85\\n90\\n95\\n100\\n105\\n110\\n115\\n120\\n2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\\nReal GDP (Q4 2019 = 100)\\nForecast\\nUS\\nUS Pre-Pandemic Trend\\n70\\n75\\n80\\n85\\n90\\n95\\n100\\n105\\n110\\n115\\n120\\n2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\\nReal GDP (Q4 2019 = 100)\\nForecast\\nEurozone\\nEurozone Pre-Pandemic Trend\\n70\\n80\\n90\\n100\\n110\\n120\\n130\\n140\\n150\\n160\\n2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\\nReal GDP (Q4 2019 = 100)\\nForecast\\nChina\\nChina Pre-Pandemic Trend\\n70\\n75\\n80\\n85\\n90\\n95\\n100\\n105\\n110\\n2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\\nForecast\\nReal GDP (Q4 2019 = 100)\\nJapan\\nJapan Pre-Pandemic Trend\\nData through Q3 2024. Forecast through 2025. \\nSource: Investment Strategy Group, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision and may change as economic and market conditions change. There can be no \\nassurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='bcf77947-e6ff-4fe6-80e8-0012b3b9cd86', embedding=None, metadata={'page_label': '26', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='26 Goldman Sachs january 2025\\nleast 10 years. Others argue that these estimates \\nare understated. The Global Investment Research \\n(GIR) Top of Mind report on generative AI from \\nJune 2024 provides an extensive review of the \\ndifferent perspectives.22\\n Irrespective of anyone’s bullishness or \\nbearishness toward AI, the US is likely to be the \\ngreatest beneficiary of any positive impact on GDP. \\nEarnings Per Share Growth: In aggregate, US \\ncompanies have outearned their counterparts in \\ndeveloped and emerging economies. \\n Since 1992, earnings growth in the US has \\noutpaced earnings in non-US developed economies \\nby an annual average of 2.4 percentage points; the \\ngrowth rate was faster about 60% of the time. We \\nexpect US EPS growth rates to continue outpacing \\nthose of non-US developed economies by about \\ntwo percentage points over the next five years. \\n Similarly, earnings growth in the US has \\noutpaced earnings in emerging market economies \\nby an annual average of five percentage points; the \\ngrowth rate was faster about 60% of the time in \\nthis comparison as well. We expect US EPS growth \\nrates to be in line with those of emerging market \\ncountries over the next five years. \\n The earnings growth rate in the US has picked \\nup momentum since the GFC. US earnings are up \\n143% relative to their peak levels before the GFC. \\nEarnings in non-US developed markets are up \\n25%, and in emerging markets they are up 33%, \\nboth lagging the US by more than 100 percentage \\npoints over about 17 years (see Exhibit 34)!\\n We also examine the data on a more granular \\nlevel (see Exhibit 35). At first glance, it appears \\nthat India’s earnings have outpaced US earnings \\nover this period. However, the faster pace is driven \\nby substantially higher inflation in India. Adjusted \\nfor inflation, earnings growth in India has lagged \\nthat of the US by 50 percentage points. Adjusted \\nfor the Indian rupee depreciation, India lagged the \\nUS by 81 percentage points. \\n All other countries lagged the US pace of \\nearnings growth. Japan, as the next-fastest EPS \\ngrower, lagged the US by 44 percentage points, and \\nthe UK, showing a decline in EPS, lagged the US by \\n147 percentage points. \\nVolatility of Earnings Per Share Growth:  \\nUS companies have provided higher earnings \\ngrowth rates but also relatively low volatility (see \\nExhibit 36). Since 1995, the average volatility of \\nearnings growth rates in the US has been 15%. By \\ncontrast, non-US developed market equities have \\nhad an average volatility of 31%; emerging market \\nequities, 22%. \\n Japanese equities have experienced the \\nhighest earnings volatility due to their higher \\nexposure to cyclical industries and a relatively \\nrigid cost structure that prevents companies from \\nimplementing layoffs during economic downturns. \\nExhibit 34: Trailing-12-Month Earnings per Share \\nin Local Currency\\nThe earnings growth rate in the US has picked up \\nmomentum since the GFC.\\n243\\n129\\n125\\n133\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n2002 2004 2006 2008 2010 2012 2014 2016 2018 2020 2022 2024\\nPre-GFC Peak = 100\\nUS\\nWorld ex-US\\nNon-US Developed\\nEmerging Markets\\nData through Q3 2024.  \\nSource: Investment Strategy Group, Datastream.\\nExhibit 35: Trailing-12-Month Earnings per Share \\nin Local Currency\\nAdjusted for the rupee’s depreciation, India’s EPS growth \\nlagged that of the US by 81 percentage points. \\n243\\n307\\n162\\n199\\n127\\n124\\n96\\n-50\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n2002 2004 2006 2008 2010 2012 2014 2016 2018 2020 2022 2024\\nPre-GFC Peak = 100\\nUS\\nIndia\\nIndia (in US$)\\nJapan\\nChina\\nGermany\\nUK\\nData through Q3 2024.  \\nSource: Investment Strategy Group, Datastream.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d4bbdc9a-9191-4eb5-995b-d22493f2af79', embedding=None, metadata={'page_label': '27', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='27Outlook Investment Strategy Group\\nHigher Exposure to Faster-Growing Sectors:  \\nThe S&P 500 has greater exposure to faster-\\ngrowing sectors of the economy, which implies \\nhigher long-term earnings growth. We have defined \\ngrowth sectors as:\\n• Semiconductors and semiconductor equipment\\n• Software and services\\n• Technology hardware and equipment\\n• Biotechnology \\n• Health-care equipment and supplies\\n• Broadline retail (e.g., Amazon)\\n• Interactive media and services (e.g., Alphabet)\\nThese sectors contribute 35% of S&P 500 \\nearnings, compared to much lower levels in the \\nEurozone, the UK, Japan, and China and other \\nemerging market countries (see Exhibit 37). \\nHigher exposure will support faster EPS growth \\nin the US. \\nLess Exposure to China’s Economic Slowdown: \\nUS equities are less exposed to the economic \\nslowdown in China. As highlighted in GIR’s 2024 \\nPortfolio Passport, only 2% of S&P 500 sales \\nare to China.23 There is wide variation across US \\ncompanies in their level of sales to China. \\n Other countries and regions have much higher \\nexposure. Eurozone equities derive 6% of their \\nrevenues from China, and Japanese equities derive \\n9%. Emerging market equities (ex-China) derive \\nabout 7% of their revenues from China (see \\nExhibit 38).\\n The US economy also has lower exposure to \\nChina through its low level of exports. US exports \\nto China represent 0.6% of GDP, while exports \\nto China account for 1.6% of Eurozone GDP \\nand 3.8% of Japan’s GDP. The level of exports \\nExhibit 36: EPS Growth Volatility\\nUS companies have provided higher earnings growth at \\nlower volatility. \\n145\\n32 31 27 22 22 18 15\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\nJapan China EAFE Europe ex-UK EM UK India US\\n%\\nData as of 2024. \\nNote: EPS growth volatility is based on data since 1995. \\nSource: Investment Strategy Group, MSCI, Datastream.\\nExhibit 37: Share of Earnings in Fast-\\nGrowing Sectors\\nEarnings in fast-growing sectors contribute 35% of S&P \\n500 earnings.\\n35\\n27\\n24\\n9\\n8\\n2\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\nUS China EM ex-China Japan Eurozone UK\\n%\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg, PAD Database, MSCI. \\nExhibit 38: Share of Total Equity Market Revenues \\nOriginated in China \\nNon-US developed and other emerging markets have \\nsignificant sales exposure to China.\\n9\\n8 7\\n7\\n6\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nJapan Non-US Developed UK EM ex-China Eurozone\\n%\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, FactSet.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2c17775d-47a9-4b75-9403-e7a8910e5011', embedding=None, metadata={'page_label': '28', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='28 Goldman Sachs january 2025\\nto China is higher in resource-rich countries like \\nBrazil at 5.6% of GDP, Saudi Arabia at 6.0% and \\nAustralia at 8.9%. \\nMore Downside Protection in Downdrafts:  \\nFinally, US equities have experienced lower \\ndrawdowns than non-US developed and emerging \\nmarket equities. We examine the median and \\naverage downdrafts over different periods based \\non the inception date of different indices. Since \\n1990, a period for which we have data across the \\nUS, EAFE and EM equity markets, the median \\ndecline in the S&P 500 was 19%, compared to a \\n26% decline in EAFE equities and a 33% drop in \\nemerging market equities (see Exhibit 39).\\n The unique combination of better earnings \\ngrowth driven by higher economic trend growth, \\nlower volatility of earnings, greater exposure \\nto faster-growing sectors and better downside \\nprotection guides us to maintain our overweight to \\nUS assets and not shift to cheaper non-US equities \\non a tactical basis. \\nChina Is Tradable But Not Investable: China is \\none of the cheapest major equity markets. The \\ngovernment has launched a series of monetary and \\nfiscal stimulus measures to boost its GDP and has \\nannounced explicit directives to support the equity \\nmarket. Theoretically, those factors should make \\nChinese equities attractive for a tactical overweight. \\n However, we do not recommend a tactical \\noverweight. We have two reasons:\\n• China’s equity market has provided anemic \\nreturns while remaining extremely volatile. \\n• As discussed in our two China Insight \\npublications, Walled In: China’s Great \\nDilemma (published in January 2016) and \\nMiddle Kingdom: Middle Income (published in \\nJanuary 2022), we believe that China will, at \\nbest, follow Japan’s trajectory since 1990. \\nWe briefly examine the data. \\nAnemic Equity Returns With High Volatility\\nSince the inclusion of China in the MSCI EM index \\nin September 1996, China has had an annualized \\nprice return of 0.4% compared to 7.6% for the \\nUS. Adding China’s higher dividends, Chinese \\nequities returned 2.9%, compared to 9.6% for \\nUS equities. US equities have outperformed by 6.7 \\npercentage points.\\n Not only have Chinese equities lagged US \\nequities, but they have been 54% more volatile as \\nmeasured by the standard deviation of returns (see \\nExhibit 40). \\n Chinese equities have also exhibited volatility \\nin nearly every calendar year since 1996. Chinese \\nequities have had, on average, peak-to-trough \\ndeclines of 29% and trough-to-peak rallies of \\nExhibit 40: MSCI China vs. S&P 500 Risk and \\nReturn Characteristics Since September 1996\\nChinese equities have not only lagged US equities in \\nperformance but also been 54% more volatile.\\n0.4\\n2.8\\n29.1\\n7.8\\n9.7\\n18.9\\n0.02\\n0.40\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\nAnnualized Price\\nReturn\\nAnnualized Total\\nReturn\\nVolatility of Daily\\nReturns\\nSharpe Ratio (Right)\\n%\\nMSCI China\\nS&P 500\\nData as of December 31, 2024.  \\nSource: Investment Strategy Group, Datastream.\\nExhibit 39: Non-US Developed and EM Equity \\nPerformance During US Equity Drawdowns\\nUS equities have experienced lower drawdowns than non-\\nUS developed and emerging market equities.\\nPeak-to-Trough Total Return\\nS&P 500 Index\\nMSCI EAFE \\nIndex MSCI EM Index\\nMedian (Since 1957) -20% - -\\nAverage (Since 1957) -27% - -\\nMedian (Since 1973) -20% -26% -\\nAverage (Since 1973) -28% -30% -\\nMedian (Since 1990) -19% -26% -33%\\nAverage (Since 1990) -28% -31% -34%\\nData as of December 2024. \\nNote: Performance is measured for S&P 500 drawdowns of 15% or more. Returns are measured \\nin US dollars.  \\nSource: Investment Strategy Group, Bloomberg.  \\nPast performance is not indicative of future results. \\n \\n ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d17ed1cb-0f0d-409e-a367-aa9c3668ed3c', embedding=None, metadata={'page_label': '29', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='29Outlook Investment Strategy Group\\n49%, which are double those of the S&P 500 (see \\nExhibit 41).\\n China’s risk-adjusted returns, as measured by \\nthe Sharpe ratio of 0.02, are lower than those of \\nany asset class in our model portfolios. \\n Although not investable, Chinese equities \\nare tradable for those who have the skills and \\nrisk appetite to actively trade stocks. The higher \\nvolatility provides an opportunity for an aggressive \\ntrader to take advantage of large price moves—or, \\nalternatively, lose a lot of money. \\nA Japan-Style Slowdown at Best \\nIn our base case, to which we assign an 80% \\nprobability, China follows Japan’s downward \\ngrowth trajectory. Since the peak in Japan’s \\nworking-age population in 1995, Japan’s average \\nannual GDP growth rate has been 0.7%. From \\ntheir peak in December 1989, Japanese equities \\nhad an initial price decline of 66%. They did not \\nrecover to this peak level for 34 years.\\n There is some upside potential if China embraces \\nthe reforms outlined in the Third Plenum of 2013 \\n(see our Insight publications for a detailed review \\nof the reforms), but because there has been minimal \\nprogress on any of the reforms, and backsliding on a \\nfew of them, over the last 11 years, we assign a 5% \\nprobability that China surprises to the upside. \\n We assign a 15% probability to our downside \\nscenario, in which China pursues an even more \\naggressive policy toward its two largest export \\nmarkets—the US and Europe—while also following \\na Soviet-era economic policy of “guns over butter.” \\n First, though, we must dispel the notion that \\nChina has achieved miraculous growth. \\n In a Foreign Affairs article titled “The Myth \\nof Asia’s Miracle: A Cautionary Fable,” Nobel \\nLaureate Paul Krugman wrote: \\n  Once upon a time, Western opinion leaders \\nfound themselves both impressed and frightened \\nby the extraordinary growth rates achieved by \\na set of Eastern economies. Although those \\neconomies were still substantially poorer and \\nsmaller than those of the West, the speed with \\nwhich they had transformed themselves from \\npeasant societies into industrial powerhouses, \\ntheir continuing ability to achieve growth rates \\nseveral times higher than the advanced nations, \\nand their increasing ability to challenge or even \\nsurpass American and European technology \\nin certain areas seemed to call into question \\nthe dominance not only of Western power \\nbut of Western ideology. The leaders of those \\nnations did not share our faith in free markets \\nor unlimited civil liberties. They asserted with \\nincreasing self-confidence that their system was \\nsuperior: societies that accepted strong, even \\nauthoritarian governments and were willing to \\nlimit individual liberties in the interest of the \\ncommon good, take charge of their economies, \\nand sacrifice short-run consumer interests for \\nthe sake of long-run growth would eventually \\noutperform the increasingly chaotic societies of \\nthe West.24\\nA cursory reading might make one think Krugman \\nwas writing about China today or perhaps even \\nJapan. He actually wrote this paragraph in 1994 \\nabout the early 1960s Soviet Union. Krugman \\nbelieved that the rapid growth in output in the \\nSoviet era was achieved by “rapid growth in \\ninputs: expansion of employment, increases in \\neducation levels, and, above all, massive investment \\nin physical capital.” He then concluded that the \\nrapid growth that was being witnessed in the \\n“newly industrializing countries of Asia” (e.g., \\nKorea and Taiwan) was driven by the same \\nextraordinary growth in inputs and not much \\nmore. “If there is a secret to Asian growth, it is \\nsimply deferred gratification, the willingness to \\nsacrifice current satisfaction for future gain,” \\nExhibit 41: MSCI China vs. S&P 500—Drawdowns \\nand Rallies\\nAverage peak-to-trough declines and trough-to-peak rallies \\nof Chinese equities have been double those of the S&P 500.\\nMSCI China\\nS&P 500\\n-29\\n49\\n-15\\n25\\n-40\\n-20\\n0\\n20\\n40\\n60\\n1996–2024 Average Maximum Peak-to-Trough\\nLoss Within Each Calendar Year\\n1996–2024 Average Maximum Trough-to-Peak\\nGain Within Each Calendar Year\\n%\\nData as of December 31, 2024.  \\nSource: Investment Strategy Group, Datastream. \\nPast performance is not indicative of future results.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='87b0737c-7ea3-42e8-aecc-cce16cc8367a', embedding=None, metadata={'page_label': '30', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='30 Goldman Sachs january 2025\\nhe wrote. He specifically warned that observers \\nshould not extrapolate Japan’s higher growth rates \\ninto the future. \\n The same has held true for the East Asian \\nTiger economies. As we have noted before, their \\neconomies relied on an export-led growth model \\nsupported by high levels of investment, cheap \\ncurrencies and cheap labor. But after rapid growth \\nin the early years of development, their growth \\nrates slowed, as shown in Exhibit 42.\\n Harvard University Professors Lant Pritchett and \\nLawrence Summers have argued that history will \\nbear down on China’s growth rates. In “Asiaphoria \\nMeets Regression to the Mean,” a 2014 NBER \\nworking paper, they argued that “regression to \\nthe mean is perhaps the single most robust and \\nempirical relevant fact about cross-national growth \\nrates.”25 The cross-country historical average has \\nbeen 2% with a standard deviation of 2%.\\n We believe that China will not escape the \\nslowdown to a trend growth rate of 2%.\\n China faces considerable headwinds to its \\ngrowth, and it is facing these headwinds from a far \\nless advantageous position than Japan, which was \\na richer and more developed economy in the 1990s \\nthan China is today. The headwinds are: \\n• Weak demographics \\n• Low levels of education \\n• Stalled reforms \\n• Rising debt \\n• Policy uncertainty \\n• Economic growth taking a backseat \\n• Less favorable geopolitical backdrop\\n• Real estate correction \\nWe compare China to Japan across some of these \\nheadwinds to explain why we think China’s growth \\ntrajectory will, at best, follow that of Japan’s. \\nWeak Demographics: China’s total population \\npeaked in 2021, whereas its working-age \\npopulation peaked earlier, in 2015, and its current \\ndemographic profile is less favorable than that of \\nJapan when Japan’s economy peaked. For example, \\nits population growth has slowed more rapidly \\nand turned negative much more quickly than \\nJapan’s did. China’s population growth rate in \\n2018 was 0.4%, which was the same as Japan’s in \\n1991, and it has been declining at a faster rate (see \\nExhibit 43). \\n China’s population is also aging at an earlier \\nstage of China’s economic development (as \\nmeasured by the increase in the percentage of \\nthe population over 65) than Japan’s. However, \\nChina’s GDP per capita is substantially lower \\nthan that of Japan in 1990 (see Exhibit 44). China \\nwill face greater demand for social safety nets \\nand health-care services, which will put a heavier \\nburden on central and local governments. \\nExhibit 42: GDP per Capita vs. GDP per Capita Growth in Subsequent 5 Years \\nEast Asian Tigers saw rapid growth in the early years of development, but then their growth rates slowed significantly.\\nChina\\nChina ISG Forecast\\nKorea\\nTaiwan\\nHong Kong\\nSingapore\\nJapan\\n-2\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\nReal GDP per Capita Growth in Subsequent 5 Years (% YoY) \\n5,000 10,000 15,000 20,000 25,000 30,000\\nReal GDP per Capita (PPP*)\\nData through 2023. China forecast through 2034.  \\nNote: X-axis is limited to a range between 5,000 and 33,000. \\nSource: Investment Strategy Group, Penn World Table. \\n* Purchasing power parity.  \\nForecasts are estimated, based on assumptions, are subject to revision and may change as economic and market conditions change. There can be no \\nassurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8b679266-6b68-414b-93dc-2c3d602514a1', embedding=None, metadata={'page_label': '31', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"31Outlook Investment Strategy Group\\nLow Levels of Education: As shown earlier in \\nExhibits 11 and 12, China’s population has low \\nlevels of education. Its average years of schooling \\nare below those of Mexico, and its share of \\nworking-age population with tertiary education is \\nbelow that of India. Importantly, China’s human \\ncapital, a contributor to economic productivity, is \\n84% of Japan’s level in 1991. Its labor productivity \\nper hour worked is 51% of Japan’s in 1991. \\nStalled Reforms: China’s reform agenda has stalled \\nat a time when China is still a middle-income \\ncountry with a low level of GDP per capita. On a \\nPPP-adjusted basis, China’s GDP per capita is 30% \\nof US GDP per capita; Japan’s was 70% of US \\nGDP per capita in 1991. Japan was and remains \\nsubstantially wealthier (see Exhibit 45). \\n One of the goals of the 2013 reform agenda \\nwas to enhance the role of the private sector and \\nimprove the efficiencies of state-owned enterprises \\n(SOEs). The opposite has occurred, with SOEs \\nplaying a far greater role in the economy. In China, \\nthe state owns anywhere from 82% to 90% of \\nthe largest banks. Overall, it is estimated that the \\ngovernment owns nearly a quarter of the market \\ncapitalization of MSCI China stocks. \\n Such large government ownership does not \\nfavor equity investors. The World Management \\nSurvey from the Stanford Institute for Economic \\nPolicy Research has consistently ranked government \\nownership as the least effective ownership structure \\nin terms of corporate performance. \\n One of the biggest drawbacks from the stalled \\nreforms is the absence of price discovery. In such \\na command and control economy, there are not \\nExhibit 43: Population Growth—China vs. Japan\\nChina’s population growth has been declining at a faster \\nrate since 2018 than Japan's since 1991. \\nJapan China\\n1991\\n0.4\\n-0.5\\n2018\\n0.4\\n-0.1\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n1975 1979 1983 1987 1991 1995 1999 2003 2007 2011 2015 2019 2023\\n%\\nData through 2023.  \\nSource: Investment Strategy Group, Haver Analytics, United Nations. \\n \\nExhibit 45: GDP per Capita—China vs. Japan \\nChina is still a middle-income country with a low level of \\nGDP per capita. \\nJapan\\nChina\\n0\\n5,000\\n10,000\\n15,000\\n20,000\\n25,000\\n30,000\\n35,000\\n40,000\\n45,000\\n50,000\\n1990 1994 1998 2002 2006 2010 2014 2018 2022\\nGDP per Capita (2021 US$, PPP*)\\nJapan 1990: 35,466\\n70% of the US\\nChina 2023: 22,135\\n30% of the US\\nData through 2023.  \\nSource: Investment Strategy Group, Haver Analytics, World Bank. \\n* Purchasing power parity.\\nExhibit 44: Aging Population and GDP per Capita—\\nChina vs. Japan  \\nChina is aging at a much lower level of GDP per capita \\ncompared to Japan. \\nJapan China\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n0 10,000 20,000 30,000 40,000 50,000\\nGDP per Capita (2021 US$, PPP*)\\nAging Population as Share of Total (%)\\nJapan 1990: \\nGDP per Capita: 35,466\\nShare of Aging Population: 12.2%\\nChina 2023:\\nGDP per Capita: 22,135\\nShare of Aging Population: 14.3%\\nData through 2023. \\nNote: Aging population is defined as 65 years and older.  \\nSource: Investment Strategy Group, Haver Analytics, United Nations, World Bank. \\n* Purchasing power parity.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ca45f300-5a2c-4ccb-a7e6-bf0660d553ae', embedding=None, metadata={'page_label': '32', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='32 Goldman Sachs january 2025\\nenough market signals to alert leadership to the \\ndeteriorating economic environment in China and \\nthe continued inefficient allocation of capital. \\n In our efforts to better estimate China’s \\ngrowth trajectory and compare it to other \\nmajor economies that have declined over the \\nlast several decades, we spoke to Joseph S. Nye \\nJr., Harvard University Distinguished Service \\nProfessor, Emeritus. He was dean of the John \\nF. Kennedy School of Government, deputy to \\nthe undersecretary of state from 1977 to 1979 \\nand assistant secretary of defense from 1994 to \\n1995. Professor Nye has also been recognized as \\none of the most influential scholars in American \\nforeign policy. His feedback was “the greatest \\ncommon mistake [for both China and the Soviet \\nUnion] is narrowing down the political process \\nto an autocracy where self-correcting criticism is \\nimpossible.”26\\nRising Debt: China has funded its growth with \\nincreasing levels of debt. China’s total debt-to-\\nGDP ratio has increased to almost 300%. The rate \\nof increase is faster than what Japan experienced \\nprior to 1991. The difference is mostly in the \\ngovernment sector. China’s augmented government \\ndebt-to-GDP currently stands at 104% (see Exhibit \\n46). That compares to an average of 64% in 1991 \\nand 1992 in Japan. \\nLess Favorable Geopolitical Backdrop: Compared \\nwith Japan in the 1990s, China is facing a much \\nless favorable geopolitical backdrop as it seeks to \\nincrease exports to reach its GDP target.  \\n Japan and China each accounted for about half \\nof the US trade deficit at their respective economic \\npeaks in 1991 and 2015. Japan’s trade disputes \\nwith the US were addressed through negotiations \\nExhibit 46: China’s Augmented Government Debt-\\nto-GDP Ratio \\nChina’s augmented government debt-to-GDP ratio currently \\nstands at 104%.\\n104\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n2005 2009 2013 2017 2021\\n% of GDP\\nData through Q2 2024. \\nSource: Investment Strategy Group, Goldman Sachs Global Investment Research, Ministry of \\nFinance of the People’s Republic of China, Wind, Bloomberg, CEIC.\\nExhibit 48: Activities Related to Real Estate as \\nShare of GDP\\nChina’s investment in real estate as a share of GDP has \\nexceeded all other real estate bubbles of the 21st century.\\n2015\\n26.4%\\n2006\\n22.2%\\n5\\n10\\n15\\n20\\n25\\n30\\n2000 2002 2004 2006 2008 2010 2012 2014 2016 2018\\n% of GDP\\n2006–2007\\n25.8%\\n2005\\n19.2%\\nUS\\nChina\\nSpain\\nIreland\\nData through 2018.  \\nSource: Investment Strategy Group, Kenneth Rogoff and Yuanchen Yang, “Rethinking China’s \\nGrowth,” Economic Policy 39, July 2024.\\nExhibit 47: China’s Export Destinations\\nExports to the US and the Eurozone account for 27% of \\nChina’s exports.\\nUS\\n15%\\nEurozone\\n12%\\nOther DM\\n29%\\nEM (ex-China)\\n44%\\nData as of September 2024. \\nNote: Numbers are based on export data over the past 12 months.  \\nSource: Investment Strategy Group, Haver Analytics, IMF.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='28932c5d-db46-4a3e-8720-0c1bc6b4dce1', embedding=None, metadata={'page_label': '33', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='33Outlook Investment Strategy Group\\nrelated to currency appreciation, market openness \\nand voluntary export restrictions. Ideologically and \\ngeopolitically, the US and Japan were aligned. \\n In contrast, it is unlikely that China’s current \\ntrade issues with the US and Europe can be \\neasily resolved. National security considerations; \\nideological differences over the South China Sea \\nand Taiwan; and alliances with Russia, Iran and \\nNorth Korea do not provide a favorable backdrop \\nfor China to increase its exports to the US and \\nEurope, which together account for 27% of \\nChina’s exports (see Exhibit 47). \\nReal Estate Correction: China’s investment in real \\nestate as a share of GDP has exceeded all other \\nreal estate bubbles of the 21st century globally. The \\nnext highest share of GDP was in Spain, followed \\nby Ireland (see Exhibit 48). \\n All real estate market bubbles have \\nexperienced large and prolonged property \\nmarket corrections. The average price \\ndecline was 41% and the average duration \\nof the decline was more than eight years. \\nJapan’s price decline was 42%, with a \\ndecline lasting 15 years. China’s property \\nmarket correction to date is only 11.5% \\nbased on the 70-city property price \\nindex for new and existing homes, and \\nthe decline has lasted just over three \\nyears. It is likely that China’s property prices have \\nfurther to drop. \\n As Kenneth Rogoff and Yuanchen Wang \\nconclude in a recent article: “It is now painfully \\nclear that China is not as different as most \\nscholars still thought just five years ago. Like \\nmany other countries in the past, it too is facing \\nthe difficult challenge of countering the profound \\ngrowth and financial effects of a sustained real \\nestate slowdown.”27\\n If China follows the path of Japan, its equity \\nmarket will also be more tradable than investable. \\nJapanese equities did not exceed their December \\n1989 peak until July 2024—nearly 34 years later. \\nWhile not investable, Japanese equities provided \\nseveral opportunities for tactical asset allocation \\n(see Exhibit 49).\\nExhibit 49: Tokyo Price Index (TOPIX) \\nJapanese equities have provided opportunities for tactical \\nasset allocation since the peak in December 1989.\\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n3,500\\n1970 1975 1980 1985 1990 1995 2000 2005 2010 2015 2020\\n321%\\n+136%+79%\\nIndex\\n2013: Abenomics\\n1985: Plaza Accord\\n1990: Asset Price Bubble\\n-66%\\n-62%\\n-56%\\n2000:\\nDot-Com Bubble\\n2007: GFC\\n2003: Japanese\\nCommercial Code Reform\\n2021:\\nCorporate\\nGovernance\\nReform\\nData through December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg. \\nPast performance is not indicative of future results. \\n \\nExhibit 50: China’s GDP Growth Rate\\nWe expect China’s GDP growth to decline to an average of \\n3% over the next 10 years.\\n12.8\\n4.3\\n7.7\\n3.0\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n2001 2004 2007 2010 2013 2016 2019 2022 2025 2028 2031\\n% YoY \\nGDP Growth (3-Year Moving Average)\\nAverage Annual GDP Growth 2010–19\\nAverage Annual GDP Growth 2025–34\\nForecast\\nData through Q3 2024. Forecast through 2034. \\nSource: Investment Strategy Group, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nIf China follows the path of Japan, \\nits equity market will also be more \\ntradable than investable. Japanese \\nequities did not exceed their \\nDecember 1989 peak until July 2024 \\nnearly 34 years later.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='404e2ce0-16c4-48e7-927f-bf28278a6cf4', embedding=None, metadata={'page_label': '34', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='34 Goldman Sachs january 2025\\nChina’s Growth Trajectory: We expect China’s \\nGDP growth to decline from an annual average of \\n7.7% before COVID to an average of 3% over the \\nnext 10 years. We expect China to grow at 2.2% \\nby 2034 (see Exhibit 50). \\n Of course, we should note that there is \\nconsiderable debate about China’s current growth \\nrates, and the range forecast by various China \\nexperts is wide (see Exhibit 51).\\n We conclude that China, even as the cheapest \\nequity market among large economies and non-\\nUS developed equity markets, does not present a \\ntactical asset allocation opportunity away from US \\nequities. \\n We now turn to bonds and cash. We illustrate \\nwhy bonds and cash, likewise, do not provide a \\ntactical asset allocation opportunity away from US \\nequities. \\nStaying Invested in US Equities Versus \\nBonds or Cash\\nAs discussed earlier, US equities are expensive. \\nHowever, we do not recommend exiting US \\nequities in favor of bonds or cash. \\n We expect higher returns in US equities relative \\nto our expected returns for intermediate taxable \\nand tax-exempt bonds, German bonds and cash in \\nour base case, as shown later in Exhibit 35. We also \\nsee a higher probability that US equities will surprise \\nto the upside as they did in 2023 and 2024. \\n The argument put forth by some market \\nparticipants is that US equity valuations must \\ncompress over time from their current levels of 21 \\ntimes next 12 months forward earnings, growing \\ncloser to the long-term average of just over 16. We \\ndo not agree with that view. \\n The premise for our recommendation to stay \\ninvested is threefold:\\n• There is no evidence of mean reversion in \\nequity valuations; valuations do not have \\nto revert to any long-term mean over any \\nspecific horizon.\\n• Valuations alone are not a good signal for \\nexiting the market.\\n• Market concentration is not a good signal for \\nexiting the market. \\nMean Reversion: We first discussed the absence \\nof mean reversion in our 2013 Outlook: Over the \\nHorizon. We shared our analysis showing there \\nwas no statistical evidence of mean reversion in \\nequity valuations. \\n We also shared in 2013 that our expected \\nreturns for equities were driven primarily by \\nour outlook for economic growth in the US and \\nthe rest of the world, along with our estimates \\nfor earnings growth for the next 12 months. We \\nadjusted our return expectations based on our \\nviews of interest rates and inflation, our \\nexpectations for monetary and fiscal \\npolicy, and, importantly, our assessment \\nof the risk of recession. Mean reversion \\nwas not a driver of our annual return \\nexpectations. \\n Since then, the S&P 500 has returned \\n416%, or 14.6% annualized, well above \\nthe average expected returns for equities. \\n The evidence over the last 12 years \\nreconfirms the absence of mean reversion. \\nExhibit 51: China Alternative GDP Indicators and \\nEstimates \\nThere is considerable debate about China’s current \\ngrowth rates.\\n1.5\\n4.3\\n4.8\\n2.6\\n4.0\\n5.0\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nEmerging\\nAdvisors Group\\nChina Activity\\nIndex\\nCapital\\nEconomics\\nChina Activity\\nProxy\\nOfﬁcial\\nGDP\\nRhodium\\nGroup\\nAutonomous\\nResearch China\\nEconomic\\nActivity Composite\\nOfﬁcial Target\\nQ3 YTD YoY % 2024 YoY %\\nAlternative Activity Indicators vs. Ofﬁcial GDP\\nAlternative GDP Estimates vs. Ofﬁcial Target (Right)\\nData through Q3 2024. Estimates through Q4 2024. \\nSource: Investment Strategy Group, Autonomous Research, Capital Economics, Emerging \\nAdvisors Group, Rhodium Group, Haver Analytics.\\nThere is no evidence of mean \\nreversion in equity valuations; \\nvaluations do not have to revert to \\nany long-term mean over any  \\nspecific horizon.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='907bdce0-c79c-4178-a837-73c23c84feea', embedding=None, metadata={'page_label': '35', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='35Outlook Investment Strategy Group\\n We have analyzed eight different valuation \\nmetrics in the US, the UK, the Eurozone and Japan, \\nincluding the widely used Shiller CAPE, which \\nis often cited as a measure of dislocation from \\nlong-term averages. Across all eight metrics and \\nthe four countries or regions, we have not found \\nany statistical evidence of mean reversion, with the \\nexception of price-to-forward earnings in the UK. \\n The top panel in Exhibit 52 shows the path of \\nthe Shiller CAPE since 1881, and the bottom panel \\nshows statistical significance over time. Statistical \\nsignificance rarely reached the 95% threshold. Mean \\nreversion has been statistically significant just 1% of \\nthe time since 1930 based on 50-year rolling windows \\nand 2% of the time using 20-year rolling windows. \\nFar from indicating mean reversion, the statistical \\nevidence instead points to a regime shift toward \\nhigher valuations in recent decades (see Exhibit \\n53), which may have structural underpinnings (see \\nSection III, US Equities). Therefore, we are not \\ncompelled to lower our expected returns based on \\na hypothetical case for mean reversion. \\nValuation: History also shows us that valuations \\nalone are not a good signal for exiting the market. \\nSince US equities first entered the ninth decile of \\nvaluations in November 2013, the S&P 500 has \\nrallied about 300%. Since they entered the 10th \\ndecile of valuations, equities have returned over \\n200% (see Exhibit 54). \\n Valuations are not an effective signal on a \\nshort-term basis either. As shown in Exhibit 55, the \\nlevel of the Shiller CAPE has historically explained \\nonly 6% of the returns for the next calendar year. \\n Here, we are reminded of a conversation with \\nLarry Summers, former secretary of the Treasury \\nand president of Harvard University, in November \\n2023 at the Goldman Sachs Alternatives Summit. \\nExhibit 52: Shiller CAPE Ratio and Statistical Significance of Mean Reversion\\nThere is little statistical evidence that the Shiller CAPE is mean-reverting.\\nSeptember 1929\\n32.6\\nDecember 1999\\n44.2\\n37.7\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n1881 1891 1901 1911 1921 1931 1941 1951 1961 1971 1981 1991 2001 2011 2021\\nShiller CAPE (x)\\nMean-reversion statistical signiﬁcance\\nof the full 1881–2024 sample: 31%\\n \\n20-Year Rolling Statistical Signiﬁcance 50-Year Rolling Statistical Signiﬁcance 95% Threshold\\n13\\n22\\n0\\n20\\n40\\n60\\n80\\n100\\n1881 1891 1901 1911 1921 1931 1941 1951 1961 1971 1981 1991 2001 2011 2021\\nStatistical Signiﬁcance (%)\\nData through December 31, 2024. \\nNote: Statistical significance is based on the Augmented Dickey-Fuller (ADF) stationarity test with drift and no lags.  \\nSource: Investment Strategy Group, Robert J. Shiller.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c3cd4d39-09df-450d-b890-924cbba2280c', embedding=None, metadata={'page_label': '36', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='36 Goldman Sachs january 2025\\nHe highlighted that “trends are mostly good” and \\n“events are mostly bad.” He then added that the \\n“news is always about events, not about trends.”28 \\n We are focusing on the trend in earnings in the \\nUS, as shown in Exhibit 56. Earnings in the US \\nhave increased at an average rate of 6.5% post-\\nWWII, and the S&P 500 has tracked the path of \\nearnings. While a pandemic or geopolitical shock \\ncould derail the economy and the S&P 500 from \\ntheir upward paths, we recommend staying focused \\non the trend. \\nMarket Concentration: History also shows us \\nthat a high level of concentration of the top stocks \\nin the S&P 500 is not a good signal for exiting \\nthe market. Some observers have posited that if \\nequity market returns have been driven by a small \\nbasket of stocks that, in turn, account for a large \\nExhibit 53: Shiller CAPE Regimes\\nStatistical evidence points to a regime shift toward higher valuations.\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n1881 1891 1901 1911 1921 1931 1941 1951 1961 1971 1981 1991 2001 2011 2021\\nShiller CAPE (x) “Low” Regime\\n “High” Regime\\n Shiller CAPE\\nData through December 31, 2024. \\nNote: A regime switching model (Hidden Markov Model) is used to categorize historical Shiller CAPE into two regimes.  \\nSource: Investment Strategy Group, Robert J. Shiller.\\nExhibit 55: S&P 500 Shiller CAPE vs. Subsequent \\nCalendar-Year Total Return \\nStarting valuation multiples tell us little about potential \\nreturns over the next year.\\n-50\\n-40\\n-30\\n-20\\n-10\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n0 10 20 30 40 50\\nShiller CAPE (x)\\nS&P 500 Returns 1 Year Forward (%)\\nR2= 6%\\nCurrent CAPE Level \\nData as of December 31, 2024. \\nNote: Analysis is based on data since 1945. \\nSource: Investment Strategy Group, Bloomberg, Robert Shiller.\\nExhibit 54: S&P 500 Total Returns After Crossing \\ninto the 9th and 10th Deciles of Valuation\\nSince US equities entered the 9th decile of valuations in \\nNovember 2013, the S&P 500 has rallied by 300%.\\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n3,500\\n4,000\\n5,000\\n4,500\\n6,500\\n6,000\\n5,500\\n1990 1995 2000 2005 2010 2015 2020\\n9th Decile, Mar-92\\n10th Decile, Jul-95 9th Decile,\\nNov-13 \\n10th Decile,\\nDec-16\\n202%\\n299%\\n194%\\n342%\\nTotal Return\\nS&P 500\\nData through December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='90173f57-46ad-4906-a652-4a86dac79f5c', embedding=None, metadata={'page_label': '37', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='37Outlook Investment Strategy Group\\npercentage of equity market capitalization, then \\nsubsequent returns will be lower. We do not find \\nevidence to support that contention over tactical \\ninvestment horizons. \\n The top five stocks in the S&P 500 account \\nfor 29% of the index’s market capitalization. This \\nlevel is the highest since 1980 (see Exhibit 57). The \\nstocks are, in order of market capitalization, Apple, \\nNvidia, Microsoft, Amazon and Alphabet. \\n As can be seen in Exhibit 58, the level of \\nconcentration has no bearing on returns over the \\nnext 12 months. The R-squared, which explains the \\nvariance in equity returns that can be attributed to \\nthe level of concentration, is negligible, at 0.04%.\\n The results are the same if one uses alternative \\nmeasures of concentration, such as the top 10 stocks. \\n Even going back to the Great Depression, \\nmarket concentration has not been an effective \\ntiming signal for exiting the equity market. \\nS&P 500 returns one year after peak levels of \\nconcentration have been attractive in three of the \\nsix periods, with the three losses resulting from \\nrecessions (see Exhibit 59). Returns 10 years after \\npeak levels of concentration have been attractive \\nin five of the six periods. The one exception \\noccurred after the bursting of the dot-com bubble \\nand included the downdraft from the GFC (see \\nExhibit 59).\\n Our regression models—both univariate and \\nmultivariate—have shown that concentration is \\nnot a statistically significant variable in forecasting \\nreturns in the US. Neither is it statistically \\nsignificant in the UK, Germany, Japan or, at a \\nmore aggregate level, the Eurozone, EAFE and all \\ndeveloped markets.\\n Based on historical data and our quantitative \\nanalyses, we conclude that concentration is not an \\neffective signal for exiting the market. \\nExhibit 56: S&P 500 Price Index vs. Earnings\\nThe S&P 500 index has followed the path of earnings \\nover time. \\n50\\n500\\n5,000\\n50,000\\n1945 1950 1955 1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010 2015 2020\\nIndexed Value in Log Scale (1945 = 100)\\nS&P 500 Trailing 12-Month Operating Earnings\\nS&P 500 Price Index\\nData through Q3 2024. \\nSource: Investment Strategy Group, Bloomberg, S&P Global.\\nExhibit 57: Market Capitalization Weight of Top 5 \\nStocks in the S&P 500 \\nThe top five stocks in the S&P 500 account for 29% of the \\nindex’s market capitalization.\\n29\\n10\\n12\\n14\\n16\\n18\\n20\\n22\\n24\\n26\\n28\\n30\\n1980 1984 1988 1992 1996 2000 2004 2008 2012 2016 2020 2024\\n%\\nData through December 31, 2024.  \\nSource: Investment Strategy Group, Goldman Sachs Global Investment Research, Bloomberg.\\nExhibit 58: S&P 500 Concentration vs. Subsequent \\n1-Year Total Return\\nThe level of concentration has no bearing on returns over \\nthe next 12 months.\\nR2 = 0.04%\\n-60\\n-40\\n-20\\n0\\n20\\n40\\n60\\n80\\n10 12 14 16 18 20 22 24 26\\nS&P 500 Top 5 Stocks’ Market Capitalization Weight (%)\\nS&P 500 Total Returns 1 Year Forward (%)\\nData through December 31, 2024. \\nNote: Analysis is based on data since December 1979.  \\nSource: Investment Strategy Group, Goldman Sachs Global Investment Research, Bloomberg.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8f18fcf6-a794-4295-91cf-464190cfbf46', embedding=None, metadata={'page_label': '38', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='38 Goldman Sachs january 2025\\nThe Hurdle of Tax Impact: Taxable clients should \\nalso consider the impact of taxes on any decision \\nto sell equities with significant capital gains. As \\nshown in Exhibit 60, equity markets must fall \\nmeaningfully to offset the impact of state and \\nlocal taxes. For example, a hypothetical New York \\nCity taxpayer who invested at the trough of the \\npandemic on March 23, 2020, needs a 24% decline \\nin equities to offset the impact of taxes. \\n We also remind clients that our \\nrecommendation to stay invested should not \\nbe mistaken for complacency about market \\ndowndrafts. As we show in Exhibit 61, there is an \\n80% probability that the S&P 500 declines 10% \\nduring a one-year period when valuations are \\nhigh. The probability of a 15% decline is 49%. \\nHowever, the likelihood of a decline persisting \\nfrom the beginning of a one-year period through \\nits end drops significantly—to just 20% for a 10% \\ndecline and 14% for a 15% downdraft. These \\nprobabilities decrease even further when recessions \\nare excluded, to just 10% and 6%, respectively. \\nThe lower likelihood of sustained losses \\nunderscores that US equities are an appreciating \\nasset class over time.  \\n Clients must therefore have the right strategic \\nasset allocation customized to their risk tolerance \\nlevel so they can withstand the inevitable volatility \\nthat we will face in 2025. \\nExhibit 59: Previous S&P 500 Peak Concentrations \\nvs. Subsequent Total Returns\\nS&P 500 returns after peak concentration levels have been \\nattractive most of the time.\\nSubsequent 1-Year Return Subsequent 10-Year Annualized Return\\n116\\n-1\\n16\\n-14 -9\\n54\\n10 9 6 10\\n-1\\n17\\n-40\\n-20\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\nMay ‘32 August ‘39 February ‘64 June ‘73 December ‘99 February ‘09\\n%\\nData through December 31, 2024. \\nNote: Concentration is measured by the market capitalization ratio of the largest stock to the \\n75% percentile stock. \\nSource: Investment Strategy Group, Goldman Sachs Global Investment Research, Bloomberg. \\nPast performance is not indicative of future results.\\nExhibit 61: S&P 500 1-Year Drawdown Probability \\nWhen Valuations Are High\\nThe probability of a 10% correction at any point over the \\ncourse of a year when valuations are high is 80%. \\n98\\n80\\n49\\n30\\n25\\n20\\n14\\n7\\n0\\n20\\n40\\n60\\n80\\n100\\n-5 -10 -15 -20\\nMagnitude of Drawdown (%)\\nProbability (%)\\nReturn from Peak to Trough Within a Year\\nReturn from Beginning to End of Year\\nData through December 31, 2024. \\nNote: Probability of drawdown is conditional on US equity valuations being in the 9th or 10th decile.  \\nSource: Investment Strategy Group, Bloomberg \\nPast performance is not indicative of future results. \\n \\nExhibit 60: Required Decline in US Equities to \\nOffset Tax Consequences of Selling\\nCapital gains taxes increase the hurdle to exit the equity \\nmarket for taxpaying investors.  \\n-23 -24\\n-21\\n-15-16 -16\\n-15\\n-10\\n-30\\n-25\\n-20\\n-15\\n-10\\n-5\\n0\\nHypothetical California\\nTaxpayer\\nHypothetical New\\nYork City Taxpayer\\nHypothetical New\\nYork Taxpayer\\nHypothetical Florida\\nTaxpayer\\n%\\nInvested at the COVID Trough (March 23, 2020)\\nInvested at the Pre-COVID Peak Level (February 19, 2020)\\nData as of December 31, 2024.  \\nNote: Required decline is based on S&P 500 price returns.  \\nSource: Investment Strategy Group, Datastream.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ad99c813-272c-42aa-b9d2-fa59ca3ddc3d', embedding=None, metadata={'page_label': '39', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='39Outlook Investment Strategy Group\\nStrategic and Tactical Assessment of Gold \\nand Bitcoin\\nGold’s 27% return and bitcoin’s 123% return in \\n2024 have prompted our clients and colleagues alike \\nto ask whether ISG has changed its view of gold and \\nbitcoin as investment asset classes or, at the least, \\nconsidered a tactical allocation to either asset.\\n We do not believe that gold and bitcoin (or \\ncryptocurrencies broadly) have a strategic role in \\nclients’ portfolios. However, gold, just like other real \\ncommodities, has presented tactical asset allocation \\nopportunities in the past and will continue to do so \\nin the future. \\n Bitcoin, as we have discussed before, is a \\nspeculative digital asset more suited to gambling \\nthan investing.\\nThe Strategic Case Against Gold\\nIn our 2010 Insight, Commodities: A Solution in \\nSearch of a Strategy, we showed why commodities \\nincluding gold and oil do not have a strategic role \\nin our clients’ portfolios. Since then, the S&P 500 \\nhas meaningfully outperformed commodities, \\nincluding gold (see Exhibit 62). \\n We recognize that gold has held a special \\nstatus as a perceived store of value and a symbol \\nof wealth for thousands of years. The oldest large \\nstash of gold was found in a cemetery in Bulgaria \\ndating back some 6,000 years. The Egyptians first \\nused gold bars as money as early as 4000 BC. The \\noldest coin that has been discovered is 2,700 years \\nold and is an alloy of gold and silver found in \\nEphesus, in modern-day Türkiye. \\n In many cultures, gold coins are given to \\nchildren to mark special holidays, gold rings are \\nexchanged between newlyweds, and gifts of gold \\nExhibit 62: Total Return Since Commodities \\nInsight Publication\\nCommodities have meaningfully lagged US equities since \\nJanuary 2010. \\n603\\n114\\n-19\\n-69\\n-200\\n-100\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\nS&P 500 Gold S&P GSCI Index Oil\\nTotal Return (%)\\nData as of December 31, 2024. \\nNote: Commodity returns are based on S&P GSCI total return indices. \\nSource: Investment Strategy Group, Bloomberg. \\n \\n \\n \\nExhibit 63: Frequency of Outperforming Inflation \\nOver a Given Investment Horizon\\nEquities have consistently outperformed inflation compared \\nto other asset classes.\\n100\\n65\\n76\\n57\\n90\\n34\\n57\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n5 10 15 20\\nHistorical Frequency of Outperforming Inﬂation (%)\\nInvestment Horizon (Years)\\nUS Equity\\nTreasury Bills\\nIntermediate Treasuries\\nLong Treasuries\\nHome Prices\\nCommodity Prices\\nGold Prices\\nData as of Q3 2024.  \\nNote: Asset performance is measured against headline consumer price \\ninflation. Analysis is based on data since 1926. \\nSource: Investment Strategy Group, Bureau of Labor Statistics, The Economist , Datastream, \\nBloomberg, Ibbotson, Robert Shiller (Yale University).  \\nPast performance is not indicative of future results.\\nFor Private Wealth  \\nManagement Clients\\nInsight\\nInvestment  \\nStrategy Group  \\nJanuary 2010\\nCommodities: A Solution in \\nSearch of a Strategy\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e276c4af-7c57-4a0a-b501-46c9c3d38db3', embedding=None, metadata={'page_label': '40', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='40 Goldman Sachs january 2025\\nhave conferred the highest levels of esteem, affection \\nand appreciation throughout human history.\\n However, gold does not add value to a well-\\ndiversified portfolio. It does not generate income, \\nit does not generate earnings, and—contrary \\nto popular belief—it is not an inflation hedge. \\nAs shown in Exhibit 63, the S&P 500 is a more \\neffective and reliable inflation hedge. Commodities \\nand gold are the two least effective inflation hedges. \\n While the recent surge in gold has attracted \\ninvestor attention, we note that in real terms the \\nyear-end price of $2,625 per troy ounce is only \\n22% above the real price in 1980 and 345% (3.6% \\nannualized) above the nominal price in 1980. In \\ncontrast, since 1980, the S&P 500 has returned \\n3,420% (8.4% annualized) in real terms and \\n12,791% (11.7% annualized) in nominal terms. \\nAgain, the S&P 500 has dwarfed the returns of gold. \\n Our multi-factor risk premium model for \\nour strategic asset allocation process estimates a \\nmean return of 4.6% for gold. Given its long-term \\nvolatility of 15%, gold has a particularly low Sharpe \\nratio of 0.10, making it an unviable asset for our \\nclients’ portfolios on a strategic basis.\\nThe Tactical View on Gold \\nDespite its long history and special status, we \\nbelieve gold is one of the hardest commodities \\nto evaluate. For value investors such as Warren \\nBuffett, the task is nearly impossible. In 1998, in a \\nspeech at Harvard University, Buffett is quoted: “It \\ngets dug out of the ground in Africa or someplace. \\nThen we melt it down, dig another hole, bury it \\nagain and pay people to stand around guarding \\nit. It has no utility. Anyone watching from Mars \\nwould be scratching their head.”29\\n As we discuss later in Section III, the 27% \\nrally in gold was its highest annual return since \\n2010. This rally was driven in part by increased \\npurchases made by central banks, especially those \\nof China, India and Türkiye. This increase is most \\nevident in the step-up in China’s central bank \\npurchases after November 2022 (see Exhibit 64). \\n Together with mainland China’s consumer \\npurchases, Chinese demand accounted for 31% of \\nglobal mined supply in 2023 and 24% of global \\nmined supply through the third quarter of 2024. \\n Geopolitical tensions, along with a strategy \\nof reducing dependence on the US dollar, have \\nprompted some central banks to diversify their \\nforeign exchange reserves away from the dollar \\nand reduce the risks associated with US sanctions \\nakin to those imposed on Russia. \\n This additional group of buyers—whose \\ndemand is uncertain and who are less sensitive to \\nprices—has rendered the ISG framework based on \\ninterest rates, inflation, and supply and demand \\nimbalances an unreliable tool for tactical asset \\nallocation on gold. We are, therefore, agnostic on \\nthe upside and downside of gold prices. \\n We conclude with the opening paragraph of \\nthe late Peter Bernstein’s 2000 book, The Power of \\nGold: The History of an Obsession: \\n  At the end of the 19th Century, John Ruskin \\n[English polymath] told the story of a man who \\nboarded a ship carrying his entire wealth in a \\nlarge bag of gold coins. A terrible storm came up \\na few days into the voyage and the alarm went \\noff to abandon ship. Strapping the bag around \\nhis waist, the man went up on deck, jumped \\noverboard, and promptly sank to the bottom of \\nthe sea. Asks Ruskin: “Now, as he was sinking, \\nhad he the gold? Or had the gold him?” 30 \\nThe Strategic Case Against Bitcoin \\nWe first addressed the role of bitcoin as an asset in \\nour 2018 Outlook: (Un)Steady as She Goes. Since \\nthat publication, the S&P 500 has outperformed \\nbitcoin by 37 percentage points—adjusted for \\nbitcoin’s 4.4 times greater volatility over this period. \\n We subsequently dedicated an ISG Insight to the \\nsubject titled Digital Assets: Beauty Is Not in the \\nEye of the Beholder. Since that report was published \\nExhibit 64: Central Bank Gold Holdings\\nChina’s central bank has increased its gold holdings by 16% \\nsince late 2022. \\n2,269 \\n876 \\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n2017 2018 2019 2020 2021 2022 2023 2024\\nMetric Tons\\nIndia\\nChina\\nData through November 2024.  \\nSource: Investment Strategy Group, Haver Analytics.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='631dae24-9ad1-4c31-9f5b-d5e323cf9262', embedding=None, metadata={'page_label': '41', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='41Outlook Investment Strategy Group\\nin 2021, the S&P 500 has lagged bitcoin by nine \\npercentage points—adjusted for bitcoin’s 3.8 times \\ngreater volatility over this period. \\n In 2024, the S&P 500 lagged bitcoin by 32 \\npercentage points—adjusted for bitcoin’s 4.4 \\ntimes greater volatility during the year. \\n Investors’ response to bitcoin’s 123% return \\nin 2024 reminds us of a recent observation \\nby Speevr Intelligence: “As is often the case, \\nthe price action will create the investment \\nthesis from henceforth even if the underlying \\nfundamentals remain the same.”31\\n We do not believe that bitcoin is supported by \\nan investment thesis—other than what Bill Gates \\nis reported to have said, which is that bitcoin is a \\n“pure greater fool theory type of investment.”32\\n The recent price action has not led us to \\ncreate an investment thesis. \\n We expect an investable asset to meet at least \\nthree of the following criteria:\\n• Generate steady, reliable cash flow on a \\ncontractual basis, like bonds\\n• Generate earnings through exposure to \\neconomic growth, like equities\\n• Provide consistent and reliable diversification \\nbenefits to a portfolio\\n• Dampen volatility\\n• Provide a consistent and reliable hedge against \\ninflation or deflation as a store of value\\nWe do not believe that bitcoin meets any of these \\ncriteria, as detailed below. \\nGenerate steady cash flow: Cryptocurrencies do \\nnot contractually generate a steady stream of cash \\nflows like a bond. While they may earn a yield \\nwhen used for staking in the proof-of-stake process \\nor when yield farming on an exchange, this yield is \\nnot a legal contractual obligation, it is not steady, \\nand it can be very risky.\\nGenerate earnings: Unlike equities, \\ncryptocurrencies as currently structured do not \\ngenerate earnings tied to economic growth. \\nThere is no economic rationale that underpins an \\nupward trajectory of prices. For example, S&P 500 \\ncompanies, in aggregate, have a long-term upward \\nprice trajectory because positive global growth \\nenables them to generate growth in their earnings.\\n There is no parallel to this growth in earnings \\nwith cryptocurrencies. If a blockchain had a token \\nthat received a toll for anyone’s usage of the \\nblockchain and participated in the growing use of \\nthe blockchain, then one could conceive a scenario \\nin which a cryptocurrency would capture a stream \\nof earnings and become a security token. But that \\nis not (yet) the case.\\nProvide diversification benefits: Bitcoin does \\nnot provide diversification benefits to an ISG \\ndiversified moderate-risk portfolio. If we wanted to \\njustify allocating even 1% to bitcoin in our model \\nportfolios, the required annualized return would \\nbe 78%, because of bitcoin’s high volatility and \\nuncertainty about its risk premium. \\nDampen volatility: In the post-2014 period, bitcoin’s \\nvolatility has been 63%, which is substantially lower \\nthan the pre-2014 volatility of 125%. That said, the \\nvolatility of a moderate-risk diversified portfolio is \\nabout 8.6%. Bitcoin does not dampen volatility.\\nHedge inflation or deflation as a store of value: The \\nhistory of bitcoin is limited, so we have no evidence \\nthat cryptocurrencies are a reliable inflation or \\ndeflation hedge that will store value in either an \\ninflationary or a deflationary environment. Based \\non about 15 years of data, bitcoin has a marginally \\nnegative correlation to core CPI, at -0.13. Equities \\nremain the most consistent and reliable inflation \\nhedge, and high-quality bonds remain the most \\nconsistent and reliable deflation hedge.\\n One of the risks we highlighted in the 2021 \\nInsight report was the threat of quantum computing. \\nDeloitte has highlighted how quantum computers \\nmay be able to derive a user’s private key from the \\nBy permisson Chip Bok and Creators Syndicate, Inc.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='42601a7f-0924-4302-84dc-0bff35aca588', embedding=None, metadata={'page_label': '42', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='42 Goldman Sachs january 2025\\ncorresponding public key and break the cryptography \\nthat underpins the bitcoin blockchain.33\\n Theoretically, quantum computers can break \\nRSA security protocols (among the most widely \\nused cryptographic methods), other protocols \\nbased on what is called elliptic curve cryptography \\n(which is used in many blockchains), and the SHA-\\n256 hash used by blockchain technology to secure \\nwallets and protect digital signatures. \\n No such computers exist—yet. However, \\nbitcoin traders should take note of Alphabet’s \\nrecent announcement concerning Willow. Willow \\nis the company’s latest quantum computing chip. \\nAlphabet reported that the new chip has enabled \\ntwo major achievements:\\n• Willow “can reduce error rates exponentially \\nas we scale up using more qubits. This cracks a \\nkey challenge in quantum error correction that \\nthe field has pursued for almost 30 years.”\\n• Willow “performed a standard benchmark \\ncomputation in under five minutes that would \\ntake one of today’s fastest supercomputers 10 \\nseptillion years—a number that vastly exceeds \\nthe age of the Universe.”34\\nOur colleague Eric Sheridan, the equity analyst \\nin Global Equity Research covering Alphabet, \\nhas stated that Willow represents a “potential \\ndramatic computing shift” that will evolve over a \\n10+ year time frame. Should this evolution occur, \\nbitcoin could become obsolete, because unlike \\nwith Ethereum, there is no central authority that \\ncan ensure the bitcoin blockchain evolves with \\nquantum computing technology. The bitcoin \\nblockchain’s strength is also its Achilles’ heel. \\nThe Tactical View on Bitcoin\\nWe cannot offer a tactical view on bitcoin because \\nwe have no way of valuing bitcoin:\\n• We cannot perform any cash flow analysis \\nsince bitcoin does not generate any cash flows, \\neither contractually like bonds or in the form \\nof earnings like equities. We cannot discount a \\nstream of cash flows to estimate a present value.\\n• We cannot tie bitcoin’s value to any other asset, \\nincluding gold. Since 2010, it has had zero \\ncorrelation to gold. \\n• Since bitcoin is not a medium of exchange or \\na unit of measurement, given its volatility, we \\ncannot tie its value to the money supply.\\n• Bitcoin is not widely used as a payment system \\nlike Visa or Mastercard, nor is it widely used \\nfor money transfers, so we cannot compare it to \\nother businesses. \\nIn our 2021 Insight report, we shared Wences \\nCasares’ view on the price of bitcoin. Casares is \\na technology entrepreneur and founder of Xapo \\nBank, a bank designed to be the custodian of \\nchoice for bitcoin. At the time, he shared his views \\nwith our team: \\n• “Bitcoin has a higher-than-60% chance of \\nsucceeding and being worth more than $1 \\nmillion in less than 10 years, \\n• a 25–30% chance of not disappearing but \\nbecoming irrelevant (in which case it will still \\nhave a price, but much lower than what it \\nis today, and probably less than $1,000 per \\nbitcoin), \\n• a 10–15% chance of failing and being \\nworthless.”35\\nAswath Damodaran, professor of finance at the \\nStern School of Business at New York University, \\nhas a totally different view. He states: “You cannot \\nvalue bitcoin or invest in it. You can only price it \\nand trade it.”36 He distinguishes between a “pricing \\ngame” for assets such as bitcoin and an “investing \\ngame” for assets such as equities. He suggests that \\n“gambling instincts” are a key personality trait in \\nthose who trade assets such as bitcoin.\\n Damodaran writes that any trader who thinks \\nhe or she is trading based on value is a “most \\ndelusional player.” \\n We agree with his assessment. Furthermore, \\nwe believe that an asset whose appreciation is \\nprimarily dependent on whether someone else is \\nwilling to pay a higher price for it is not a suitable \\ninvestment for our clients.\\n For those who trade bitcoin, we do offer a note \\nof caution. As shown in Exhibit 65, one of our \\nanalytical tools suggests that the recent surge in \\nBitcoin prices is indicative of “explosive behavior.” \\nWhen this statistical significance has breached \\nthe 95% level, bitcoin prices have subsequently \\nexperienced significant price drops. After the 2017 \\npeak, bitcoin prices dropped more than 80%, and \\nafter the November 2021 peak, prices dropped \\nmore than 75%. \\n Recent developments underscore this view and \\nhighlight the purely speculative nature of these assets. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b0df77fa-5c31-4f5e-95d7-a7403bd5b2db', embedding=None, metadata={'page_label': '43', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='43Outlook Investment Strategy Group\\nIn November 2024, the founder of a cryptocurrency \\nplatform paid $6.24 million for a conceptual piece of \\nart consisting of a banana stuck to a wall with duct \\ntape. Similarly, Dogecoin, a memecoin with a market \\ncapitalization of $50 billion, rallied 90% after the \\nNovember 5 election in the US.\\nOur One- and Five-Year Expected \\nTotal Returns\\nOur one-year expected returns are driven by our \\nestimates of: \\n• A 20% probability of a US recession, which \\nis slightly below reported average consensus. \\nOur colleagues in Global Investment Research, \\nJan Hatzius and David Mericle, estimate a \\nlow probability of 15%. We forecast a slightly \\nhigher probability of recession relative to the \\nunconditional probability of 18% since WWII and \\n13% since 1980: our single-variable econometric \\nmodels imply an 18% probability, and a \\nmultivariable model estimates a 20% probability. \\n• Global growth of 3.1%, which is in line with \\nour estimates of global growth in 2024 and \\nslightly above our estimate for trend growth of \\n2.9%. We estimate:\\n–  US growth of 2.3%, which is above trend \\ngrowth of 1.9%. \\n–  Eurozone growth of 1.0%, which is slightly \\nbelow trend growth of 1.2%.\\n–  UK growth of 1.2%, which is also slightly \\nbelow trend growth of 1.4%.\\n–  Japanese growth of 1.0%, which is above \\ntrend growth of 0.6%.\\n–  Emerging market growth of 4.1%, which is \\nabove trend. We have the least confidence in \\nour emerging market growth estimates, given \\nthe growing lack of transparency in Chinese \\neconomic data and the ongoing Russia-\\nUkraine war.\\n• Mid-single-digit EPS growth in most countries \\nand regions. We generally have more confidence \\nin our earnings forecasts than in forecasts of \\nany multiple expansion or contraction that \\nwould impact the total return. Historically, \\nyear-over-year earnings have been less volatile \\nthan changes in market multiples. We estimate:\\n–  US earnings growth of 10%, which is slightly \\nabove trend and the highest of any country or \\nregion other than India for 2025.\\n–  Earnings growth of 2% in the Eurozone, 3% \\nin the UK and 6% in Japan. \\n–  Emerging market earnings growth of 10%, \\nwhich is primarily driven by 13% earnings \\ngrowth in India. We expect modest earnings \\ngrowth of 5% in China. \\n• An appreciation of 2% in the dollar as measured \\nby the DXY, and a modest depreciation in \\nemerging market currencies. \\nAs shown in Exhibit 66, we expect Japan to have \\nthe best-performing equity market, with a high-\\nsingle-digit return in our base case scenario, to which \\nwe assign a 55% probability. We assign a 25% \\nprobability to returns exceeding our expectations and \\na 20% probability to a negative mid-teens return. \\n We expect US equities to be the second-\\nbest-performing market, with an expected high-\\nsingle-digit return in our base case scenario, to \\nwhich we assign a 60% probability. We assign a \\n25% probability that US equities will exceed our \\nexpectations; we also assign a 15% probability \\nthat they will deliver a negative mid-teens return.\\n We expect UK and Eurozone equities to each \\ndeliver a mid-single-digit return in our base case, to \\nwhich we assign a 50% probability. We assign 25% \\nprobabilities to both the upside and downside for \\nthese two markets. \\nExhibit 65: Statistical Significance of Explosive \\nBehavior in the Price of Bitcoin \\nWhen the statistical significance has exceeded 95%, Bitcoin \\nprices have historically experienced significant price drops.   \\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024\\nStatistical Signiﬁcance (%)\\nData through December 31, 2024.  \\nNote: The statistical significance of explosive behavior measures the degree of support for the \\npresence of a bubble in the data. The methodology has been replicated from an academic paper \\nby Peter C. B. Phillips et al., Testing For Multiple Bubbles: Historical Episodes of Exuberance and \\nCollapse in the S&P 500, International Economic Review, 2015. \\nSource: Investment Strategy Group, Bloomberg. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='daa51fdb-bf64-47b5-a81e-f5ef99d628a4', embedding=None, metadata={'page_label': '44', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='44 Goldman Sachs january 2025\\n Note that non-US developed market equity \\nreturns are presented in Exhibit 66 in local \\ncurrency terms. If the US dollar appreciates by our \\nexpected 2%, then US equities will outperform \\nmost other markets. \\n Emerging market equities are expected to \\nhave a mid-single-digit return. We assign a 55% \\nprobability to our base case. The volatility in our \\nbase case for emerging markets is the greatest of \\nall the markets. We assign a 25% probability to \\nthe upside scenario and a 20% probability to the \\ndownside scenario. On a probability-weighted \\nbasis, emerging market equities have the second-\\nlowest expected return after the UK. \\n We expect mid-single-digit returns for most \\ndeveloped and emerging market bonds, except for \\nUK bonds, where we expect a low-double-digit \\nreturn for 10-year UK gilts in our base case. \\n If this forecast is realized, the reference \\nbenchmark for a US moderate-risk model portfolio \\nwill deliver favorable returns in 2025, although \\nslightly below the 9% and 10% returns realized \\nin 2024 for taxable and tax-exempt clients, \\nrespectively. In 2023, the reference benchmarks \\nboth returned 14%. \\n The reference benchmark for a taxable \\nmoderate model portfolio is 50% global equities \\nand 50% 1- to 10-year municipal bonds, and the \\nreference benchmark for the tax-exempt moderate \\nmodel portfolio is 50% global equities and 50% \\nintermediate government and corporate bonds. \\nOur Tactical Tilts \\n2024 was a busy year for tactical tilts. We added \\n30 new tilts, removed 24 tilts and adjusted the \\nExhibit 66: ISG Prospective Total Returns \\n2\\n4 4 5 5 5 6 6 6 6 6 7 7 7 8 8 8\\n9\\n12\\n6\\n7\\nDXY (9%)\\nMuni 1–10 (3%)\\nUS Cash (0%)\\nEM Local Debt (13%)\\nMuni HY (7%)\\nHedge Funds (6%)\\n5-Year Treasury (4%)\\n10-Year Germany (5%)\\nUS Corporate HY (11%)\\nBank Loans (8%)\\n10-Year Treasury (7%)\\nUK Equity (Local) (15%)\\nEM Equity (US$) (22%)\\nEurope Ex-UK Equity\\n(Local) (17%)\\nMSCI All-Country\\nWorld (15%)\\nS&P 500 (15%)\\nNon-US Developed\\nEquity (Local) (15%)\\nJapan Equity (Local) (17%)\\n10-Year UK (7%)\\n0\\n2\\n4\\n6\\n8\\n12\\n10\\nTaxable Moderate\\nReference Benchmark (8%)\\nTax-Exempt Moderate\\nReference Benchmark (8%)\\nAsset Class (Model-Based Volatility)\\n%\\n2025 Prospective Return  \\n-2\\n3 3 3 4 4 5 5 5 5 5 5 5 5 5 6 6 6 6\\n4\\n5\\n-3\\n0\\n-1\\n-2\\n2\\n1\\n3\\n4\\n5\\n7\\n6\\nTaxable Moderate\\nReference Benchmark (8%)\\nTax-Exempt Moderate\\nReference Benchmark (8%)\\nAsset Class (Model-Based Volatility)\\n%\\n5-Year Prospective Annualized Return\\nDXY (9%)\\n10-Year Germany (5%)\\nMuni 1–10 (3%)\\nUS Cash (0%)\\nHedge Funds (6%)\\nJapan Equity (Local) (17%)\\nEM Local Debt (13%)\\n5-Year Treasury (4%)\\nMuni HY (7%)\\nBank Loans (8%)\\nS&P 500 (15%)\\nUS Corporate HY (11%)\\nMSCI All-Country\\nWorld (15%)\\nNon-US Developed\\nEquity (Local) (15%)\\nUK Equity (Local) (15%)\\nEM Equity (US$) (22%)\\n10-Year Treasury (7%)\\n10-Year UK (7%)\\nEurope Ex-UK Equity\\n(Local) (17%)\\nData as of December 31, 2024. \\nSource: Investment Strategy Group. See endnote 37 for list of indices used. \\nForecasts have been generated by ISG for informational purposes as of the date of this publication. There can be no assurance the forecasts will be \\nachieved. Indices are gross of fees and returns can be significantly varied. Please see additional disclosures at the end of this Outlook .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9b41a176-b6b1-46b6-bbbd-5f448f12ff20', embedding=None, metadata={'page_label': '45', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='45Outlook Investment Strategy Group\\nweights of some of the tilts. We initiated new tilts \\nat double the rate of our long-term average of 15. \\n S&P 500 volatility as measured by VIX \\naveraged 16, below its long-term average of 20, \\naffording the ISG team fewer opportunities for \\ntactical asset allocation in US equities. Volatility \\nin Japanese equities was closer to its historical \\naverage, which afforded the team a profitable but \\nvery short-lived tactical tilt in 2024. \\n On the other hand, the volatility in the fixed \\nincome market was much higher and afforded more \\nopportunities for tactical tilts. The MOVE Index, \\na measure of volatility in US Treasury securities, \\naveraged 104, compared to a post-GFC average \\nof 79. The SMOVE Index, a measure of volatility \\nin European fixed income securities, averaged 74, \\ncompared to a post-GFC average of 61. \\n The tactical tilts returned just over 2% \\nafter fees for 2024, outperforming the funding \\nsource—the Bloomberg 1-10 Year Municipal Bond \\nIndex—which returned 0.9%. Our tactical tilts are \\ntypically funded out of bonds. \\n The overall beta of tactical tilts to the S&P 500 \\nwas 0.14, compared to a long-term average of 0.36 \\nsince the inception of ISG in 2001. \\n As 2025 begins, we have 13 tactical tilts, and \\nthe level of risk allocated to such tilts is close to the \\nlowest level of risk over the past decade. \\nOverweight UK Fixed Income: Since our first \\nUK fixed income-driven tactical tilt, initiated in \\nAugust 2023, we have increased our allocation to \\nUK securities. We currently have three tactical tilts \\ndriven by our views on the UK. We expect: \\n• Below-trend growth of 1.2%. \\n• A sharp fall in core inflation from an average of \\n3.7% in 2024 to 2.7% in 2025.\\n• Declining rates. We expect central bank policy \\nrates to steadily decline to a terminal rate of \\n2.5–3.0%, whereas the markets are expecting \\na much shallower decline. Historically, gilt \\nrates have fallen sharply following the first rate \\ncut, which in this cycle was implemented in \\nAugust 2024. \\nWe expect these tilts to provide mid- to high-single-\\ndigit returns. We assign a 30% probability to a UK \\nrecession, which would result in further upside. \\nOverweight European Fixed Income: We initiated a \\ntactical tilt to European fixed income in November \\n2024 through a swap structure. We expect \\nbelow-trend growth of 1% in the Eurozone but \\nhave assigned a relatively high 40% probability \\nto recession, given the likelihood of Trump \\nadministration tariffs and slower growth in China. \\nWe expect the ECB to lower rates further than \\nmarket expectations. History shows that European \\nrates decline over 12 months after the start of an \\nECB easing cycle, whether the Eurozone falls into \\nor avoids a recession. \\n We expect this tilt to provide a mid-single-digit \\nreturn. Should the Eurozone slip into recession, this \\ntilt would provide further upside. \\nAllocation to a Relative Value Currency Trade in \\nDeveloped Markets: We initiated a tilt in August \\n2024 that is long the US dollar relative to the Swiss \\nfranc. This tilt was an outgrowth of a 2023 long \\neuro/short Swiss franc tilt. Unlike most central \\nbanks, the Swiss National Bank (SNB) uses both \\npolicy rates and the currency as policy levers in its \\ntool kit. \\n Having experienced six years of inflation below \\n0.5% and deflation of greater than 0.5% over \\nmultiple periods between 2011 and 2013, and \\nin 2015 and 2016 (see Exhibit 67), the SNB has \\nbecome concerned that the recent rapid drop in \\ninflation could spiral into deflation. As a result, the \\nSNB will pursue a more dovish policy to cheapen \\nthe Swiss franc and use currency intervention \\nas needed. \\nExhibit 67: Switzerland Core Consumer \\nPrice Index\\nThe SNB has become concerned that the rapid drop in \\ninflation could spiral again into deflation. \\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n2010 2012 2014 2016 2018 2020 2022 2024\\n% YoY\\n0.9\\nData through November 2024. \\nSource: Investment Strategy Group, Bloomberg.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f3805d85-2f12-41c3-ae71-42a6b74fee08', embedding=None, metadata={'page_label': '46', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='46 Goldman Sachs january 2025\\n The Swiss franc is also overvalued relative to \\nthe US dollar by about 15–20% based on a series \\nof purchasing power metrics. \\n We expect this tilt to provide a mid-single-\\ndigit return. \\nAllocation to a Relative Value Currency Trade \\nin Emerging Markets: We initiated a tilt that is \\nlong the US dollar relative to the Indian rupee in \\nNovember 2024. We expect Asian currencies in \\nemerging markets to face downward pressure from \\ndollar appreciation amid heightened risk of trade \\nwars and tariffs, and downside risks to growth in \\nthe region. \\n Policymakers in India also hope to capitalize \\non a US and European de-risking strategy toward \\nChina by expanding India’s export capacity and \\nmaintaining export competitiveness with regional \\npeers. A weaker currency is likely to become a \\nhigh priority for India if it wants to ensure its \\ncompetitiveness. India’s real effective exchange rate \\nis trading 10% above its 20-year moving average \\nand the rupee is one of the most overvalued \\ncurrencies among key emerging market currencies.\\n In our base case, we expect a mid-single-\\ndigit return. \\nOverweight US Energy Infrastructure Master \\nLimited Partnerships: The allocation to master \\nlimited partnerships (MLPs) is the longest-standing \\ntactical tilt recommended by ISG. It was initiated \\nas an option tilt in 2015 and changed to a long \\nsector position in January 2016. Since then, we \\nhave frequently adjusted the position’s size as \\nopportunities have arisen. This tilt has an inception-\\nto-date return of 44%. The MLP tilt was up 27% \\nin 2024, compared to the S&P 500 at 25% and to \\nintermediate municipal bonds at 0.9%. Intermediate \\nbonds are the funding source for this tilt. \\n While we have reduced the allocation to \\nMLPs given their strong performance over the last \\nfour years, we retain a small allocation for the \\nfollowing reasons:\\n• Valuations are still attractive even after such \\nstrong returns. Valuation as measured by \\nthe ratio of enterprise value to EBITDA is \\n9.6 times. While this is cheap relative to the \\nlong-term average of 11.3 times EBITDA, we \\nconsider 9.7 times a more realistic target, as it \\nremoves the extreme overvaluations between \\n2010 and 2015.\\n• We expect earnings to continue to grow \\nat about 7%.\\n• The tax-advantaged distribution yield of 6.7% \\nis attractive relative to fixed income and high \\nyield rates. The companies in the Alerian \\nMLP Infrastructure Index generate free cash \\nflow that is 1.6 times distributions, so the \\ndistribution yield appears secure.\\nWe expect this tilt to provide a high-single-digit \\nreturn. Our expected return is based on a $60–80 \\nrange for WTI crude oil prices. \\nOverweight US Health Care: We initiated a tactical \\ntilt to US health-care stocks in September 2024, \\ndriven by extremely cheap valuations relative to the \\nS&P 500. The sector trades at a 23% discount to \\nthe S&P 500 and has been cheaper only 4% of the \\ntime since 1994. It underperformed the S&P 500 \\nby 24 and 22 percentage points in 2023 and 2024, \\nrespectively. Historically, such underperformance has \\nbeen followed by midteens outperformance relative \\nto the S&P 500 over the following 12 months.\\n We do not expect the incoming Trump \\nadministration policies to have a material impact \\non this sector:\\n• The FDA accounts for just 0.10% of the overall \\nfederal budget, and nearly half the agency’s \\nbudget is sourced from user fees from the \\nprivate sector. \\n• Medicare, Medicaid, the Children’s Health \\nInsurance Program and other mandatory \\nprograms account for 91% of the budget of the \\nDepartment of Health and Human Services. \\nPresident-elect Trump’s nominee for health \\nsecretary has not been critical of these programs. \\n• The Inflation Reduction Act of 2022 established \\ndrug pricing legislation, and it is unlikely to be \\nchallenged. \\nIn our base case, we expect this tilt to provide \\nreturns in the high teens. \\nOverweight European Aerospace and Defense: \\nWe initiated this tilt in April 2024. The original \\npremise for this tilt was that European countries \\nwould increase their defense spending because of \\nthe Russian invasion of Ukraine. Increased pressure \\nfrom the incoming Trump administration bolsters \\nour position. European defense spending grew \\n10% in 2024. At that level of spending growth, ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8b037bdb-8733-475f-8d8e-0aa25cde9ab0', embedding=None, metadata={'page_label': '47', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='47Outlook Investment Strategy Group\\nwe expect current valuations of 18 times 2025 \\nearnings to increase to just above 20 times. \\n Airbus is the largest company in this sector and \\nhas an order backlog of about eight years. \\n In our base case, to which we assign a 60% \\nprobability, we expect midteens returns. \\nOverweight Mexican Stocks: We initiated a tactical \\ntilt to Mexican stocks in April 2024. Valuations \\nare extremely low and have been lower only 2% \\nof the time over the last 20 years. Mexican equity \\nvaluations now stand below the levels seen during \\nthe COVID pandemic. \\n While foreign investors have been reducing \\ntheir exposure to emerging market equities, \\ntheir selling of Mexican stocks has been more \\npronounced. The current investor base of domestic \\npension and mutual funds points to a more stable \\ninvestor foundation going forward. We also think \\nthat Mexican earnings will be more resilient in the \\nface of tariffs and domestic uncertainty because \\nof the defensive composition of the Mexican \\nmarket, with more exposure to consumer staples, \\ntelecommunications and banking stocks.\\n We expect a base case return in the low teens. \\nOverweight South African Stocks: We initiated a \\ntactical tilt to South African equities in September \\n2024. Valuations are attractive, at a market \\nmultiple of 9.3 times, which is a discount of 23% \\nto the MSCI Emerging Markets index; South \\nAfrican equities have historically traded at a \\n2% premium to other emerging market equities. \\nEarnings are expected to increase at an annualized \\npace of 18% over the next two years.\\n Just like Mexican stocks, South African stocks \\nhave been hit by outflows in 33 of the last 36 \\nquarters, as a result of which investor positioning \\nis very light. \\n We also expect South Africa to benefit from \\nChina’s stimulus program, as 73% of South \\nAfrica’s exports to China are metals and ores, such \\nas gold, platinum, iron ore and coal briquettes.\\n We expect a base case return in the low teens.\\nAllocation to Physical Uranium: We initiated a \\nsmall allocation to physical uranium in March \\n2022. Nuclear energy is becoming an increasingly \\nattractive source of electricity as countries seek \\nreliable, carbon-free and secure sources of energy. \\nSome view nuclear energy as a permanent solution; \\nothers see it as a transitional source of energy away \\nfrom hydrocarbons while other renewable energy \\nsources are being developed.\\n There has been a sharp decrease in uranium \\nexploration and production from existing mines. \\nUranium mine production has been insufficient to \\nmeet annual reactor requirements since 2018 (see \\nExhibit 68). Utilities have accessed inventories and \\nextended the enrichment process to extract more \\nyield per unit of raw uranium to make up for this \\nshortfall. \\n Additional supply has been slow to come \\nonline. Announced production was delayed or \\ncanceled in 2024 due to the challenges in ramping \\nup mining activity, including lack of access to \\nskilled labor, shortages of raw materials such as \\nsulfuric acid and shortages of water. \\n Geopolitics has also interfered with the supply/\\ndemand imbalance. The United States’ Prohibiting \\nRussian Uranium Imports Act was passed in \\nMay 2024, and Russia retaliated by announcing \\nrestrictions on the export of enriched uranium to the \\nUS, both moves that threaten supplies to the US. \\n Demand, on the other hand, is growing, with \\nnew reactors coming online in China, operable \\nreactors restarting in Japan and over 30 countries at \\nthe UN’s COP28 and COP29 climate conferences \\npledging to increase their nuclear energy capacity. \\n The realization of this supply/demand imbalance \\nsent uranium prices skyrocketing in 2023. Prices \\nappreciated 90% in 2023, but declined 20% in 2024. \\nExhibit 68: Annual Uranium Mine Production vs. \\nReactor Requirements\\nUranium mine production has been insufficient to meet \\nannual reactor requirements since 2018. \\nMine Production\\nReactor Requirements\\n2016\\n164\\n2020\\n122\\n276\\n131\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n2008 2013 2018 2023 2028 2033 2038\\nMillion Pounds U 3O8\\nProjection\\nData through 2023. Projection through 2040. \\nNote: Projections are based on base case from UxC.  \\nSource: Investment Strategy Group, UxC LLC.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b439beb7-6904-4263-85ac-235688582265', embedding=None, metadata={'page_label': '48', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='48 Goldman Sachs january 2025\\n We expect prices to resume an upward trajectory \\nfor a total return in the high teens in our base case.\\nAllocation to Systematic Strategies: We deploy \\none systematic strategy called Trend-Based \\nRotation (TBR) to provide uncorrelated sources of \\nincremental return to a portfolio.\\n This strategy was introduced in 2021 and \\nwas adjusted in 2023. The goal is to rotate risk \\namong 14 asset classes across commodities, US \\nand non-US equity indices, US corporate bonds, US \\nTreasury bonds and US cash. The strategy is driven \\nby the trend in each asset class, is based on the \\nmomentum factor and provides diversification to \\nISG’s more value-oriented tactical tilts.\\n We expect the TBR strategy to deliver high-\\nsingle-digit returns. \\nRisks to Our 2025 Economic and Financial \\nMarket Outlook\\nKaren Elliott House, Pulitzer Prize-winning \\njournalist, former publisher of the Wall Street \\nJournal and author, recently wrote:\\n  In 45 years of covering international politics, \\nI’ve never seen the world more complex or \\nconfrontational than it is now. China is saber-\\nrattling at Taiwan. North Korea is sending \\ntroops to fight in Ukraine. South Korea is in an \\nimpeachment crisis. European leaders are weak. \\nThe US is essentially without a president.38\\nThe sentiment of extreme complexity and \\nheightened geopolitical risks has been echoed by \\npolitical and military leaders. \\n In December, in his first speech as the newly \\nselected NATO secretary-general, Mark Rutte \\nshared his concerns: “I’ll be honest: the security \\nsituation does not look good. It’s undoubtedly the \\nworst in my lifetime. And I suspect in yours too.”39\\n UK Chief of Defence Staff Admiral Sir Tony \\nRadakin expressed a similar view in an annual \\nlecture at the Royal United Services Institute for \\nDefence and Security Studies: “The security outlook \\nis more contested, more ambiguous and more \\ndangerous than we have known in our careers.”40\\n In an event hosted by the Brookings Institution, \\nCommander of US Indo-Pacific Command Admiral \\nSamuel Paparo raised concerns about risks in the \\nIndo-Pacific: “Over the summer, I saw the most \\nrehearsals and the most joint exercises from the \\nPeople’s Republic of China that I had ever seen. \\nWith the widest geography, the jointest [sic] \\noperations for air, missile, maritime power, that I’d \\nseen over an entire career of being an observer.”41\\n On cybersecurity, Jen Easterly, director of \\nthe Cybersecurity and Infrastructure Security \\nAgency, warned of growing cyber threats: “To me, \\nthe big story from the last couple of years [for] \\nbusinesses large and small, critical infrastructure \\nowners and operators, is really about the actor \\nknown as Volt Typhoon, that has been working to \\nembed, to burrow into, our most sensitive critical \\ninfrastructure … for disruption or destruction in the \\nevent of a major crisis in the Taiwan Strait. So this \\nis a world where a war in Asia could see very real \\nimpacts to the lives of Americans across our nation, \\nwith attacks against pipelines, water facilities, \\ntransportation nodes, against communications, all \\nto induce societal panic and to deter our ability to \\nmarshal military might and citizen will. That is a \\nvery real, not a theoretical threat.”42\\n Terrorism, which is fresh in our minds after \\nthe New Year’s Eve truck attack in New Orleans, \\nis another major concern. US Homeland Security \\nSecretary Alejandro Mayorkas told Bloomberg \\nNews that while domestic violence remains the \\ngreatest threat facing the US, “the threat of foreign \\nterrorism is uppermost in our minds as well now, \\nmore so than it was last year. The war in the \\nMiddle East following the October 7 attacks has \\nheightened the threat landscape.”43\\n Geopolitical risks are extremely high and \\ncould easily derail our economic and financial \\nmarket outlook. \\n As usual in assessing such risks, we have \\nconsulted with internal colleagues as well as \\nexternal geopolitical experts. The experts have \\nexpressed a wide range of views. \\n In alphabetical order, they are: \\n• Andrew Bishop, senior partner and global head \\nof policy research, Signum Global Advisors \\n• Ian Bremmer, president and founder, \\nEurasia Group\\n• General Sir Nick Carter, former chief of the \\nDefence Staff in the UK \\n• Lauren Gloudeman, director for China, \\nEurasia Group\\n• Reva Goujan, director, Rhodium Group\\n• Robert Kahn, managing director, Global \\nMacro-Geoeconomics, Eurasia Group ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2fa70246-2b3d-4f6e-a897-a4ed6a94c6a8', embedding=None, metadata={'page_label': '49', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"49Outlook Investment Strategy Group\\n• Jonathan Lang, practice head, Trade and Supply \\nChains, Eurasia Group\\n• Joseph S. Nye Jr., Harvard University \\nDistinguished Service Professor, Emeritus\\n• Karl Rove, senior advisor (2000–04) and \\ndeputy chief of staff (2004–07) to President \\nGeorge W. Bush\\n• Rear Admiral Michael Studeman, US Navy \\n(retired), former commander of the Office of \\nNaval Intelligence \\n• Sir Alex Younger, former chief of the Secret \\nIntelligence Service in the UK and regional \\nadvisor at Goldman Sachs\\nBefore we delve into the geopolitical risks, we will \\naddress a recurring question about the high levels \\nof US debt-to-GDP. Headlines and commentary \\nreferencing “the coming debt avalanche,”44 \\n“a cataclysmic reckoning”45 and “government \\nspending just keeps on growing”46 are frequent \\nwarnings of large deficits and rising interest rates. \\nUS Debt Sustainability \\nWhile the growth trajectory of US debt is not \\nsustainable in the long run, it also does not present \\nan imminent risk at current levels of debt-to-\\nGDP. No one knows the tipping point at which \\nthe interest burden on the US government and \\nthe US economy crowds out essential government \\nexpenditures and private sector capital formation, \\nbut it is likely at least 10 years away. \\n Debt levels become unsustainable when an \\neconomy can no longer carry its debt burden. \\nThe government is then forced to either increase \\ntaxes, which shrinks the economy and makes the \\ndebt burden even greater, or default explicitly or \\nimplicitly through high inflation. As investors see a \\nrapidly rising debt trajectory, they demand a higher \\ninterest rate. The higher interest rates lead to more \\ndebt, resulting in a vicious downward economic \\nspiral. Countries need to implement credible fiscal \\nadjustments before they reach an unsustainable \\nlevel of debt-to-GDP—and that applies to the US \\neven with the US dollar being the reserve currency \\nof the world. \\n Let’s first examine the estimates of the level \\nat which federal debt is no longer sustainable \\n(see Exhibit 69). According to the Penn Wharton \\nBudget Model from the University of Pennsylvania, \\nthe level for the US is 175–200% of GDP.47 \\n The IMF estimates a debt-to-GDP sustainable \\nlevel to be between 160% and 183%, for an average \\nof 172%. These models make several assumptions \\nabout growth rates, interest rates, and the feedback \\nloop between rising debt levels and the need for \\nhigher interest rates to fund those debt levels. \\nExhibit 70: US Debt-to-GDP Ratio vs. Previous \\nCBO Projections\\nFiscal reforms can have a meaningful impact on the debt \\ntrajectory. \\n2019\\nPre-Covid\\nActual\\n2011 CBO Projection\\n2013 CBO Projection\\n79\\n9794\\n150\\n75\\n115\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\n2010 2015 2020 2025 2030\\n%\\nData through 2023. Projection through 2030. \\nSource: Investment Strategy Group, Congressional Budget Office (CBO), Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved. \\n \\nExhibit 69: US Debt-to-GDP Ratio vs. Potentially \\nUnsustainable Levels\\nThe US debt load should not become a major burden for the \\nnext 10 years. \\n166\\n172\\n188\\n248\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050\\n%\\nCBO Projection\\nUS Debt-to-GDP\\nIMF Research Paper 'Fiscal Space' Estimate\\nWharton Budget Model Estimate\\nUK Post-WWII Peak\\nData through 2023. Projection through 2054. \\nSource: Investment Strategy Group, Haver Analytics, Congressional Budget Office (CBO), Penn \\nWharton Budget Model, Bank of England, IMF Staff Position Note: D’Ostry et. Al. (2010). \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='737dcadc-7dd8-482f-92e6-e0d8122bba8e', embedding=None, metadata={'page_label': '50', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='50 Goldman Sachs january 2025\\n The most important variable impacting the \\ntrajectory of debt-to-GDP is the gap between interest \\nrates and nominal GDP growth rates (the “r-g gap”). \\nOur colleagues, Jan Hatzius, chief economist and \\nhead of Global Investment Research, and David \\nMericle, chief US economist in Global Investment \\nResearch, estimate that the unsustainable level for \\ndebt-to-GDP is well below 180% (the average of \\nthe midpoints of the studies noted above), given \\nassumptions about the r-g gap, the current level of \\nnominal interest rates on US debt, and the fiscal \\nbalance excluding interest payments.\\n We estimate that at current levels of the r-g gap, \\nthe US debt load should not become a major burden \\nfor the next 10 years. Any further productivity boost \\nto growth would further improve debt sustainability. \\nHowever, should interest rates rise and growth \\nslow—whether due to a tariff war or other factors—\\nthe debt sustainability profile will deteriorate and \\nthe risks increase.\\n The key question is whether Congress and \\nany administration will lower the debt trajectory \\nin the absence of a crisis. Exhibit 70 shows that \\nfiscal reforms in the US can have a meaningful \\nimpact on the debt trajectory. The Budget Control \\nAct of 2011 and the American Taxpayer Relief \\nAct of 2012 lowered the debt trajectory, and the \\nCongressional Budget Office’s 2013 forecast for \\ndebt-to-GDP in 2030 was lowered from 150% to \\n115%. The 2020 COVID-19 pandemic shifted the \\nactual debt trajectory.\\n We reiterate the first pillar of our investment \\nphilosophy: history is a useful guide. The US \\nlowered its debt trajectory without a crisis in 2011 \\nand 2012. Meanwhile, the US has a few years to \\naddress the debt trajectory. \\nThe Greatest Risk: China \\nChina poses the greatest risk to our 2025 outlook. \\nThe risks have increased through three main \\nchannels: \\n• An escalation in the trade war through \\nhigher tariffs imposed by the US and China’s \\nretaliatory actions \\n• China’s more aggressive and assertive policies in \\nthe South China Sea, toward Taiwan and other \\nneighboring countries like the Philippines, and \\ncontinued support of Russia and North Korea\\n• More aggressive cyberattacks on US \\ninfrastructure, telecommunications systems and \\ngovernment agencies \\nPillars of the Investment Strategy Group’s Investment Philosophy\\nAsset allocation process is client-tailored and independent of implementation vehicles\\nInvestment Strategy Group\\nAnalytical Rigor\\nHistory Is a\\nUseful Guide\\nAppropriate\\nDiversification\\nValue\\nOrientation\\nAppropriate\\nHorizon\\nConsistency', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='de56f016-707d-4fb4-ae1f-3044cca9f3d9', embedding=None, metadata={'page_label': '51', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='51Outlook Investment Strategy Group\\nA fourth channel is a liquidation of all US assets \\nheld in official Chinese holdings. China holds \\na mix of US securities across financial assets, \\nincluding Treasuries, agencies, corporate bonds \\nand equities (see Exhibit 71). Some of these assets \\nare held overseas—most likely in Belgium or \\nLuxembourg. \\n Based on the Treasury International Capital \\ndata, we estimate that the People’s Bank of China \\ncontinues to hold about 60% of its official FX \\nreserves in US assets and has not reduced that \\nshare meaningfully in recent years (see Exhibit 72).\\n We think it is highly unlikely that China will \\nliquidate its entire holdings of US assets: such an \\naction would endanger the stability of its currency, \\ncause losses to its reserves and destabilize its \\nfinancial markets and economy. \\nEscalation of the Trade War Is Likely: With current \\ntariffs on Chinese imports in place from the first \\nTrump administration and the sanctions both \\ncountries have imposed on imports and exports, a \\ntrade war is currently simmering. We expect it to \\nboil over and lead to market volatility. \\n The US has restrictions on exports of advanced \\nchips and dual-purpose technology that can be \\nused for military purposes. China has export \\nrestrictions on graphite, gallium, germanium, and \\nrare earth extraction and separation technologies \\nthat are critical to US defense technologies, \\nmilitary communications, computers, televisions \\nand smartphones. China has also sanctioned firms \\nlike Skydio, a US drone manufacturer that relies \\non Chinese batteries, and US senators, including \\nFlorida senator Marco Rubio, who is the Trump \\nadministration nominee for secretary of state. \\n It is highly likely that we will see some \\nescalation in the trade war, but we believe the \\nmagnitude of that escalation is uncertain. On the \\nbasis of the rhetoric about trade tariffs during the \\npresidential campaign, and the nominees of the \\nincoming administration, we believe existing tariffs \\nwill be increased and new tariffs may be imposed. \\nChina will respond to these tariffs, but experts \\ndiffer as to whether the initial retaliation will be \\nsoft or forceful. \\n Most of our external experts think a deal \\nto resolve US-China tensions is highly unlikely. \\nEurasia Group assigns a 45% probability to \\n“unmanaged decoupling” with no agreement on \\ntrade and a freeze in bilateral relations, and a 30% \\nprobability to “managed decoupling,” where a \\ndeal is reached after some tariff escalation. Signum \\nGlobal Partners does not expect any kind of accord \\nsince it believes China is unlikely to agree to the \\nTrump administration’s demands. Rhodium Group \\nbelieves that the level of retaliation will escalate \\nand a crisis is more likely. \\nExhibit 71: Chinese Holdings of US Assets\\nChina holds a mix of US securities, including assets held \\nwith overseas custodians.  \\n1,888 \\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n2012 2014 2016 2018 2020 2022 2024\\nUS$ Billions\\nDeposits\\nTreasuries\\nLong-term Agency Bonds\\nLong-term Corporate Bonds\\nEquities\\nBelgian Treasury Holdings\\nChinese Holdings of \\nUS Assets\\nData through October 2024. \\nNote: Chinese holdings are based on data from the Treasury International Capital (TIC) System. \\nBelgian Treasury Holdings are included in Chinese holdings to account for custodial bias in the \\nTIC dataset.  \\nSource: Investment Strategy Group, Haver Analytics, US Department of the Treasury.\\nExhibit 72: Chinese Holdings of US Assets vs. \\nChina’s FX Reserves\\nWe estimate that the PBOC has not meaningfully reduced \\nits US holdings.\\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n2012 2014 2016 2018 2020 2022 2024\\nUS$ Billions\\nChinese Holdings of US Assets\\n60% of China’s Ofﬁcial FX Reserves\\nData through October 2024. \\nNote: Chinese holdings are based on data from the Treasury International Capital (TIC) System. \\nBelgian Treasury Holdings are included in Chinese holdings to account for custodial bias in the \\nTIC dataset.  \\nSource: Investment Strategy Group, Haver Analytics, IMF, US Department of the Treasury.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='90c72098-4c7f-43e4-a3ca-4daedd785a3b', embedding=None, metadata={'page_label': '52', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='52 Goldman Sachs january 2025\\n China’s strategy to increase its exports as a \\nshare of global trade is likely to face hurdles given \\nthe escalation in the trade war with the West and \\ngrowing resistance (discussed below) in the Global \\nSouth—a term referring to a collection of low- and \\nmiddle-income countries in Africa, Latin America, \\nAsia, the Caribbean and Oceania. \\n China’s share of global trade currently stands \\nat 11.3%, down from a peak of 12.6% in 2021. In \\nconstant dollars, its share of global trade has also \\npeaked (see Exhibit 73). It is unlikely that China’s \\nshare of world trade can grow much further; in \\nfact, given rising geopolitical tensions with the US \\nand Europe, which together account for 27% of \\nChina’s exports, China’s share is likely to shrink. \\nThe Global South cannot and will not replace the \\nWest as China’s major export market. \\n In a recent report, Jonathan Anderson, founder \\nof Emerging Advisors Group and former chief Asia \\neconomist at Goldman Sachs, wrote that the US \\nand Europe allowed the emerging market tigers, \\nespecially China, to “export, industrialize and \\ngrow out of poverty” at the expense of their own \\nindustrial workforce.48 He argues that China will \\nnot do the same for the Global South. \\n In fact, as Anderson emphasizes, “it is exactly \\nthe opposite.” China has increased its exports to \\nthe Global South and now has a $1 trillion annual \\nsurplus with these countries. A trade war with the \\nGlobal South may have just begun:\\n• South Africa has told China it wants to reduce \\nits current trade deficit.\\n• Brazil has imposed a quota system and tariffs on \\ncheap steel imports including those from China.\\n• Brazil and Türkiye have imposed import duties \\non electric vehicles, leading China to build \\nmanufacturing plants in both countries.\\n• Chile has imposed anti-dumping tariffs of up to \\n35% on steel from China.\\n• Colombia has increased tariffs on steel from \\nChina to a maximum of 35%. \\n• Thailand’s Federation of Thai Industries has \\nasked the government to impose more tariffs \\non imported goods in response to the entry of \\nTemu, a Chinese online retailer.\\n• Indonesia is planning import duties of 100–\\n200% to protect its domestic industries.\\nTrade wars will escalate. \\nRisks of Military Escalation: Officially, China has \\ndoubled its military expenditures over the last 10 \\nyears. The US Department of Defense estimates \\nthat military spending could be 40–90% higher \\nthan the official stated numbers.49\\n With its greater military firepower, China has \\nbecome more assertive toward Taiwan and in the \\nSouth China Sea. It has steadily increased its military \\nincursions into Taiwan’s Air Defense Zone (see \\nExhibit 74), and in its drill on October 14, 2024, \\nExhibit 73: China’s Share of Global Goods and \\nServices Exports\\nChina’s share of global exports has declined since 2021 and \\nis likely to shrink further.\\nShare of Nominal US$ Exports\\nShare of 2015 US$ Exports\\n%\\n11.3\\n12.8\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n2005 2007 2009 2011 2013 2015 2017 2019 2021 2023\\nData through 2023. \\nSource: Investment Strategy Group, Haver Analytics. \\nExhibit 74: China’s Incursions into Taiwan’s Air \\nDefense Identification Zone (ADIZ)\\nChina has steadily increased its military incursions into \\nTaiwan’s Air Defense Zone.  \\n2023\\n2024\\n1,703\\n3,075\\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n3,500\\nJan Apr JulFeb May AugMar Jun Sep Oct Nov Dec\\nCumulative Incursions\\nData through December 2024.  \\nSource: Investment Strategy Group, ChinaPower Project, Center for Strategic and \\nInternational Studies.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='58f2a63b-8e5e-487f-b912-2c1877e5e368', embedding=None, metadata={'page_label': '53', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='53Outlook Investment Strategy Group\\nit deployed a record number of aircraft, an aircraft \\ncarrier, and navy and coast guard ships to simulate a \\nblockade of Taiwan. The operation was called Joint \\nSword-2024B to convey to the Taiwanese that China \\nhad “a sword hanging over their heads.”50\\n While President Xi Jinping told President \\nJoseph Biden that Taiwan posed a red line for \\nChina, all our experts think the risk of a war \\nbetween the US and China is low over the next few \\nyears but will increase over time. In the meantime, \\nthe risk of a maritime or aerial accident is high. \\n With respect to the South China Sea, China \\npromotes its maritime claim by referring to the \\nnine-dash line. These dashes first appeared on \\na map produced by China in the 1930s, were \\nrepeated in 1947 with the official map of China, \\nand then were submitted to the United Nations \\nin 2009. China has not clarified the significance \\nof the nine-dash line with respect to its claims for \\nsovereignty over islands, minerals or navigation \\nrights. And its claims are rejected by other South \\nChina Sea countries, including the Philippines \\nand Indonesia. China’s claims were also rejected \\nby the Permanent Court of Arbitration in The \\nHague in 2016.\\n The risk of significant military engagement is \\nlow in 2025. \\nCyber Threats: China has been and will continue \\nto be the biggest sponsor of cyberattacks in the \\nUS. According to the FBI, China has stolen more \\npersonal and business data from Americans than all \\nother cybercrime nations combined.51 As outlined \\nby Mike Gallagher, former chairman of the Select \\nCommittee on the Strategic Competition Between the \\nUnited States and the Chinese Communist Party, at \\none of its committee hearings, China’s cyberattacks \\nhave focused on three areas in sequence: \\n1. Initially, China’s hacking was directed to the \\ntheft of intellectual property and valuable \\ntechnology.\\n2. Cyberattacks next focused on gathering \\ninformation on hundreds of millions of \\nAmericans. The Chinese actors are referred \\nto as Salt Typhoon actors. While the full \\nscope of the intrusion is not clear, nine \\nUS telecommunications companies were \\ncompromised, giving the actors the ability to \\naccess the cell phone records of nearly every \\nAmerican. \\n3. The third attack focused on US critical \\ninfrastructure. Known as Volt Typhoon actors, \\nthe hackers have compromised the IT systems \\nof US communications, energy, transportation, \\nand water and wastewater systems sectors. US \\nand allied cybersecurity experts believe that \\nChina has accessed some of these IT systems \\nfor at least five years. \\nChina’s DF-17 hypersonic missile represents a potent threat to US and \\nallied forces in the Western Pacific region.\\nThe South China Sea remains a flashpoint between China and its neighbors. \\nThe Chinese ship Yi Peng 3 was accused of sabotaging subsea cables in the \\nBaltic Sea in November 2024.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2de83d07-c043-4fe1-a4cf-9a6b74b92f5b', embedding=None, metadata={'page_label': '54', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='54 Goldman Sachs january 2025\\nAt the same congressional hearing, FBI Director \\nChristoper Wray testified about the scale of China’s \\nhacking operation: “If you took every single one \\nof the FBI’s cyber agents and intelligence analysts \\nand focused them exclusively on the China threat, \\nChina’s hackers would still outnumber FBI cyber \\npersonnel by at least 50 to one.”52\\n The Trump administration is expected to respond \\nmore forcefully to such active threats. Representative \\nMichael Waltz of Florida, who is the incoming \\nnational security advisor, a former Green Beret and \\nrecipient of four Bronze Stars, has stated that the \\nUS needs to go on offense and impose “higher costs \\nand consequences to private actors and nation-state \\nactors that continue to steal our data, that continue \\nto spy on us and that … are literally putting cyber \\ntime bombs on our infrastructure, our water systems, \\nour grids, even our ports.”53\\n The risk of cyberattacks and damage to US \\ninfrastructure is high. \\n We conclude with a warning from an article in \\nthe Economist on a 1922 book called The Problem \\nof China, published in England.54 The book author, \\nBertrand Russell, British philosopher and Nobel \\nlaureate, wrote that China, with its resources, \\npopulation and patriotic spirit, could become the \\n“greatest Power in the world after the United \\nStates” (our emphasis). However, he also wrote, \\n“the danger of patriotism is that, as soon as it has \\nproved strong enough for successful defence, it is \\napt to turn to foreign aggression.” \\nRussia-Ukraine War\\nThe Russia-Ukraine war has led to over 1 million \\ncasualties, including both injured and killed \\nRussians and Ukrainians. Our experts have highly \\ndivergent views on this war. Some believe that a \\nceasefire soon is likely, because both Russia and \\nUkraine have suffered heavy casualties, both \\ncountries are likely to have a shortage of military \\nequipment and supplies, Ukraine is facing a \\nshortage of troops and Russia is relying on North \\nKoreans to supplement its troops. \\n Sir Alex Younger’s base case is a peace deal. \\nGeneral Sir Nick Carter thinks an end to the war \\nor an armistice is most likely. Andrew Bishop of \\nSignum believes that talks in the first half of 2025 \\nwill progress, but a deal will not be achieved \\nbecause of Russia’s demands for territory and a \\nweak and subservient Ukraine. \\n In the meantime, Russia’s extensive cyberattacks \\non Ukraine, Europe and the US continue. According \\nto the US government’s Helsinki Commission, in \\nrecent years Russia has perpetrated cyberattacks, \\nGPS signal disruptions, physical sabotage, \\ndisinformation, covert operations and arson.55\\n Russian fighter jets have dumped fuel on allied \\nspy planes, and Russian warships have fired shots \\nat NATO vessels and aircraft.56 Russia is also \\nsuspected of having cut undersea communications \\ncables in the Baltic Sea.57\\n Irrespective of the evolution of the Russia-\\nUkraine war, Russia will be a source of sabotage, \\ncyberattacks, geopolitical disruption and market \\nvolatility in 2025. \\nThe Middle East\\nA few of our experts were particularly optimistic \\nabout peace in the Middle East—a region that has \\nbeen a powder keg since the Iranian Revolution \\nin the late 1970s. Estimates of casualties from the \\nIran-Iraq War through two Gulf wars, civil wars \\nin Yemen and Syria, and, most recently, the Israel-\\nGaza war and Israel-Lebanon war, are estimated at \\nover 3 million. \\n The rationale for their optimism was that \\nPresident-elect Trump will pressure Israel to \\nend both wars (Gaza and Lebanon) and will \\nfocus on incentivizing Saudi Arabia to join the \\nAbraham Accords. \\n The greatest uncertainty emanates from Iran. \\nIran has been weakened from two Israeli strikes \\nin 2024, the assassination of Iran’s defense leader, \\nQasem Soleimani, in 2020, Israel’s surgical killing \\nof Hamas, Hezbollah, and Iranian Revolutionary \\nGuard leadership, and the fall of Bashar al-\\nAssad in Syria. It also faces domestic economic \\nchallenges and prospects of instability given the \\nage and physical health of its leader, Ayatollah Ali \\nKhamenei. \\nFinland seized a Russian tanker after it was suspected of severing an \\nundersea cable.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6398e6f3-3085-4716-9937-b8f8ec1bd1fb', embedding=None, metadata={'page_label': '55', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='55Outlook Investment Strategy Group\\n The question is whether Iran ramps up \\nits nuclear program and provokes a response \\nfrom Israel. \\n Ian Bremmer of Eurasia Group believes that \\n“Iran’s only long-term options are reaching out … \\nto establish constructive relations with the US and \\nthe West … and/or developing a full-blown nuclear \\nweapons program.” He estimates that it would \\ntake Iran at least six months to develop nuclear-\\nweapons capability and a year or two to build a \\nmissile system for delivery of nuclear warheads. \\n Two of our experts think it is unlikely that Iran \\nwill ramp up its nuclear weaponization program. \\nThe US and Israel would detect the ramp-up \\nand Israel is likely to strike Iran to preempt the \\nbuild-up. \\n De-escalation is a more likely path.\\n The region remains a powder keg even if peace \\nis achieved in 2025. \\nNorth Korea\\nNorth Korea significantly advanced its military \\ncapabilities in 2024. It has also provided 10,000 \\ntroops as well as an estimated $5.5 billion in \\nmunitions to Russia. In return, Russia has supplied \\nair defense missiles to North Korea.58\\n There has been speculation that North Korea \\nhas asked Russia for advanced technology that \\ncould help it with its intercontinental ballistic \\nmissile program, satellites and nuclear submarines.\\n Our experts believe that Russia will be more \\nrestrained in its technology transfer to North \\nKorea given a desire for future rapprochement \\nwith the West. \\nCyber Threats\\nWe have already reviewed geopolitical cyber \\nthreats from China and Russia. Iran and North \\nKorea are also active in cyberattacks undertaken \\nfor both geopolitical and commercial objectives. \\n The remaining cyber risks are related to \\ncybercrime. Industries most often targeted are \\nhealth care, financial services, industrial technology \\nand energy.59\\n World cybercrime is projected to cost $10.5 \\ntrillion in 2025, according to Statista.60 This \\nstaggering estimate (and we believe it is a very rough \\nestimate) includes damaged and destroyed data, lost \\nproductivity, theft of intellectual property, theft of \\npersonal and financial data, fraud, embezzlement, \\npost-attack disruption to businesses and restoration \\nof hacked data and systems, and reputational harm.\\n Global cybersecurity spending is expected to \\nreach $212 billion this year.61\\n In its “Cybersecurity Forecast 2025” report,62 \\nGoogle warns: \\n• Cyberattackers will increasingly use AI. \\n• The “big four”—China, Russia, Iran and North \\nKorea—will remain the most active in cyber \\nactivities.\\n• Ransomware and extortion will be the most \\ndisruptive forms of cybercrime.\\n• Less-skilled actors will participate in \\ncybercrime. \\n• Cryptocurrency organizations will increasingly \\nbe targeted by attackers to steal digital assets. \\nCyber risks continue to grow.\\nTerrorism\\nThe overall terrorist threat has increased, according \\nto all national security and intelligence agencies. \\nThe impact of the Israel-Gaza war and the Israel-\\nLebanon war, and the threats from Russian and \\nother state actors, have all increased the threat \\nposed by both foreign terrorists and homegrown \\nterrorists inspired by foreign terrorist organizations. \\n General Bryan Fenton, commander of the US \\nSpecial Operations Command, recently shared \\nthat the impact of events like the Israel-Hamas \\nwar takes roughly two years to manifest itself.63 \\n“Violent extreme organizations” exploit such \\nevents and organize groups to rally against the \\nWest. He also stated that there has been “renewed \\ninterest in jihad” such as he has not seen since the \\nArab Spring over a decade ago. \\n Risk of terrorism has increased in 2025. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='66e3a03a-1cd7-4ec7-bfa9-d9ada2c4db4f', embedding=None, metadata={'page_label': '56', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='56 Goldman Sachs january 2025\\nKey Takeaways\\nOur two primary investment themes of US Preeminence and Staying \\nInvested have served our clients well over the past nearly 16 years. US \\nequities have outperformed other equity markets and staying invested \\nhas allowed clients to capture the 11-fold increase in US equities. These \\ninvestment themes remain valid. However, we do not expect US equities \\nto outperform other equities by the same magnitude; nor do we expect US \\nequities to replicate the high absolute returns observed in recent years. \\nThe key takeaways from our 2025 Outlook are as follows:  \\n• Steady, Above-Trend Global Growth: We expect global economic growth \\nto reach 3.1% in 2025, compared to its 2.9% trend rate. The US will grow \\nat 2.3%, propelled to above-trend growth by the momentum of 2024. \\nWe expect Japan to also grow above trend, but the Eurozone and the UK \\nwill likely record a third consecutive year of below-trend growth. While \\nemerging market countries together will grow slightly above trend, there \\nwill likely be some dispersion among the BRICS countries, with Russia \\ngrowing slightly above trend due to its war efforts, China also above trend, \\nand Brazil and India around or just below trend.\\n• Monetary Policy Easing: We believe that most major central banks in \\ndeveloped economies will continue the easing policies started in 2024. \\nJapan, on the other hand, will slowly raise rates, continuing the policy \\nthat ended its negative interest rates. In emerging markets, China will \\ncontinue to support its economy by easing monetary policy to supplement \\nits multipronged fiscal stimulus launched in 2024. We believe India might \\nhave room for modest easing, while Brazil will raise its policy rate further \\nto combat inflation. Russia faces a dilemma in which it is inclined to raise \\nrates to fight inflation but may have to ease modestly in the second half of \\n2025 to support growth.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='32e4a3a0-3a05-4d0c-9e24-455a0700236d', embedding=None, metadata={'page_label': '57', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='57Outlook Investment Strategy Group\\n• Recession Risk in the US: We have lowered our risk of a US recession \\nfor the year ahead to 20%, which is slightly above the unconditional \\nprobability of recession of about 18% since WWII. \\n• Modest Mid-Single-Digit Benchmark Returns: After a 9% return in 2024 \\nfor a 50% stocks-50% tax-exempt bonds benchmark and 10% return for \\na 50% stocks-50% taxable bonds benchmark, we expect more modest but \\nstill favorable returns for investors in 2025. We expect a well-diversified \\nportfolio that leverages our strategic asset allocation process to outperform \\na passive benchmark over time. We have made some marginal changes to \\nour model portfolios. \\n• Significant Geopolitical Risks: We face a period of significant geopolitical \\nrisks—possibly the greatest since the inception of ISG. The biggest risk is \\na deterioration in US-China relations because of an escalation in the trade \\nwar, China’s more aggressive maneuvers toward Taiwan and its more \\naggressive cyber activities in the US. We expect some progress toward \\npeace in the Middle East, but uncertainty about Russia remains. We expect \\nsome de-escalation from Iran. We do not believe the US debt trajectory is a \\nnear-term risk. \\n• Vigilance: As usual, we diligently watch for unexpected risks and remain \\nvigilant in search of market opportunities. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f2b63e11-b772-4d45-8aa8-d29b5c4b8ffb', embedding=None, metadata={'page_label': '58', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='58 Goldman Sachs january 2025\\n2025 Global \\nEconomic Outlook: \\nDiverging Paths \\nSECTION II\\nwhile global economies rarely follow the same trail, \\nthis year they are charting distinctly different courses. Resilient \\nUS growth stands in sharp contrast to the softer trajectories \\nwe expect in Europe and the UK, where recession risks remain \\nnearly twice as high as in the United States. At the same time, \\nJapan is expected to outpace its trend growth this year, as it \\ncontinues to emerge from decades of deflation.  \\n Similar divergences are visible even within regions. In \\nEurope, Germany is grappling with stagnation, while Spain \\ncontinues to flourish. Among emerging markets, parts of Latin \\nAmerica are witnessing robust domestic demand, whereas \\nChina is struggling with weak consumer confidence, a faltering \\nhousing market and policy uncertainty. \\n This uneven economic landscape is prompting varied policy \\nresponses. The Federal Reserve is reducing the pace of rate cuts \\nafter progress toward its inflation target slowed last year and \\nconcerns emerged among some FOMC members about the \\ninflationary impact of the incoming administration’s agenda. \\nAs a result, the Federal Reserve is likely to deliver fewer cuts \\nthan the ECB or BOE, which are working to manage weaker \\neconomic growth. Breaking from the pack, Japan’s central bank ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='377f4681-42e5-4af4-ae8d-3cdc6e82f7bc', embedding=None, metadata={'page_label': '59', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='59Outlook Investment Strategy Group\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='52c94389-751a-446a-ac1a-a1ab2a398126', embedding=None, metadata={'page_label': '60', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='60 Goldman Sachs january 2025\\nis set to further tighten policy as it cautiously \\nunwinds decades of ultra-loose policy. \\n Recent political developments are also \\nshaping the outlook. In the US, the incoming \\nadministration has pledged sweeping reforms \\nacross trade, immigration, regulation and taxation. \\nWhile these measures may bolster domestic \\ngrowth, they also raise uncertainties that could \\nweigh on global business investment. \\n At the same time, fiscal policies are influencing \\nthis year’s trajectory, following elections in countries \\nrepresenting 40% of the world’s population in \\n2024.64 While France is navigating a fiscal deficit \\ncrisis that limits its spending flexibility, the UK is \\nprioritizing public investment to address years of \\nunderfunding. In the US, expectations center on \\npro-growth initiatives such as tax cuts, deregulation \\nand infrastructure investment, although questions \\nremain about their long-term fiscal impact. \\n Still, these varied paths may still converge \\nat a common destination. While global growth \\nremains far from uniform across countries, it is still \\nexpected to edge above its historical trend this year. \\nAt the same time, artificial intelligence continues to \\npromise new avenues of potential growth, even as \\nadoption rates and regulatory hurdles differ widely \\namong regions. \\n Despite this favorable growth backdrop, most \\ncentral banks are expected to ease monetary policy \\nfurther this year. Due in part to this, the risk of \\nrecession remains far from our base case, making \\nit likely that most economies continue ascending \\ntoward higher elevations in 2025 (see Exhibit 75).  \\nUnited States: Standing Strong \\nThe US economy continued to exceed expectations \\nlast year. Far from slipping into a recession, it saw \\nreal growth of 2.8%, remaining firmly above its \\ntrend rate. At the same time, inflation cooled closer \\nto the US Federal Reserve’s 2% target, and the once \\noverheated labor market moved into better balance. \\nIn response to this progress, the Federal Reserve \\nbecame less restrictive, cutting its policy rate by a \\nfull percentage point in the final months of 2024. \\n As we look ahead, the new year brings new \\nchallenges. Investors have shifted from recession \\nfears to worries about the economic implications \\nof the incoming administration’s policies. Concern \\nis focused on the possibility that additional fiscal \\nstimulus will amplify inflationary pressures, \\nespecially if combined with tariff-related price \\nincreases and labor shortages tied to restrictive \\nimmigration policies. Although specifics remain \\nsparse, significant changes in trade, immigration \\nand fiscal policy are anticipated (see Exhibit 76). \\n This blend of policies is likely to lift inflation \\nhigher than it otherwise would have been. We \\nestimate that new tariffs will add approximately \\n0.4 percentage points to goods inflation, raising \\nour year-end core inflation forecast to 2.6% \\n(see Exhibit 77). Although this represents an \\nimprovement from last year’s level, the progress is \\nmore marginal than previously expected.\\n Still, a sustained resurgence in inflation is \\nunlikely. Tariffs generally cause a onetime hike in \\nthe price of the targeted items rather than ongoing \\nExhibit 75: ISG Outlook for Developed Economies\\nReal GDP Growth \\nAnnual Average (%)\\nHeadline Inflation*\\nAnnual Average (%)\\nCore Inflation*  \\nAnnual Average (%)\\nPolicy Rate**  \\nEnd of Year (%)\\n10-Year Bond Yield*** \\nEnd of Year (%)\\n2024 2025  \\nBase Case \\n2025  \\nGood Case \\n2025  \\nBad Case 2024 2025 2024 2025 2024 2025 2024 2025\\nUnited States 2.8 2.1–2.5 2.8 1.0 3.0 2.4–2.8 3.4 2.5–2.9 4.375 3.625 4.57 4.10–4.60\\nEurozone 0.7 0.8–1.2 1.3 0.3 2.3 1.8–2.2 2.8 2.0–2.4 3.00 1.75 2.37 1.75–2.25\\nUnited Kingdom 0.9 1.0–1.4 1.5 0.5 2.5 2.3–2.7 3.7 2.5–2.9 4.75 3.50 4.57 3.50–4.00\\nJapan -0.3 0.8–1.2 1.3 0.3 2.6 2.0–2.4 2.3 2.0–2.4 0.25 0.75 1.09 1.25–1.75\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Haver Analytics, Bloomberg. \\n* Inflation refers to CPI inflation. Japan core inflation excludes fresh food, but includes energy. \\n** The US policy rate refers to the midpoint of the Federal Reserve’s target range. The Eurozone policy rate refers to the ECB deposit facility. The Japan policy rate refers to the BOJ uncollateralized \\novernight call rate. \\n*** For Eurozone bond yield, we show the 10-year German bund yield. \\nForecasts are estimated, based on assumptions, are subject to revision and may change as economic and market conditions change. There can be no \\nassurance the forecasts will be achieved.  ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='05be9d3b-ae23-4a30-a979-85552646142c', embedding=None, metadata={'page_label': '61', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='61Outlook Investment Strategy Group\\nincreases. The transitory nature of past tariffs has \\ntypically led consumers to look through their price \\neffects when gauging future inflation trends. The \\nresearch staff of the Federal Reserve reached a \\nsimilar conclusion in a 2018 analysis, characterizing \\ntariff-induced price increases as temporary shocks \\nthat should not alter monetary policy. \\n Outside tariffs, the prospects for disinflation are \\nmore promising. Ongoing weakness in rental rates \\ncontinues to feed through to official housing inflation \\nwith a mechanical lag that reflects the annual lease \\nrenewal cycle (see Exhibit 78). At the same time, the \\ncooling labor market has eased pricing pressures in \\nthe employee-intensive service sector, which accounts \\nfor three-quarters of the weight in the Federal \\nReserve’s preferred inflation gauge (see Exhibit \\n79). Evidence of this can be seen in Exhibit 80, \\nwhere a range of indicators show that the demand \\nfor workers—which previously far exceeded their \\nsupply—has returned to more normal pre-pandemic \\nlevels. This shift should continue to put downward \\npressure on wage inflation, especially in the labor-\\nheavy service sector (see Exhibit 81). \\n While it’s possible that immigration \\nrestrictions and deportations under the incoming \\nadministration could disrupt this progress, we are \\nskeptical that this will be the outcome. Empirical \\nstudies consistently show limited evidence that \\nchanges in immigration significantly impact overall \\nwage growth. Moreover, net immigration flows \\ninto the US have already slowed considerably \\nsince their 2023 surge, with no clear signs of \\nrelated labor market tightening. Even if significant \\nadditional deportations were to occur, we expect \\nnet immigration to remain positive as new \\npermanent residents and visa-based entries add \\nworkers to the labor force (see Exhibit 82).\\n Aside from concerns around labor supply, \\nsufficient labor demand is critical at this stage. \\nExcess job openings have already contracted \\nsignificantly (see Exhibit 83). This raises the risk \\nthat any further softening could result in layoffs, \\nputting the economy at risk. While that remains \\nExhibit 77: Core PCE Inflation Forecast vs. No \\nTariff Scenario\\nCore PCE inflation is likely to end the year 0.4 percentage \\npoints higher than it would without tariffs.\\nCore PCE\\n% YoY\\nCore PCE No Tariffs\\n2.6\\n2.2\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n2019 2020 2021 2022 2023 2024 2025\\nForecast\\nData through November 2024. Forecast through 2025. \\nSource: Investment Strategy Group, BEA. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 78: US PCE Housing Inflation and \\nMarket Rents\\nOngoing weakness in rental rates continues to feed through \\nto official housing inflation with a lag.\\nHousing PCE \\nZillow Observed Rent\\nApartment List\\nBLS New Tenant Rent Index\\n3.4\\n-0.6\\n1.0\\n-5\\n0\\n5\\n10\\n15\\n20\\n2016 2017 2018 2019 2020 2021 2022 2023 2024\\n% YoY\\n4.8\\nData through November 2024. \\nNote: Zillow and Apartment List indices measure rents for a new lease by a new tenant.  \\nSource: Investment Strategy Group, Apartment List, Zillow, Haver Analytics.\\nExhibit 76: US Trade, Fiscal and Immigration \\nPolicy Baseline Assumptions\\nWe expect meaningful changes, despite falling short of full \\ncampaign proposals.\\nPolicy Issue ISG Baseline Assumptions\\nTrade Policy Tariffs on Chinese imports increase 20pp in H1 2025.  Tariffs \\nexpand to other countries in H2 2025. The overall average tariff \\nrate rises approximately 4pp.\\nFiscal Policy The 2017 Tax Cuts and Jobs Act (TCJA), worth around 1.5% of \\nGDP per year, is extended in full. Limited additional tax cuts will \\nmainly impact 2026.\\nImmigration \\nPolicy\\n500k additional deportations per year. Overall net immigration \\nfalls just below 500k.\\nSource: Investment Strategy Group.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0ffcd926-46ff-45bd-8a42-6fae1027f185', embedding=None, metadata={'page_label': '62', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='62 Goldman Sachs january 2025\\na legitimate concern, we expect resilient labor \\ndemand amid another year of solid growth, with \\nreal GDP expanding 2.3% in 2025. \\n Our economic forecast is rooted in the \\nresilience of the US consumer, whose aggregate \\nspending makes up 70% of GDP. As has been \\nthe case throughout this economic expansion, \\nExhibit 79: Contribution to US Core PCE Inflation\\nServices disinflation should continue despite tariff-driven \\ngoods inflation. \\n2.6\\n2016 2017 2018 2019 2020 2021 2022 2023 20252024\\n% YoY\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\n6 Core Services Ex-Housing\\nHousing\\nCore Goods\\nCore PCE \\nForecast\\nData through November 2024. Forecast through 2025. \\nSource: Investment Strategy Group, BEA. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 81: US Wage and Employment \\nCost Growth\\nA cooling labor market is putting downward pressure on \\nwage inflation.\\n% YoY\\nAverage Hourly Earnings\\nAtlanta Fed Wage Tracker\\nEmployment Cost Index\\nIndeed\\n4.0\\n4.3\\n3.9\\n3.2\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n2017 2018 2019 2020 2021 2022 2023 2024\\nData through November 2024. \\nNote: Horizontal dotted line indicates the estimated rate of wage growth consistent with  \\n2% inflation.  \\nSource: Investment Strategy Group, Atlanta Fed, Indeed Hiring Lab, Haver Analytics. \\n \\nExhibit 80: Measures of US Labor \\nMarket Tightness\\nThe labor market has returned to more normal  \\npre-pandemic levels.\\nIndex\\nJobs-Workers Gap\\nQuits Rate\\nLabor Differential\\nNFIB Positions Unable to Fill\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n2017 2018 2019 2020 2021 2022 20242023\\n2019 Average = 1\\n0.3\\n0.6\\n1.0\\n0.9\\nData through November 2024. \\nNote: The Conference Board labor differential is a survey measure of consumers who think jobs \\nare plentiful versus those who think jobs are hard to get. National Federation of Independent \\nBusiness (NFIB) survey is the percentage of firms with positions unable to fill. \\nSource: Investment Strategy Group, BLS, Conference Board, National Federation of Independent \\nBusiness, Haver Analytics.  \\nExhibit 82: US Annual Net Immigration\\nNet immigration is set to remain positive, even with \\nadditional deportations.  \\nForecast\\n460\\n-1,000\\n 0\\n 1,000\\n 2,000\\n 3,000\\n 4,000\\n2000 2004 2008 2012 2016 2020 2024 2028\\nOther Foreign National (Undocumented)\\nINA Nonimmigrant (Visas)\\nPermanent Resident\\nTotal Net\\nThousands\\nData through 2023. Forecast through 2028. \\nNote: CBO historical data with ISG forecasts based on an extra 500k deportations. \\nSource: Investment Strategy Group, CBO, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2e909a25-8ec2-42ee-974a-69eab35914f5', embedding=None, metadata={'page_label': '63', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='63Outlook Investment Strategy Group\\nconsumption should continue to benefit from \\nongoing job gains and income growth (see Exhibit \\n84). Healthy balance sheets—compliments of \\nsignificant gains in net worth in recent years—are \\nalso likely to support spending, as will demand \\nfor big-ticket items as the Federal Reserve reduces \\npolicy rates. Tax cuts later in the year could \\nprovide a small additional tailwind, but we do not \\nview fiscal policy as a key driver of the spending \\noutlook. Any fiscal package is likely to prioritize \\npreventing a fiscal cliff by extending the 2017 \\ntax cuts, rather than providing households with \\nsignificant new cash flows (see Exhibit 85). \\n Business spending, the other key component of \\ndomestic demand, is expected to grow at a modest \\npace. Policy-driven incentives could influence \\nsome investment decisions, but there is little \\nreason to expect a broader surge in investment. \\nBusiness confidence and investment intentions \\nremain subdued, even though both have improved \\nsince the November election (see Exhibit 86). \\nPersistent concerns over inflation and high labor \\ncosts continue to weigh on sentiment. At the same \\ntime, there is scant evidence that overly restrictive \\nor burdensome tax or regulatory policies are \\nencumbering businesses (see Exhibit 87). Trade \\nuncertainty and the waning effects of government \\nincentives for factory construction represent two \\nadditional headwinds to an investment boom. \\n Against this backdrop, the Federal Reserve \\nis likely to continue reducing policy rates. We \\nanticipate three 25-basis-point cuts this year, \\nExhibit 83: US Beveridge Curve (Job Openings vs. \\nUnemployment Rate)\\nExcess job openings have contracted significantly, with \\nfurther declines risking rising layoffs.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n2 4 6 8 10 12 14 16\\nUnemployment Rate (%)\\nDec 2000–Jun 2009\\nJul 2009–Mar 2020\\nApr 2020–Present\\nJob Openings Rate (%)\\nData through November 2024. \\nSource: Investment Strategy Group, BLS. \\n \\n \\nExhibit 85: US Primary Balance With Tax Cut \\nExtension Contribution\\nExtending 2017 tax cuts prevents a fiscal contraction, with \\nany boost unlikely to be felt until 2026.\\n% of GDP\\nPrimary Balance (CBO) Extend TCJA (Current Tax Code)\\n-14\\n-12\\n-10\\n-8\\n-6\\n-4\\n-2\\n0\\n2\\n4\\n6\\n1990 1994 1998 2002 2006 2010 2014 2018 2022 2026 2030\\nForecast\\nData through 2023. Forecast through 2030. \\nNote: CBO estimates used for primary balance without Tax Cuts and Jobs Act (TCJA) extensions. \\nTax Foundation estimates used for contribution of TCJA extensions.  \\nSource: Investment Strategy Group, CBO, Tax Foundation. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 84: US Real Labor Income and Real \\nPersonal Consumption Expenditure\\nConsumption should continue to benefit from solid real \\nincome growth. \\nForecast\\nReal Labor Income\\nReal Personal Consumption Expenditure\\n2.2\\n2.0\\n-10\\n-5\\n0\\n5\\n10\\n2005 2007 2009 2011 2013 2015 2017 2019 2021 2023 2025\\n% YoY\\nData through November 2024. Forecast through 2025. \\nSource: Investment Strategy Group, BEA, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c8a79eba-812a-4083-bd20-ff0e7f119b13', embedding=None, metadata={'page_label': '64', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='64 Goldman Sachs january 2025\\nbringing the policy rate to 3.5–3.75%. While the \\nFederal Reserve is expected to initially look past \\ntariff-driven inflation, rate cuts could pause earlier \\nthan we expect if inflation expectations begin to \\nrise, the labor market retightens or a fiscal stimulus \\nlarger than we expect is introduced. That said, we \\nthink the hurdle for the Federal Reserve to restart \\nrate hikes is very high. \\n Resilient consumer spending, a balanced labor \\nmarket and continued monetary easing create a \\nsolid foundation for economic growth in 2025. As \\na result, we have lowered our year-ahead recession \\nprobability to 20%, close to the long-term average \\nbut below current consensus (see Exhibit 88). Just \\nas the US economy repeatedly defied recession \\nexpectations in recent years, we see it again \\nstanding strong in the face of new uncertainties in \\nthe year ahead. \\nThe Eurozone: Gathering Clouds \\nThe Eurozone economy outperformed forecasts \\nlast year, expanding by 0.7% compared to \\nexpectations of 0.5%. Despite this modest upside \\nsurprise, absolute growth remained below its \\ntrend for a second consecutive year and masked \\nnotable divergences across countries and sectors \\n(see Exhibit 89). While manufacturing-oriented \\neconomies, particularly Germany, struggled to \\ngain momentum and even teetered on the brink \\nExhibit 88: 12-Month US Recession Probability \\nAccording to WSJ Survey of Economists\\nWe lowered our year-ahead recession probability to 20%, \\nbelow consensus of 26%. \\n26\\nISG: 20\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n2006 2008 2010 2012 2014 2016 2018 2020 2022 2024\\n%\\nData through October 2024. \\nNote: Average probability of recession starting within 12 months. Shaded periods  \\ndenote recessions.  \\nSource: Investment Strategy Group, Wall Street Journal . \\n \\n \\nExhibit 86: US Real Business Investment and \\nSurveyed Business Investment Intentions\\nInvestment intentions have improved slightly since \\nNovember, but remain subdued.\\nBusiness Investment (Left)\\nCapex Intentions in Regional \\nFed Surveys,\\nForward 3 Months (Right)\\n-3.5\\n-3.0\\n-2.5\\n-2.0\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-20\\n-15\\n-10\\n-5\\n0\\n5\\n10\\n15\\n20\\n2006 2008 2010\\n2012 2014 2016 2018 2020 2022 2024\\n% YoY Average Z-Score\\nData through November 2024.  \\nNote: Investment intentions are measured as average z-scores of indicators from regional \\nFed surveys.  \\nSource: Investment Strategy Group, Fed Regional Surveys, BEA. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 87: US Small Business Single Most \\nImportant Problem\\nConcerns in 2024 were focused on labor shortages and \\ninflation, unlike in 2016.\\nMid-2015\\nCurrent\\n23\\n32\\n78 80\\n56\\n5 4\\n28\\n98\\n0\\n69\\n51\\n87\\n6\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nSmall Business\\nOptimism\\nRegulation\\n& Red Tape\\nTaxes Cost of\\nLabor\\nCost of\\nFinancing/\\nInterest Rates\\nInﬂation Poor\\nSales\\nPercentile (%)\\nData as of November 2024. \\nNote: Percentile ranks since 1973. \\nSource: Investment Strategy Group, NFIB.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0c553bad-2f21-4b29-8f0f-a7840ae6803e', embedding=None, metadata={'page_label': '65', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='65Outlook Investment Strategy Group\\nof recession, services-driven economies like Spain \\nenjoyed stronger growth. \\n These divergences are likely to persist in 2025. \\nUnlike the region’s service sector, the Eurozone’s \\nmanufacturing sector faces headwinds from both \\nstructural weaknesses and the renewed threat \\nof US tariffs. While details on potential tariffs \\nremain unclear, their threat alone is likely to \\ndampen business investment in manufacturing-\\nintensive industries. In fact, trade uncertainty \\ncould weigh more heavily on the region’s GDP \\nthan the direct revenue loss from tariff-impacted \\nexports. According to International Monetary \\nFund (IMF) estimates, trade policy uncertainty tied \\nto a 10-percentage-point across-the-board US tariff \\nhike would account for 87% of the anticipated \\neconomic impact, reducing simulated Eurozone \\nGDP by 30 basis points this year and 60 basis \\npoints in 2026.65\\n Against this challenging backdrop, we project \\nthe Eurozone economy will grow just 1% this year, \\nextending its streak of below-trend performance. \\nThe primary driver of growth is consumption, \\nsupported by a resilient labor market, steady real \\nincome gains, receding inflation and a moderation \\nin household savings that currently remain very \\nelevated (see Exhibit 90). \\n Progress on inflation toward the 2% target of \\nthe European Central Bank (ECB) should allow the \\ncentral bank to continue easing monetary policy \\n(see Exhibit 91). Following a reduction in the \\npolicy rate from 4% to 3% in 2024, we expect the \\nECB to reduce rates by 125 basis points in 2025, \\nbringing the policy rate to 1.75%. If tariff-related \\nheadwinds intensify, we believe the ECB could take \\nrates further below neutral.\\n Monetary policy alone, however, cannot \\nresolve the Eurozone’s structural challenges. Fiscal \\nsupport and structural reforms remain critical to \\nExhibit 90: Eurozone Excess Savings\\nEuropean households have accumulated substantial excess \\nsavings that could support faster consumption growth. \\nExcess Savings (Left)\\nExcess Savings, % of GDP (Right)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\n1,600\\n2020 2021 2022 2023 2024\\n€ Billions % of GDP\\nData through Q2 2024. \\nSource: Investment Strategy Group, Haver Analytics.\\nExhibit 89: Select Eurozone Countries’ Cumulative \\nGDP Growth Since Q4 2019\\nThe German economy has considerably lagged the rest of \\nthe Eurozone.\\n9.7\\n8.5\\n7.0 7.0 6.6\\n5.6\\n4.7\\n4.0\\n0.1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nGreece Netherlands Portugal Belgium Spain Italy Eurozone France Germany\\n% \\nData through Q3 2024. \\nSource: Investment Strategy Group, Haver Analytics.\\nExhibit 91: Eurozone Inflation Outlook\\nWe expect disinflation to continue in 2025 driven by \\nmoderating services inflation.\\nForecast\\n2.0\\n-2\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n2019 2020 2021 2022 2023 2024 2025\\n% YoY Contributions\\nCore Goods\\nServices\\nFood, Alcohol and Tobacco\\nEnergy\\nHICP\\nData through November 2024. Forecast through 2025. \\nSource: Investment Strategy Group, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='bd1ea871-bf89-40b5-80f6-f5de51bd010b', embedding=None, metadata={'page_label': '66', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='66 Goldman Sachs january 2025\\nrestoring the region’s competitiveness, as outlined \\nin the September 2024 EU competitiveness report \\nby former Italian Prime Minister Mario Draghi.66 \\nUnfortunately, meaningful progress in these \\ncritical areas will likely be limited by political \\nfragmentation across the region. \\nUnited Kingdom: A Muted Recovery\\nUK growth remained below trend for a \\nsecond consecutive year in 2024, even as GDP \\nexpanded by a better-than-expected 0.9%. \\nThe upside surprise largely reflected a recovery \\nin fixed investment that began in late 2023. \\nHousehold consumption also contributed, though \\nimprovements were subdued despite strong real \\ndisposable income growth of 6.2% annualized \\nin the first half of last year. High interest rates \\nand lingering concerns about the durability of \\nhousehold income gains likely led consumers to \\nfavor saving over spending (see Exhibit 92). \\n Consumption should continue growing this \\nyear, but at a more measured rate. The slowdown \\nreflects moderating real income gains tied to a \\ncooling labor market. However, spending should \\nstill benefit from receding inflation and declining \\ninterest rates, which will lower borrowing costs, \\nease household debt burdens, reduce the impetus \\nto increase savings and provide a modest boost to \\ninvestment activity. \\n Turning to fiscal policy, the autumn budget has \\ncreated room for public investment and government \\nspending. This may be a welcome development \\nfor the UK’s long-term growth potential, given the \\npersistent underinvestment of past decades (see \\nExhibit 93). The new Labour Party government’s \\npro-growth, pro-investment agenda aims to reverse \\nthe underinvestment trend. If successful, it could lift \\nmedium- to long-term growth prospects, especially if \\ncoupled with a closer EU-UK relationship—another \\nof the government’s goals. \\n However, near-term risks remain. The autumn \\nbudget also signals a slower pace of fiscal \\nconsolidation, raising upside risks to inflation \\nthat could complicate monetary policy. Despite \\ndelivering two 25-basis-point cuts in 2024, the \\nBank of England (BOE) has maintained a cautious \\nstance amid persistently high services inflation. We \\nexpect receding pricing pressures going forward, \\nwith our forecast calling for core inflation to \\ndecline by a full percentage point in 2025 (see \\nExhibit 94). This should allow the BOE to cut rates \\nby 125 basis points, bringing the policy rate to \\n3.5% by year-end.\\n The combination of moderating inflation and \\nfiscal support underpins our expectation for GDP \\ngrowth to pick up modestly to 1.2% this year. \\nEven so, risks to the outlook remain skewed to the \\ndownside, reflecting potential headwinds from US \\ntariff hikes, rising layoffs and resistance to fiscal \\nstimulus. \\nExhibit 93: UK vs. G-7 Investment Ratio \\nFixed investment in the UK has consistently lagged that in \\nother G7 countries. \\n23\\n18\\nRest of G7 Range\\nRest of G7 Median\\nUK\\n12\\n17\\n22\\n27\\n32\\n37\\n1990 1996 2002 2008 2014 2020\\n% of GDP\\nData through 2023. \\nSource: Investment Strategy Group, Haver Analytics.\\nExhibit 92: UK Savings Rate as a Share of \\nDisposable Income\\nUK households have saved an increasing share of their \\ndisposable income.\\nHousehold Savings Rate\\nAverage 2008–19\\n9.7\\n8.2\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n2008 2010 2012 2014 2016 2018 2020 2022 2024\\n% of Disposable Income\\nData through Q3 2024. \\nSource: Investment Strategy Group, Haver Analytics.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ddc06d74-8a8d-4ac7-a5ab-338e5f0ebfb0', embedding=None, metadata={'page_label': '67', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='67Outlook Investment Strategy Group\\nJapan: A Fragile Recovery\\nJapan’s economy made further headway last year \\nin its decades-long battle with deflation. Rising \\nincomes and stronger household consumption \\nsupported modest price increases, providing \\ntangible evidence that deflation may finally be \\nending. Yet this long-awaited progress remains \\nfragile, as new risks threaten to undermine the \\ngains and test policymakers’ resolve. \\n Admittedly, last year’s 0.3% decline in GDP \\nraises questions about the durability of Japan’s \\nrecovery. However, this contraction largely \\nreflected one-off factors, such as temporary supply \\nchain constraints in the auto sector that weighed \\non growth early in the year. The economy grew \\nat or above trend later in the year, driven by \\nimproving real disposable incomes that boosted \\nhousehold consumption. Moreover, the year saw \\nthe largest wage increases in more than three \\ndecades following the annual shunto negotiations \\n(see Exhibit 95). These gains helped lift average \\nhourly earnings and sustain Japan’s high and rising \\nlabor force participation rate amid ample job \\nvacancies (see Exhibit 96). \\n In response to this improving economic \\nbackdrop, the Bank of Japan (BOJ) ended its highly \\naccommodative policies last year. It abandoned \\nyield curve control and negative interest rates, lifting \\nthe effective policy rate to 0.25%. We expect this \\nnormalization to continue, with two additional \\n25-basis-point hikes this year. This would bring the \\npolicy rate to 0.75%, just below the BOJ’s estimated \\n1.0–2.5% neutral range. \\nExhibit 96: G-4 Labor Force Participation Rates\\nJapan’s labor force participation has increased significantly \\nsince 2013. \\n75.0\\n75.4\\n78.3\\n81.6\\n68\\n72\\n76\\n80\\n84\\n2005 2008 2011 2014 2017 2020 2023\\n%\\nUnited States\\nJapan\\nEurozone\\nUnited Kingdom\\nData through Q3 2024.  \\nSource: Investment Strategy Group, Haver Analytics.\\nExhibit 95: Japan Negotiated Spring \\nWage Increases\\nThe 2024 shunto negotiations resulted in the largest wage \\nincrease since 1991.\\n5.3\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n1990 1996 2002 2008 2014 20201992 1998 2004 2010 2016 20221994 2000 2006 2012 2018 2024\\n% YoY\\nData through 2024. \\nSource: Investment Strategy Group; Ministry of Health, Labour and Welfare; Haver Analytics. \\n \\n \\nExhibit 94: UK Inflation Outlook\\nDisinflation is expected to continue in 2025 driven mainly by \\nslowing services inflation.  \\nForecast\\n2.3\\n-2\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n2019 2020 2021 2022 2023 2024 2025\\n% YoY Contributions\\nCore Goods\\nServices\\nFood, Alcohol and Tobacco\\nEnergy\\nCPI\\nData through November 2024. Forecast through 2025. \\nSource: Investment Strategy Group, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ef80a9b6-788e-4672-80d6-1b0f74628e10', embedding=None, metadata={'page_label': '68', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='68 Goldman Sachs january 2025\\n More broadly, Japan is well positioned to build \\non its progress toward ending deflation. Early \\nsigns from the next round of shunto negotiations \\npoint to another year of substantial wage growth. \\nAt the same time, there is broad political support \\nfor a significant minimum wage increase. Both \\ndevelopments should strengthen household \\nconsumption, a critical input in our forecast for \\n1% GDP growth this year. Higher wages will also \\nfeed through to higher services prices, keeping \\ninflation near the BOJ’s 2% target in our forecast.\\n Despite this benign outlook, significant risks \\nremain. Japan runs a sizable $70 billion trade \\nsurplus with the US, leaving it vulnerable to new US \\ntariffs, particularly in the auto sector (see Exhibit \\n97). If tariffs are limited to a 10-percentage-point \\nrate on Japanese cars, the direct effect on GDP \\nwould be a manageable 0.1% drag. However, the \\nbroader consequences of heightened trade policy \\nuncertainty and spillover effects from potential \\ntariffs on Japan’s other trading partners could pose \\na much more significant risk to growth. \\nEmerging Markets: Mounting Headwinds\\nGrowth in emerging markets (EMs) was better \\nthan forecast last year. Ongoing disinflation \\nallowed central banks to ease monetary \\npolicy, while resilient labor markets supported \\nconsumption growth. Strong export demand \\nalso played a role, improving countries’ external \\nbalances and facilitating foreign reserve \\naccumulation. Even so, EM growth moderated \\nslightly to 4.2%, largely due to slower \\ngrowth in China and India—the two \\nlargest EM economies. \\n Looking ahead, we expect EM \\ngrowth to slow further to 4.1% in 2025 \\n(see Exhibit 98). Mounting headwinds, \\nparticularly trade and tariff uncertainty, \\nwill likely weigh on EM economies. \\nCountries heavily reliant on exports to \\nthe United States, such as Mexico and \\nthe manufacturing hubs in Asia, remain \\nespecially vulnerable to broad-based \\nExhibit 98: Emerging Markets Real GDP Growth \\n(PPP-Weighted)\\nWe expect EM growth to slow to 4.1% in 2025. \\n4.4\\n5.3\\n2.9\\n2.0\\n5.3\\n7.7\\n3.6\\n3.2\\n4.2\\n5.0\\n3.0\\n1.8\\n4.8\\n6.5\\n3.9\\n3.4\\n4.1\\n4.8\\n2.9\\n2.3\\n4.4\\n6.4\\n1.4\\n2.3\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nEM EM Asia CEEMEA Latam China India Russia Brazil\\n% YoY\\n2023\\n2024 ISG Estimate\\n2025 ISG Forecast\\nData through Q3 2024. Forecasts from Q4 2024 through 2025. \\nSource: Investment Strategy Group, International Monetary Fund. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 97: Japan-US Trade Balance by Major \\nProduct Categories\\nJapan runs a large trade surplus with the US, driven by \\nauto exports.\\n45.5\\n18.8\\n10.0 8.1 5.9\\n-4.2 -4.8\\n-10.4 -12.4\\n-20\\n-10\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\nTransport\\nEquipment\\nNonelectrical\\n Machinery\\nElectrical\\n Machinery\\nOther\\nGoods\\nManufactured\\nGoods\\nRaw\\nMaterials\\nChemicals\\nFoodstuff\\nMineral\\nFuels\\nUS$ Billions \\nData as of November 2024. \\nNote: Showing 12-month totals through November 2024. \\nSource: Investment Strategy Group, Haver Analytics. \\n \\nGrowth in emerging markets was \\nbetter than forecast last year. \\nOngoing disinflation allowed central \\nbanks to ease monetary policy, while \\nresilient labor markets supported \\nconsumption growth.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f0e16ddb-0869-463b-8633-30fc495f110e', embedding=None, metadata={'page_label': '69', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='69Outlook Investment Strategy Group\\nUS tariffs. Even open EM economies not directly \\ntargeted by tariffs may still feel the indirect effects \\nof weaker global trade among their key trading \\npartners in Europe and China (see Exhibit 99). \\n Unfortunately, EM policymakers have limited \\ntools with which to counter this difficult external \\nbackdrop. Years of generous government spending \\nand higher borrowing costs have strained EM \\ngovernment balance sheets, necessitating tighter \\nfiscal policy. Should these measures fall short, \\nalready elevated public debt-to-GDP ratios could \\nclimb even higher. \\n EM central banks face similar challenges in \\ndelivering stimulative policy. Inflation remains \\nabove target levels in many EM countries, as \\ntight labor markets and high wage growth have \\nbolstered services inflation (see Exhibit 100). \\nAdditionally, our modest expectations for easing \\nin US monetary policy set a practical \\nlower bound for EM rates, and concerns \\nover currency depreciation will also \\ndiscourage more aggressive easing. \\n For these reasons, the risks to our EM \\ngrowth forecast are tilted to the downside.  \\nEmerging Asia \\nEmerging Asia’s growth story last year ran \\ndeeper than the overall numbers revealed. \\nAlthough slower growth in India and \\nChina weighed on the headline figure, the \\nrest of the region saw a resurgence, with growth \\naccelerating from 3.5% to 4.2%. This rebound \\nwas driven by strong export growth in countries \\nsuch as Korea, Taiwan, Malaysia, Vietnam and \\nThailand. Partly in response to this broader growth, \\ncentral banks across the region remained cautious, \\ndelivering only modest rate cuts despite inflation \\ngenerally falling to or below their targets. \\n Looking forward, the region’s export demand \\nfaces opposing forces. On the one hand, new US \\ntariffs and slowing Chinese growth pose clear \\nheadwinds. On the other hand, some countries \\ncould benefit from the frontloading of export \\ndemand ahead of anticipated tariffs or from \\nbroader supply chain realignment.\\n One such beneficiary since the last trade war \\nbetween the US and China has been India. Its \\nmanufacturing goods surplus with the US has \\nExhibit 99: Total Goods Exports and Goods \\nExports to the US as a Percentage of GDP\\nMexico, manufacturing hubs in Asia, and other open \\neconomies are vulnerable to broad-based US tariffs.\\nVietnam\\nMalaysia\\nCzechia\\nHungary\\nUAE\\nThailand\\nPoland\\nKorea\\nMexico\\nSaudi Arabia\\nSouth Africa\\nRomania\\nChile\\nPeru\\nRussia\\nTürkiye\\nIndonesia\\nChina\\nPhilippines\\nBrazil\\nColombia\\nIndia\\nArgentina\\nEgypt\\nTotal Exports (Left)\\nExports to US (Right)\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n0\\n20\\n40\\n60\\n80\\n100\\n% of GDP % of GDP\\nData as of 2023. \\nSource: Investment Strategy Group, International Monetary Fund. \\n \\nExhibit 100: Emerging Markets Services Inflation\\nTight labor markets and high wage growth have bolstered \\nEM services inflation. \\nInterquartile Range\\nMedian\\n6\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n2014 2016 2018 2020 2022 2024\\n% YoY\\nData through November 2024. \\nNote: Includes services inflation data from Brazil, Mexico, Colombia, Chile, Russia, Poland, \\nCzechia (HICP inflation), Hungary, Türkiye, China, Malaysia, South Korea and Taiwan.  \\nSource: Investment Strategy Group, Haver Analytics.\\nEmerging Asia’s growth story last \\nyear ran deeper than the overall \\nnumbers revealed. Although slower \\ngrowth in India and China weighed \\non the headline figure, the rest of the \\nregion saw a resurgence.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d38c57ec-9f21-4342-a9a0-43d091958c06', embedding=None, metadata={'page_label': '70', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='70 Goldman Sachs january 2025\\nsurged since 2017 by $33 billion, to $56 billion \\n(see Exhibit 101). However, this success may now \\ndraw scrutiny from the new US administration. \\nDespite this risk, we expect India to maintain \\nits status as the fastest-growing major economy, \\nunderpinned by resilient domestic consumption \\nand robust services activity. \\nChina \\nChina’s economy remains in the doldrums. Two \\nyears after its chaotic exit from the government’s \\n“zero-COVID” policy, occasional bursts of cyclical \\nactivity still fade almost as quickly as they appear. \\nThese repeated false dawns, combined with \\npolicymakers’ inability to chart a clear path to a \\nmore balanced economy, have left the durability of \\nany recovery in question. \\n As a case in point, the strong 6% pace of GDP \\ngrowth in last year’s first quarter was comfortably \\nabove Beijing’s “around 5%” target. But growth \\nfaltered to just 2% in the second quarter, \\nprompting a round of incremental measures \\nfocused more on limiting economic downside \\nrisks than reviving the economy. It was not until \\nlate September that a more comprehensive policy \\npackage was unveiled, rekindling hopes that \\nChina’s economy might be turning the corner. \\n China’s economic challenges, evident in these \\nfrequent growth swings, are both cyclical and \\nstructural in nature. Cyclically, adverse terms of \\ntrade and subdued domestic demand in the wake \\nof the pandemic, combined with inconsistent policy \\nsupport, have resulted in considerable excess supply, \\nparticularly of solar panels and autos. Structurally, \\nChina’s overreliance on credit and investment-\\ndriven growth in the property sector and by local \\ngovernments has fostered a persistent debt overhang \\nthat is constraining growth (see Exhibit 102). \\n In addition to these hurdles, higher US tariffs \\nare coming into focus. In a plausible scenario in \\nwhich the US places additional 20% tariffs on all \\nChinese goods, China’s annual goods export growth \\ncould drop from around 5% in 2024 to close to \\nzero. While the economic impact would still be \\nmanageable, the resulting 0.6-percentage-point drag \\non GDP is yet another unhelpful headwind (see \\nExhibit 103). \\n Against this backdrop, we expect pro-growth \\ninitiatives this year. Fiscal policy plays a dominant \\nrole in our forecast, with the official fiscal deficit \\nwidening from 3% to nearly 4% of GDP. We also \\nforesee additional rate cuts totaling 30 basis points, \\nfurther liquidity injections and other quasi-fiscal \\nmeasures (see Exhibit 104). Still, we think it is \\nunlikely policymakers will fully abandon their \\npreference for gradualism and downside risk \\nmanagement. Policy measures are therefore likely \\nto remain incremental, reactive and calibrated. \\n Considering the various crosscurrents and \\nassuming an additional 20% tariff rate, we expect \\nExhibit 101: India Goods Trade Surplus \\nWith the US\\nIndia has been a beneficiary of the last trade war between \\nChina and the US.\\nTotal Goods Trade Surplus\\nManufacturing Trade Surplus\\n45\\n56\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n2006 2008 2010 2012 2014 2016 2018 2020 2022 2024\\nUS$ Billions (12-Month Rolling Sum)\\nData through October 2024. \\nNote: Classification of manufacturing goods based on North American Industry Classification \\nSystem (NAICS) codes.  \\nSource: Investment Strategy Group, Haver Analytics, Census Bureau.\\nExhibit 102: China’s Debt-to-GDP Ratio \\nChina’s total outstanding debt has reached almost 300% of \\nGDP and is weighing on the economy.\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n2006 2008 2010 2012 2014 2016 2018 2020 2022 2024\\n% of GDP\\nNon-Financial Corporate Sector\\nHouseholds\\nGovernment\\nTotal 296\\nData through 2023. Estimate through 2024. \\nSource: Investment Strategy Group, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='354be3c2-935c-4942-99d3-784eb831f431', embedding=None, metadata={'page_label': '71', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='71Outlook Investment Strategy Group\\nChina’s GDP growth to slow from an estimated 4.8% \\nin 2024 to 4.4% this year. Household consumption \\nand policy support are expected to be the main \\ndrivers of growth, while property investment remains \\na drag (see Exhibit 105). Our forecast also calls for \\nsubdued price increases, with headline consumer \\ninflation rising modestly to a still tepid 0.8%.\\n The key downside risks to this outlook \\ninclude a larger-than-expected tariff rate and an \\ninadequate policy response. China’s debt overhang \\nalso remains a lingering risk. On the upside, a \\ngreater sense of urgency among policymakers \\ncould result in more proactive measures aimed at \\ndirectly boosting domestic demand and mitigating \\ndeflationary pressures. \\nCentral and Eastern Europe, the Middle East \\nand Africa\\nAmid global economic turbulence, parts of Central \\nand Eastern Europe, the Middle East and Africa \\ndemonstrated surprising resilience last year. \\nEastern Europe, for example, saw a resurgence in \\nconsumption demand, as receding inflation and \\nhealthy labor markets lifted real wage growth. \\nSimilarly, the South African economy was bolstered \\nby fewer electricity outages. \\n However, challenges in other parts of the region \\nremain. In Türkiye, although economic reforms \\nare bearing fruit, the country’s staggering 44% \\ninflation rate necessitates an extended period \\nExhibit 103: Exports’ Share of and Contribution to \\nChina’s GDP Growth \\nExports were an important growth driver for the Chinese \\neconomy in 2024.\\nShare of GDP Growth (Left)\\nContribution to GDP\\nGrowth (Right)\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\n-20\\n-10\\n0\\n10\\n20\\n30\\n40\\n50\\n2015 2016 2017 2018 2019 2020 2021 2022 2023 2024\\nPercentage Points, YTD% of Growth, YTD\\nData through Q3 2024. \\nSource: Investment Strategy Group, Haver Analytics. \\n \\n \\n \\n \\nExhibit 104: China’s Fiscal Financing \\nChina’s fiscal policy is expected to become more \\nexpansionary in 2025.  \\n7.4\\n9.5\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n2007 2010 2013 2016 2019 2022 2025\\n% of GDP\\nForecast\\nData through 2024. Forecast through 2025. \\nNote: This chart measures China’s overall fiscal stance by aggregating all financing activities by \\nthe central and local governments, as well as those by the quasi-government agencies. \\nSource: Investment Strategy Group, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 105: Chinese Housing Sector Activities \\nHousing investment will likely continue to decline given the \\nmuch larger correction in new starts. \\n70\\n57\\n30\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n2019 2020 2021 2022 2023 2024\\nIndex (2019 = 100), 3-Month Moving Average  \\nReal Estate Investment\\nFloor Space Sales\\nFloor Space New Starts\\nData through November 2024. \\nSource: Investment Strategy Group, Haver Analytics.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='aa1e91ec-76a3-432a-bf81-4cb5b015b815', embedding=None, metadata={'page_label': '72', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='72 Goldman Sachs january 2025\\nof restrictive monetary policy that will weigh \\non growth. For Czechia and Hungary, trade \\nuncertainty in sectors tied to Germany’s auto and \\nmanufacturing industries is compounding existing \\nstructural vulnerabilities. \\n Meanwhile, Russia faces mounting economic \\npressures. Although its economy has repeatedly \\ndefied expectations since 2022, its performance was \\nunderpinned by a surge in war-related production \\nand expansionary fiscal policy. The government \\nplans to increase military spending again this \\nyear (see Exhibit 106); however, cuts to social \\nexpenditures will offset the potential economic \\nboost. Moreover, domestic demand is increasingly \\nstruggling under the weight of highly restrictive \\npolicy rates, which now stand at 21% in response to \\nspiraling inflation. Given this backdrop, we forecast \\nRussia’s GDP growth to weaken to 1.4% this year \\namid several downside risks. \\nLatin America\\nLatin America’s 1.8% growth rate last year masks \\nstark differences across the region. For instance, \\nChile, Colombia and Peru experienced a rebound \\nin investment fueled by easier monetary policy and \\nrobust external demand. In contrast, Mexico faced \\nwaning infrastructure investment, and Argentina \\nendured its second consecutive year of recession \\namid stringent fiscal consolidation. \\n In the year ahead, economic developments are \\nexpected to remain divergent. Argentina’s recovery \\nfrom recession should boost Latin America’s \\nheadline growth, but the outlook for Mexico and \\nBrazil remains clouded by uncertainties. In the case \\nof Mexico, tensions surrounding US-Mexico trade \\nrelations and the impact of recent government \\nreforms are likely to dampen business investment. \\nReduced fiscal support could also offset the \\nbenefits of easier monetary policy.\\n In Brazil, the government’s loose fiscal stance \\nat a time of already above-trend growth has \\ncontributed to overheating. In December, this trend \\ncaused Brazil’s central bank to raise interest rates \\nto just above 12%. We expect further hikes in the \\ncoming year, given a strong labor market, high \\nwage growth, a weaker currency and runaway \\ninflation expectations (see Exhibit 107). In turn, \\nour forecast calls for slower growth this year. \\nExhibit 107: Brazil Consensus Inflation \\nExpectations\\nInflation expectations are unanchored in Brazil. \\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n%\\nEnd-2025\\nInﬂation Target (Midpoint)\\nEnd-2026\\nEnd-2027\\n2022 2023 2024\\nData through December 27, 2024. \\nSource: Investment Strategy Group, Haver Analytics, Banco Central do Brasil. \\n \\n \\n \\n \\n \\nExhibit 106: Russia Central Government Defense \\nand Security Spending\\nThe Russian government plans to increase military spending \\nagain this year.\\nForecast\\n5.3 5.1 5.5 5.8 5.9\\n6.3\\n6.8\\n5.3\\n4.8 4.8\\n5.2\\n4.5\\n5.1\\n5.8\\n7.2\\n7.9\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022*\\n2023*\\n2024*\\n2025*\\n% of GDP\\nNational Defense\\nNational Security and Law Enforcement\\nTotal\\nData through 2024. Forecast through 2025. \\nSource: Investment Strategy Group, Haver Analytics, EmergingMarketWatch, Ministry of Finance \\nof the Russian Federation. \\n* The Russian government no longer publishes data on defense and national security spending. \\nFigures for 2022–25 are taken from the government’s budget proposals. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f81ed2e1-e011-4095-a69f-fa8517ebdc9e', embedding=None, metadata={'page_label': '73', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='73Outlook Investment Strategy Group', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='79fde7c4-372c-4417-a592-0df4ad3af2a9', embedding=None, metadata={'page_label': '74', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='74 Goldman Sachs january 2025\\nSECTION III\\nfew would have expected the depths of the pandemic to \\nlaunch one of the S&P 500’s most rapid ascents in history. Yet \\nless than five years later, the index has climbed a staggering \\n183%. The resulting pace—surpassed in less than 2% of \\ncomparable periods—equates to a 24% annualized total return. \\nConsidering US equities weathered a bear market in 2022, these \\ngains are all the more impressive.  \\n This exceptional performance is not limited to equities or \\nUS assets. Measured across the same time span, US corporate high \\nyield has appreciated at a more than 9% annualized pace. The \\nannualized total return generated by the MSCI All Country World \\nIndex (excluding the United States) has been nearly 16%. While \\nthis lagged US equities’ advance, it still represents an outstanding \\nabsolute return, ranking in the top quintile for this index.  \\n After such an epic run, investors are naturally wondering \\nwhether the season is nearing its end. To be sure, there is no \\nshortage of hazards (see Section I). These concerns also come \\nat a time when most risk assets are expensive by historical \\nstandards. US stocks, for instance, have been cheaper than \\ncurrent levels at least 90% of the time. Even within bonds, high \\nyield and investment grade spreads—the compensation for \\n2025 Financial  \\nMarkets Outlook: \\nExtending the Season', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8054b17a-88c8-4d06-b61b-03890ccefe8f', embedding=None, metadata={'page_label': '75', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='75Outlook Investment Strategy Group\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9bfb7f62-babe-40f7-a2a1-9e0449ec6230', embedding=None, metadata={'page_label': '76', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='76 Goldman Sachs january 2025\\ndefaults—sit near all-time lows. As a result, investors \\nhave less of a buffer to absorb adverse developments, \\nand prospective returns are likely to be more modest \\nwhile accompanied by higher volatility.\\n Still, we should not mistake challenging \\nterrain for the end of the run. With global and US \\ngrowth expected to edge above trend this year, \\nthe foundation for global profit growth remains \\nintact. Similarly, anticipated US tax reform should \\npreserve lower tax rates, easing concerns about \\nhigher tax burdens pressuring corporate profits. \\nMeanwhile, the current level of global central \\nbank policy rates leaves ample room to deliver \\nadditional support. \\n Most importantly, an ongoing global expansion \\nsupports our view that the probability of a \\nUS recession remains low at just 20%. This is \\nparticularly relevant for equity investors, as \\nrecessions account for nearly three-quarters of past \\nbear markets. Absent recessions, the S&P 500 has \\ndelivered positive annual total returns 87% of the \\ntime since WWII. \\n With supportive monetary policy and ongoing \\ngrowth acting as a lift, we believe markets can \\nextend their run this year, albeit with a few icy \\npatches along the way (see Exhibit 108). \\nUS Equities: High Altitude\\nThe S&P 500 has spent the last two years scaling \\nnew heights. Price returns in both 2023 and 2024 \\nexceeded 20%, placing each year in the top quintile \\nof post-World War II performance. Even more \\nstriking, the combined 53% price gain in these \\nyears has occurred less than 7% of the time. It \\nalso came with below-average volatility. This rare \\ncombination of strong returns and low volatility \\nplaces the index’s risk-adjusted performance over \\nthis period in the top 10% of all observations since \\n1945 (see Exhibit 109). \\n The market’s spirited climb has carried \\nvaluations into thinner air. About 60% of last \\nyear’s price return came from P/E expansion rather \\nthan from earnings growth. As a result, equity \\nvaluations moved further into the 10th decile, \\nleaving the S&P 500 more expensive than at least \\n90% of the time since World War II. With less of a \\nvaluation buffer to absorb adverse developments, \\nthe market is more vulnerable to downside risks. \\nPast periods with similarly elevated valuations saw \\nmuted equity returns and lower odds of positive \\noutcomes over the subsequent five years. \\nExhibit 108: ISG Global Equity Forecasts—Year-End 2025\\n2024 YE\\nEnd-2025 Central \\nCase Target Range\\nImplied Upside from \\nEnd-2024 Levels\\nCurrent Dividend \\nYield Implied Total Return\\nS&P 500 (US) 5,882 6,200–6,300 5–7% 1.3% 7–8%\\nMSCI Europe ex-UK 1,765 1,790–1,880 1–7% 3.3% 5–10%\\nMSCI UK 2,332 2,340–2,460 0–5% 4.1% 4–10%\\nMSCI Japan 1,713 1,780–1,870 4–9% 2.5% 6–12%\\nMSCI EM (Emerging Markets) 1,075 1,100–1,150 2–7% 2.9% 5–10%\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Datastream, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision and may change as economic and market conditions change. There can be no \\nassurance the forecasts will be achieved. Indices are gross of fees and returns can be significantly varied. Please see additional disclosures at the end of \\nthis Outlook.\\nExhibit 109: S&P 500 Rolling 24-Month  \\nRisk-Adjusted Price Return\\nThe US equity volatility-adjusted performance of the past \\ntwo years was in the top 10% of results since 1945.\\n1.9\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n1947 1957 1967 1977 1987 1997 2007 2017\\nAnnualized Return / Volatility (x)\\nData through December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d91858b2-bf12-4aec-9ed0-91b34d3a65a1', embedding=None, metadata={'page_label': '77', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='77Outlook Investment Strategy Group\\n Still, we do not think this narrower margin of \\nsafety undermines the case for staying invested. \\nValuations are a notoriously unreliable market \\ntiming tool; the starting P/E ratio explains only \\n6% of the variation in the next year’s return (see \\nExhibit 110). Moreover, past periods with elevated \\nvaluations have often been followed by substantial \\ngains—highlighting the penalty for exiting equities \\nprematurely based solely on valuation concerns \\n(see Exhibit 111). \\n History also reminds us that what constitutes an \\nunsustainably high level of valuations is a moving \\ntarget. Over the last few decades, valuations have \\nshifted structurally higher (see Exhibit 112). Even \\nexcluding the dot-com bubble, the Shiller CAPE has \\nexceeded its pre-1992 average about 99% of the \\ntime since 2002. This persistent elevation challenges \\nthe widely held belief that valuations revert to their \\nlong-run average over time.  \\n Several factors may help explain this valuation \\nshift. For one, economic growth has become less \\nvolatile over time as the economy has evolved from \\ncyclical manufacturing sectors to more service-\\nbased and technology-driven industries (see Exhibit \\n113). Additionally, investors have developed greater \\nfaith in policymakers’ ability to manage shocks \\nand sustain growth, reinforced by the effectiveness \\nof automatic stabilizers, direct fiscal transfers and \\nunconventional monetary policy during the global \\nfinancial crisis of 2008 and the pandemic in 2020. \\n As a result of the foregoing, the US economy \\nnow spends less time in recession than it did in \\nearlier decades, with downturns occurring less \\nfrequently and resolving more quickly. The US has \\nspent just 8% of the time in recession since 1992, \\ncompared to nearly 20% in the preceding decades \\n(see Exhibit 114).\\n The valuation shift is also a function of \\nidiosyncratic factors. S&P 500 companies today \\nExhibit 111: S&P 500 Total Returns After Crossing \\nInto the 9th and 10th Deciles of Valuation\\nEquities continued to rally even after valuations became \\nexpensive in the past two bull markets.\\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n3,500\\n4,000\\n5,000\\n4,500\\n6,500\\n6,000\\n5,500\\n1990 1995 2000 2005 2010 2015 2020\\n9th Decile, Mar-92\\n10th Decile, Jul-95 9th Decile,\\nNov-13 \\n10th Decile,\\nDec-16\\n135%211%\\n194%\\n342%\\nTotal Return\\nS&P 500 Price Index\\nData through December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg. \\nExhibit 110: S&P 500 Shiller CAPE vs. Subsequent \\nCalendar-Year Total Return\\nStarting valuation multiples tell us little about potential \\nreturns over the next year.\\n-50\\n-40\\n-30\\n-20\\n-10\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n0 10 20 30 40 50\\nShiller CAPE (x)\\nS&P 500 Returns 1 Year Forward (%)\\nR2 = 6%\\nCurrent CAPE Level \\nData through December 31, 2024. \\nNote: Based on data since 1945. \\nSource: Investment Strategy Group, Bloomberg, Robert Shiller.\\nExhibit 112: Shiller CAPE Since WWII\\nValuations have shifted structurally higher over the last \\nfew decades.\\nShiller CAPE (x)\\n37\\n14\\n26\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n1945 1950 1955 1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010 2015 2020\\nShiller CAPE\\nPre-1992 Median\\nPost-2002 Median\\nData through December 2024.  \\nSource: Investment Strategy Group, Robert Shiller, Bloomberg, S&P Global.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='76c34b73-a991-4730-9ae4-2aa366ebc25d', embedding=None, metadata={'page_label': '78', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='78 Goldman Sachs january 2025\\ngenerate more profits and distribute a larger share \\nof those profits to shareholders via buybacks \\nand dividends than in the past (see Exhibit 115). \\nMoreover, the dominant companies within the \\nindex—which represent a large share of its \\nweight—operate with profit margins three times \\nas high as those of past leaders and the broader \\nS&P 500 universe (see Exhibit 116). These \\nelevated margins reflect substantial competitive \\nbarriers—such as network effects and winner-\\ntake-all business models—that make it harder for \\ncompetitors to erode their profitability. As a result, \\ninvestors perceive these earnings as more durable \\nand sustainable. \\nExhibit 113: Rolling 20-Year Volatility of US Real \\nGDP Growth\\nEconomic growth has become less volatile over time.\\n2.3\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n1967 1972 1977 1982 1987 1992 1997 2002 2007 2012 2017 2022\\n%\\nData through Q3 2024. \\nNote: Excluding the pandemic year 2020. Volatility is based on the quarter-over-quarter \\nannualized real GDP growth.  \\nSource: Investment Strategy Group, Bloomberg. \\nExhibit 115: S&P 500 Valuation and Fundamentals\\nS&P 500 firms have higher profitability and payout ratios \\ntoday, supporting their higher valuations.\\n14\\n13\\n5\\n26\\n16\\n9\\n32\\n74\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nShiller CAPE ROE (%) Net Margin (%) Payout Ratio (Right)\\n%\\nValuation Fundamentals\\nPost-2002 MedianPre-1992 Median\\nData as of December 2024.  \\nNote: Based on data since 1945 for Shiller CAPE and ROE, since 1970 for net margin, and since \\n1977 for payout ratio. The payout ratio includes both dividends and net buybacks. \\nSource: Investment Strategy Group, Robert Shiller, Bloomberg, S&P Global, Goldman Sachs \\nGlobal Investment Research, Empirical Research Partners Analysis.\\nExhibit 114: Percentage of Months the US \\nEconomy Was in Recession\\nThe US economy now spends less time in recession than in \\nprevious decades.\\n19\\n8\\n0\\n5\\n10\\n15\\n20\\n25\\nBefore 1992 1992 and After\\n%\\nData as of December 2024.  \\nNote: Based on data since 1945. \\nSource: Investment Strategy Group, Bloomberg, NBER.\\nExhibit 116: Characteristics of Top 10 S&P 500 \\nFirms During Past Periods of High Concentration\\nToday’s largest firms have profit margins significantly higher \\nthan those of past leaders and the broader S&P 500.\\n27\\n10\\n27\\n11\\n39\\n30\\n6\\n8\\n12\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\nShare of S&P 500 Market Cap Median Proﬁt Margin\\n%\\nS&P 500 Proﬁt Margin\\nToday\\n2000\\n1973\\nData as of December 31, 2024.  \\nSource: Investment Strategy Group, Goldman Sachs Global Investment Research, S&P Global. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='92e00507-fb63-4906-9374-80083f512f70', embedding=None, metadata={'page_label': '79', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='79Outlook Investment Strategy Group\\n Taken together, these structural and company-\\nspecific factors support investors’ willingness to \\npay a higher P/E ratio, or more years of future \\nearnings today. Although this does not imply that \\nvaluations are immune to periodic downdrafts, it \\ndoes suggest that the range of likely outcomes has \\nshifted higher. \\n Despite the focus on valuations, the most \\ninfluential drivers of the stock market historically \\nhave been the economy and corporate earnings. \\nWhen the economy is expanding, investors have \\nenjoyed an 87% probability of a positive return \\nand a much greater likelihood of large gains \\nthan large losses (see Exhibit 117). In contrast, \\nrecessions have coincided with nearly three-fourths \\nof bear markets—or equity declines of 20% \\nor more. With our forecast placing just a 20% \\nprobability on a recession this year (see Section II, \\nUnited States), we think the economic backdrop \\nremains favorable for stocks.\\n The close relationship between the economy and \\nmarket performance is largely driven by earnings. \\nBecause corporations are paid in nominal dollars, \\ntheir sales and earnings tend to track nominal GDP \\ngrowth over time. Rising sales typically boost profit \\nmargins as well, since companies often have some \\nfixed costs that do not scale with higher revenues. \\nAs a result, margins historically expanded about \\ntwo-thirds of the time during past periods with \\npositive sales growth (see Exhibit 118). \\n Given these linkages, the S&P 500 has closely \\nfollowed the path of earnings over time (see \\nExhibit 119). Even with the significant expansion \\nin the P/E ratio over the last decade, earnings and \\ndividends still contributed three-fourths of the S&P \\n500’s total return.\\n Earnings figure prominently in our forecast \\nas well. We expect S&P 500 EPS to grow \\napproximately 10% to $265 this year, reflecting a \\nmid-single-digit increase in sales alongside modest \\nExhibit 117: Odds of Various S&P 500 1-Year Total \\nReturns During US Economic Expansions\\nInvestors enjoy high odds of a positive return and a greater \\nlikelihood of large gains when the economy is expanding.\\n4\\n87\\n64\\n32\\n0\\n25\\n50\\n75\\n100\\nDecline of\\nat Least 10%\\nPositive Return Return of 10%\\nor Greater\\nReturn of 20%\\nor Greater\\nProbability (%)\\nData through December 31, 2024. \\nNote: Based on data since 1945.  \\nSource: Investment Strategy Group, Bloomberg. \\nExhibit 118: Percentage of Time Profit Margins \\nExpanded When S&P 500 Sales Grew\\nPeriods with positive sales growth saw margins remain flat \\nor expand about two-thirds of the time.\\n64 67 67\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n80\\n70\\n100\\n90\\nFull Sample During 1970s Last 20 Years\\n%\\nData as of Q3 2024.  \\nNote: Based on data since 1970. \\nSource: Investment Strategy Group, Goldman Sachs Global Investment Research, S&P Global, \\nFactSet, Compustat.\\nExhibit 119: S&P 500 Price Index vs. Earnings\\nS&P 500 prices have followed the path of earnings over time.\\n50\\n500\\n5,000\\n50,000\\n1945 1950 1955 1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010 2015 2020\\nIndexed Value in Log Scale (1945 = 100)\\nS&P 500 Trailing 12-Month Operating Earnings\\nS&P 500 Price Index\\nData through Q3 2024. \\nSource: Investment Strategy Group, Bloomberg, S&P Global.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='313835b0-10e9-48ad-a26c-1e739ff60f0b', embedding=None, metadata={'page_label': '80', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='80 Goldman Sachs january 2025\\nfurther profit margin gains. When this earnings \\ngrowth is combined with a 1.3% dividend yield \\nand some compression in the P/E ratio, our base \\ncase this year implies high-single-digit total returns \\nand an S&P 500 target range of 6,200–6,300 (see \\nExhibit 120). \\n As seen in Exhibit 121, we attach a higher \\nprobability to upside than to downside surprises \\nthis year. While our bad case accounts for markets \\npricing in a recession, our good case features \\nstronger earnings growth and a further boost to \\nP/E ratios due to optimism around AI, tax cuts and \\nderegulation. Supporting this favorable skew, past \\nperiods that saw earnings growth similar to our \\nforecast—even those starting with high valuations—\\nhad positive returns more than 80% of the time. \\n While some worry about the impact of \\nborrowing costs on earnings, we are more \\nsanguine. Only 9% of S&P 500 debt matures \\nannually, and 92% of it carries a fixed rate. This \\nmeans S&P 500 borrowing costs are relatively \\nimmune to higher interest rates over the next \\nseveral years (see Exhibit 122). Interest expense \\nmust also be netted against the interest income \\nExhibit 120: Decomposition of ISG Central Case \\nS&P 500 Return at Year-End 2025\\nWe expect earnings growth to be the key driver of S&P 500 \\nreturns in 2025.\\n0\\n2\\n4\\n6\\n8\\n12\\n10\\nEarnings Growth Impact of\\nMultiple\\n Contraction*\\nPrice Return Dividend Yield Total Return\\nReturn (%)\\n– = + =\\n10\\n6\\n8\\n4\\n1\\nData as of December 31, 2024. \\nNote: The returns and decomposition are based on the midpoint of ISG’s central case forecast range. \\nSource: Investment Strategy Group, Bloomberg. \\n* Includes the compounding effect between earnings and valuation multiples. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance the forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook . \\n \\nExhibit 122: Weighted Average Interest Rate for \\nS&P 500 Non-Financial Firms\\nS&P 500 borrowing costs are relatively immune to higher \\ninterest rates over the next several years.\\n3.7\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n1962\\n1965\\n1968\\n1971\\n1974\\n1977\\n1980\\n1983\\n1986\\n1989\\n1992\\n1995\\n1998\\n2001\\n2004\\n2007\\n2010\\n2013\\n2016\\n2019\\n2022\\n2025\\nCorporate Interest Rates (%)\\n1982: 12.1\\nProjected*\\n2026E: 3.9\\n2025E: 3.8\\nData through Q3 2024. Forecasts through 2026. \\nSource: Investment Strategy Group, Bloomberg, Barclays, Michael Smolyansky, “End of an \\nEra: The Coming Long-Run Slowdown in Corporate Profit Growth and Stock Returns,” Board of \\nGovernors of the Federal Reserve System, 2023. \\n* The projections assume that floating-rate debt reprices lower by 75bps than what was paid \\nover the prior 12 months during 2025 and assume the Federal Reserve holds the federal funds \\nrate constant from year-end 2025 levels during 2026. For maturing fixed-rate debt, we assume \\nit is refinanced at the yield of the Bloomberg US Investment Grade Corporate Index (5.33% as of \\nDecember 31, 2024). \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance the forecasts will be achieved. \\nExhibit 121: ISG Total Return Forecast Scenarios \\nfor S&P 500—Year-End 2025\\nWe attach a higher probability to upside than downside \\nsurprises this year.\\n14\\n8\\n-15\\n-20\\n-15\\n-10\\n-5\\n0\\n5\\n10\\n15\\nGood Case\\n(6,600 Price)\\n(25% Probability)\\nCentral Case\\n(6,200–6,300 Price)\\n(60% Probability)\\nBad Case\\n(4,900 Price)\\n(15% Probability)\\nTotal Return (%)\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance the forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2d43ea90-43d2-41f2-bdfd-b5fc539632fc', embedding=None, metadata={'page_label': '81', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='81Outlook Investment Strategy Group\\nnow being earned on the almost $2 trillion of \\ncash and short-term investments held by non-\\nfinancial firms.\\n Other factors support our expectation of \\nfurther gains. Both the broadening participation \\nof stocks in last year’s rally and the persistence \\nand low volatility of the advance have been \\nreliable indicators of a continued climb in stocks \\n(see Exhibits 123 and 124). Additional gains also \\nfollowed previous all-time highs in the S&P 500 \\n(see Exhibit 125), as well as periods when the \\nFederal Reserve reduced rates and the US economy \\navoided a recession—consistent with our forecasts \\n(see Exhibit 126). Importantly, the number of \\nrate cuts delivered in those periods had no clear \\nbearing on S&P 500 performance (see Exhibit \\n127). Moreover, there is considerable scope for \\nrebalancing from cash into equities given around \\n$8 trillion in money market funds. Lastly, the \\ncurrent advance has ample upside in time and price \\nrelative to past recoveries (see Exhibit 128). \\n Despite our constructive stance, we are acutely \\naware of the many legitimate risks that could \\nundermine our forecast (see Section I, Risks to Our \\n2025 Economic and Financial Market Outlook). \\nInvestors are particularly concerned about the \\nimpact of disruptive trade and immigration \\npolicies, which could stoke inflationary pressures \\nand necessitate more hawkish monetary policy. The \\nrise in bullish sentiment is also a concern, given it \\nis typically a contrarian indicator. \\n However, we view these risks as a reason to \\nexpect bouts of market volatility, rather than an end \\nExhibit 123: S&P 500 Price Returns in the Year \\nFollowing Past Market-Based Technical Signals\\nThe broadening participation of stocks in last year’s rally has \\npreceded well-above-average equity returns.\\n% of Positive Returns (Right)\\nUnconditional Average Return*Average Return Following Signals\\n15.5\\n20.3\\n12.0 12.5\\n8.6 8.8 8.8 9.7\\n96 100\\n94\\n100\\n0\\n20\\n40\\n60\\n80\\n100\\n0\\n5\\n10\\n15\\n20\\n25\\nRapid Breadth Expansion\\nBased on McClellan\\nSummation Index\\nS&P 500 Up in \\n8 of the 9 First \\nMonths of the Year\\nS&P 500 Up in  \\n5 Consecutive \\nMonths\\nNet 20%+ of S&P 500\\nStocks at Yearly High\\non the Same Day\\n% %\\nData as of December 31, 2024.  \\nSource: Investment Strategy Group, Bloomberg, SentimenTrader, Bespoke Investment Group, The \\nLeuthold Group.  \\n* Unconditional 1-year average returns may vary across signals because not all signals have the \\nsame length of historical data. \\nPast performance is not indicative of future results.  \\nExhibit 124: S&P 500 Return in 15 Months After a \\nHigh Risk-Adjusted 2-Year Return\\nThe persistence and low volatility of the advance over the \\nlast two years have preceded continued equity upside.\\nEntered Recession in the 15 Months After Return\\nNo Recession in the 15 Months After Return\\n30\\n7 10\\n33\\n6\\n32\\n6\\n-15\\n-1\\n22\\n6\\n-2\\n11\\n7\\n15\\n10\\n-20\\n-10\\n0\\n10\\n20\\n30\\n50\\n40\\nApr-55\\nOct-59\\nJun-64\\nMay-86\\nNov-89\\nFeb-96\\nFeb-05\\nApr-07\\nFeb-11\\nSep-13\\nDec-17\\nMar-22\\nAverage\\nMedian\\nAverage Non-Recession\\nMedian Non-Recession\\nPrice Return (%)\\nData through December 31, 2024.  \\nNote: A signal with high risk-adjusted rolling 2-year return is identified when the expanding-\\nwindow percentile rank of the rolling 2-year risk-adjusted price return exceeds 90% for the first \\ntime in one year. The latest signal triggered in September 2024, so we look at the subsequent 15 \\nmonths, which is equivalent to the time period from the latest signal to year-end 2025.  \\nSource: Investment Strategy Group, Bloomberg. \\nPast performance is not indicative of future results.\\nExhibit 125: S&P 500 1-Year Subsequent Returns \\nFollowing an All-Time High\\nAdditional gains followed past all-time highs in the S&P 500.\\n11.1\\n12.312.0\\n13.4\\n76 79\\n0\\n20\\n40\\n60\\n80\\n10\\n30\\n50\\n70\\n90\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\nAll Episodes Outside Recessions\\nTotal Return (%) % of Positive Returns\\n% of Time Return is Positive (Right)MedianAverage\\nData as of December 31, 2024. \\nNote: Based on data since 1945. \\nSource: Investment Strategy Group, Bloomberg. \\nPast performance is not indicative of future results.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='21bb22e6-dc03-4e58-8106-73c03b6c142a', embedding=None, metadata={'page_label': '82', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='82 Goldman Sachs january 2025\\nto the economic expansion (see Section II, United \\nStates). And while US equities at high altitude imply \\ngreater risk of a sudden descent, we do not see that \\nrisk as pressing enough to act upon today. \\nNon-US Developed Market Equities: \\nMind the Gap\\nLast year’s equity performance reminded us that \\nit’s hard to please everyone. While the MSCI \\nEAFE Index—which tracks equities across Europe, \\nAustralasia and the Far East—delivered a solid \\n12% return, it still paled in comparison to the US \\nmarket’s 25% surge. This widening performance \\ngap has become an all-too-familiar source of \\ndisappointment for many investors, fueled by \\nslower earnings growth and muted valuation \\nexpansion outside the US (see Exhibit 129). \\n For the year ahead, non-US developed markets \\nmay once again struggle to exceed lofty investor \\nexpectations. After two consecutive years of \\ndouble-digit gains, many global investors have \\nraised the bar for what constitutes an attractive \\nequity return. At the same time, persistent \\nglobal trade policy uncertainties and Europe’s \\nstructural challenges are likely to cap valuation \\nexpansion. As a result, we see limited potential \\nfor EAFE to meaningfully reverse its multiyear \\nunderperformance relative to the US in 2025 (see \\nExhibit 130). \\n Even amid these concerns, we expect non-US \\ndeveloped market equities to deliver a solid high-\\nsingle-digit total return this year. Our forecast \\nreflects 3% earnings growth, a 3% dividend yield \\nExhibit 128: S&P 500 Total Return During \\nBull Markets\\nThe current advance has ample upside relative to  \\npast recoveries.\\n70\\n250\\n120\\n147\\n103\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\nCurrent Average Median Average Median\\nSince Oct 2022 Bear Market Trough to Next Bear\\nMarket Peak\\nNear-Bear Market Trough to Next\\nNear-Bear Market Peak\\n%\\nData as of December 31, 2024. \\nNote: Based on data since 1945. A bear market is defined as a peak-to-trough price decline over \\n20%. A near-bear market is defined as a peak-to-trough price decline over 19%.  \\nSource: Investment Strategy Group, Bloomberg.\\nExhibit 126: S&P 500 Performance Around the \\nFirst Federal Reserve Rate Cut\\nEquities generated robust returns in the past when the Fed \\ncut rates and the US economy avoided a recession.\\nEntered Recession in the Year After Fed First Cut (Count=7)\\nNo Recession in the Year After Fed First Cut (Count=5)\\n0.89\\n1.19\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\n1.05\\n1.10\\n1.15\\n1.20\\n1.25\\n-12 -9 -6 -3 0 3 6 9 12\\nMonths Relative to First Fed Cut\\nAverage Indexed Total Return\\nData through 2024.  \\nNote: Based on data since 1945. \\nSource: Investment Strategy Group, Bloomberg. \\nPast performance is not indicative of future results.\\nExhibit 127: S&P 500 Returns vs. Fed Funds Rate \\nChange in the Year Following the First Fed Cut\\nThe number of rate cuts in the year following the initial cut \\nhad no clear bearing on S&P 500 performance.\\nReturn So Far Since the First Fed Cut in Sep 2024\\n18 18\\n22\\n24\\n15\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nOct-84\\n-2.00\\nJun-89\\n-1.38\\nDec-66\\n-0.89\\nJul-95\\n-0.50\\nSep-71\\n0.37\\nFirst Fed Cut Date and Fed Funds Rate Change in the Next Year (%)\\nTotal Return (%)\\n5 \\nData as of December 31, 2024.  \\nNote: Based on data since 1945. Only showing the five instances when there was no recession in \\nthe year after the first federal funds rate cut. The x-axis labels show the net change in the federal \\nfunds rate and includes hikes if any. The Federal Reserve ended the cutting cycle and started \\nhiking within the year following the first cut in Dec-66 and Sep-71 (hence the positive 0.37 Fed \\nfunds rate change following Sep-71).  \\nSource: Investment Strategy Group, Bloomberg.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='55213685-08f6-415f-802b-d98bf28edb46', embedding=None, metadata={'page_label': '83', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='83Outlook Investment Strategy Group\\nand a relatively muted 1% expansion in the P/E \\nratio (see Exhibit 131). While this may feel modest \\ncompared to recent gains, it would still be in line \\nwith EAFE’s long-term historical average and \\nattractive relative to cash and bonds. \\nEurozone Equities: A Low Bar for Upside \\nWhen expectations are low, so too is the hurdle \\nfor upside surprises. That is the setup this year for \\nearnings in Europe excluding the UK. Our forecast \\ncalls for a modest 2% earnings growth in 2025, \\nfollowing a similarly disappointing expansion last \\nyear. This subdued outlook reflects the region’s tepid \\nGDP growth, which is weighing on a profitability \\nboost arising from moderating unit labor costs. \\n Still, with our projection well below the region’s \\n7% historical average earnings growth, even \\nmodest improvements could exceed expectations. \\nThat is important, because earnings growth, \\nand the region’s 3% dividend yield, are the two \\nlargest drivers of our mid-single-digit total return \\nforecast this year. In contrast, we expect limited \\nP/E expansion, given rising trade and geopolitical \\nuncertainties.  \\nExhibit 129: Drivers of 2024 S&P 500 and MSCI \\nEAFE Returns\\nEAFE’s earnings growth and P/E expansion lagged \\nmeaningfully behind those of the US. \\nEAFEUS\\n10\\n14\\n1\\n25\\n3\\n5\\n3\\n12\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nEarnings Growth Change in P/E Multiple Dividend Yield Total Return\\n%\\nData as of December 31, 2024.  \\nNote: Change in P/E multiple includes compounding effect.  \\nSource: Investment Strategy Group, Datastream. \\n \\n \\n \\nExhibit 130: S&P 500 and MSCI EAFE Total Returns\\nWe do not expect EAFE to significantly close its multiyear \\nperformance gap with the US this year.  \\n26\\n25\\n8\\n17\\n12\\n8\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n2023 2024 2025 (ISG Forecast)\\nS&P 500 MSCI EAFE\\n%\\nData through December 31, 2024. Forecast through 2025. \\nSource: Investment Strategy Group, Datastream. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance the forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook .\\nExhibit 131: 2025 Total Return Projections for Non-\\nUS Developed Equity Markets\\nWe anticipate another positive return for non-US developed \\nequity markets, albeit lower than last year.\\n2 3\\n6\\n3\\n2 0.4\\n0.6\\n1\\n3\\n4\\n2\\n3\\n7 7\\n9\\n8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nEurope Ex-UK UK Japan EAFE\\nEarnings Growth Change in P/E Multiple Dividend Yield\\n%\\nData as of December 31, 2024.  \\nNote: Numbers may not add up due to rounding. \\nSource: Investment Strategy Group, Datastream. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance the forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9e12f59a-f9fe-4787-9edb-70ccc078a104', embedding=None, metadata={'page_label': '84', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='84 Goldman Sachs january 2025\\n While risks remain, so does the scope for \\npositive catalysts. A potential resolution to the \\nRussia-Ukraine war, faster-than-expected global \\ngrowth or greater-than-expected easing from the \\nECB could lead to equity returns that exceed \\nforecasts. The potential for positive surprises is \\nparticularly true today given already depressed \\ninvestor sentiment. \\n Against this backdrop, we are focused on \\nidentifying opportunities where expectations \\nmay be unjustifiably low. For now, we think the \\nsector with the best return prospects is aerospace \\nand defense, which stands to benefit from higher \\nEuropean defense spending and a normalization in \\naircraft production.\\nUK Equities: Room to Grow\\nWhen you’re already in the basement, there’s not \\nmuch further to fall. Such is the case with UK \\nequities, which delivered a 9% return last year \\ndespite a second consecutive year of declining \\nearnings. This gain is even more striking given \\nthe numerous headwinds facing UK stocks last \\nyear—rising fiscal pressures following the Autumn \\nBudget, persistent investor outflows, company \\ndelistings and a shrinking value of IPO issuance \\n(see Exhibit 132). \\n Part of the explanation of last year’s \\nparadoxical strength is that equities are forward-\\nlooking. Prospects for profitability are better this \\nyear, with our forecast calling for 3% earnings \\ngrowth. This result would be in line with long-\\nterm average growth and consistent with our \\nexpectation of near-trend global GDP growth.\\n Valuations also have some room for upside. \\nUK equities continue to trade at a larger discount \\nto the US than their growth differential warrants. \\nAt the same time, absolute valuations sit in the \\nbottom third of their historical range. While this \\nleaves scope for a modest rise in the P/E ratio, \\nthe potential increase may be limited. The index’s \\nsector composition—laden with low-multiple \\nfinancial and energy firms—acts as a natural \\nconstraint on valuations.  \\n Combining these factors, we forecast a mid-\\nsingle-digit total return for UK equities this year. \\nThis outlook reflects 3% earnings growth, 4% \\ndividend yield and a slight expansion in the P/E \\nratio. While a third consecutive year of earnings \\ndeclines remains a key risk, we believe today’s \\ndepressed sentiment creates some buffer for \\ndownside surprises and leaves room for positive \\ncatalysts. \\nJapanese Equities: The Best of the \\nNon-US Bunch\\nJapan’s steady strides continue to inspire \\nconfidence. Its earnings now stand 163% higher \\nthan their 2008 levels, while UK earnings have \\nseen virtually no increase over the same period. \\nJapan’s profit growth has also outpaced other non-\\nUS developed markets, albeit by a smaller margin \\n(see Exhibit 133). We expect Japan’s 6% earnings \\ngrowth this year to exceed that of its counterparts \\nonce again. \\n Despite its superior long-term relative earnings \\ntrend, Japan trades at only a slight valuation \\npremium relative to other developed equity \\nmarkets. This leaves modest room for valuations \\nto expand, particularly if two long-term tailwinds \\nmaterialize. First, investors might increasingly \\nreward higher earnings growth with a higher \\nvaluation multiple if Japan succeeds in sustaining \\npositive inflation. Second, ongoing corporate \\ngovernance reform efforts could also continue to \\nsupport incremental valuation increases. \\n Despite these potential long-term catalysts, \\nnear-term constraints are likely to limit valuation \\nupside. A slower pace of earnings growth this \\nExhibit 132: United Kingdom Total Annual \\nIPO Volume\\nSentiment around UK equities is quite depressed, \\nexemplified by the declining volume of IPOs.\\n4.4\\n0.4 0.5\\n0.1\\n1.7\\n3.2\\n3.9\\n3.4\\n0.6\\n0.1\\n1.4\\n2.1\\n1.7\\n3.8\\n4.6\\n5.1\\n2.1\\n5.5\\n3.7\\n3.3 3.4\\n7.9\\n0.8 0.5 0.6\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n2000\\n2001\\n2002\\n2003\\n2004\\n2005\\n2006\\n2007\\n2008\\n2009\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n£ Billions\\nData through December 31, 2024.  \\nSource: Investment Strategy Group, Bloomberg.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='29678fde-2471-4a2b-ad6e-7ecde6182f67', embedding=None, metadata={'page_label': '85', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='85Outlook Investment Strategy Group\\nyear—from 7% to 6%—may leave investors \\nhesitant to pay higher P/E multiples. Ongoing \\nmonetary policy tightening by the Bank of Japan \\nmight have a similar effect, as demonstrated by \\nthe market volatility surrounding last July’s hike. \\nUncertainty around US trade policy may further \\nweigh on sentiment, especially since cyclical \\nindustry groups make up a larger share of Japan’s \\nequity index (see Exhibit 134). \\n Considering the above, we expect a \\ncombination of 6% earnings growth, little to no \\nP/E multiple expansion and a 2% dividend yield to \\nresult in a high-single-digit total return for Japan \\nthis year. While Japan is not alone in offering \\nthis return, our confidence in that outlook puts \\nJapan at the front of the pack among the non-US \\ndeveloped markets.\\nEmerging Market Equities: Deferred Hopes\\nEmerging markets often hold out the promise \\nof big returns, but last year reminded us how \\nrarely they deliver. Recall that consensus forecasts \\nprojected the MSCI EM Index to return 22% \\nin 2024. Instead, a mix of factors—slower-\\nthan-expected rate cuts, a stronger US dollar, \\ndisappointing Chinese stimulus and President \\nTrump’s victory—left investors underwhelmed. \\nAs a result, EM stocks achieved a gain of only \\n8%, lagging the S&P 500 by 17 percentage \\npoints in the process. This extended a long \\npattern of underperformance, with EM equities \\nundershooting consensus targets in 12 of the last \\n15 years and lagging US stocks 11 times over the \\nsame period (see Exhibit 135).\\n Investors hoping 2025 will finally break \\nthis streak of underperformance are likely to be \\ndisappointed. In our base case, we expect MSCI \\nExhibit 133: Trailing 12-Month EPS Across \\nDeveloped Equity Markets Indexed to 2008\\nJapan’s earnings have grown faster than those of other  \\nnon-US developed equity markets. \\n325\\n139\\n141\\n97\\n263\\n-50\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n2008 2010 2012 2014 2016 2018 2020 2022 2024\\nIndex, Q4 (2008 = 100)\\nUS\\nEAFE\\nEurope ex-UK\\nJapan\\nUK\\nData through December 2024.  \\nSource: Investment Strategy Group, Datastream.\\nExhibit 134: Weight of Cyclical vs. Defensive \\nSectors for Developed Equity Markets\\nJapan and Europe are highly exposed to cyclical sectors, \\nwhile the UK is more defensive.\\nDefensive SectorsCyclical Sectors\\n54\\n64 63\\n51\\n64\\n46\\n36 37\\n49\\n36\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nUS EAFE Europe ex-UK UK Japan\\n%\\nData as of December 31, 2024.  \\nSource: Investment Strategy Group, Datastream.\\nExhibit 135: MSCI EM Expected Returns at the \\nStart of the Year vs. Actual Returns\\nAnalysts have consistently overestimated year-ahead \\nreturns for EM equities.\\nActual ReturnExpected Return at Start of the Year (Consensus)\\n15 17\\n29\\n14\\n18 19\\n24\\n20 18\\n28\\n16 15\\n30\\n27\\n22\\n26\\n19\\n-18\\n19\\n-2 -2\\n-15\\n12\\n38\\n-14\\n19 19\\n-2\\n-20\\n10 8\\n-30\\n-20\\n-10\\n0\\n10\\n20\\n30\\n40\\n50\\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\\nMSCI EM Total Return (%)\\nData through December 31, 2024.  \\nNote: Expected return is based on bottom-up consensus price targets at the start of the year. \\nSource: Investment Strategy Group, FactSet.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a66fdad2-2089-4b8e-858b-4dff3c5f8555', embedding=None, metadata={'page_label': '86', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='86 Goldman Sachs january 2025\\nEM earnings to grow by 10% this year, supported \\nby healthy sales growth and modest margin \\nexpansion. However, elevated global uncertainty \\nis likely to depress P/E ratios. Combined with EM \\nequities’ 3% dividend yield, these factors point to a \\nmid-single-digit total return this year—well below \\ncurrent consensus forecasts of a 26% gain. \\n To be fair, risks to our forecast are two-sided. \\nOn the upside, substantial fiscal stimulus from \\nChina, softer trade measures from the Trump \\nadministration, stronger global demand for \\nsemiconductors and a rebound in commodity \\nprices could all support stronger-than-expected \\nreturns. In contrast, harsh US tariffs, higher-for-\\nlonger interest rates, persistent capital outflows \\nand geopolitical instability could further erode \\nconfidence and weigh on EM performance. \\n Against this backdrop, we remain highly \\nselective in our EM positioning. We hold modest \\ncurrency-hedged overweight positions in Mexican \\nand South African equities. These markets stand \\nout not only for their attractive valuations but also \\nfor solid earnings growth, light investor positioning \\nand continued monetary easing (see Section I, Our \\nTactical Tilts).\\n2025 Global Currency Outlook\\nWinning is becoming a habit for the US dollar. It \\nnot only posted its third gain in the last four years \\nin 2024 but also outperformed every other major \\ncurrency. Several factors supported this strength, \\nincluding higher interest rates on dollar assets, the \\nprospect of fresh US tariffs on foreign goods and \\nstronger domestic growth prospects compared to \\noffshore peers. \\n While the dollar’s outperformance was \\nconsistent, the severity of losses varied widely \\nacross currencies (see Exhibit 136). Among \\ndeveloped market peers, the yen, Norwegian krone \\nand New Zealand dollar endured the worst of the \\nUS dollar’s dominance. The New Zealand dollar \\nsuffered an 11% drop, nearing its weakest level \\nin two years as New Zealand’s economy \\nfaced a deeper-than-expected recession. \\nThe British pound proved more resilient, \\nslipping just 2%. Like the US dollar, \\nthe pound benefited from interest rates \\nhigher than those of developed peers. \\nThere is also renewed optimism in the \\nUK following the election of a Labour \\ngovernment that promises to soften the \\nterms of Brexit.\\nExhibit 136: 2024 Currency Performance (vs. US Dollar)\\nThe US dollar’s strength was broad-based last year.\\n2024 Spot Return (%)\\nG-10 EM Asia EM EMEA EM Latin America\\nUSD \\nAppreciation\\n-2\\n-6 -7 -8 -9 -9 -10 -11 -11\\n3\\n0\\n-3 -3 -3 -4 -5\\n-6\\n-13\\n-3\\n-5\\n-8\\n-13\\n-17\\n-21\\n-1\\n-12 -12\\n-19\\n-21\\n-25\\n-20\\n-15\\n-10\\n-5\\n0\\n5\\nUK\\nEuro\\nSwitzerland\\nCanada\\nSweden\\nAustralia\\nJapan\\nNorway\\nNew Zealand\\nMalaysia\\nThailand\\nChina\\nIndia\\nSingapore\\nPhilippines\\nIndonesia\\nTaiwan\\nKorea\\nSouth Africa\\nPoland\\nCzechia\\nHungary\\nTürkiye\\nRussia\\nPeru\\nChile\\nColombia\\nMexico\\nBrazil\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg.\\nWe remain highly selective in our \\nemerging market positioning. We \\nhold modest currency-hedged \\noverweight positions in Mexican and \\nSouth African equities.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='4c6f3b5a-9341-4754-81b9-1e139e449fbb', embedding=None, metadata={'page_label': '87', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='87Outlook Investment Strategy Group\\n Emerging market currencies fell even further \\nbehind the dollar than the currencies of developed \\ncounterparts. Several Latin American currencies \\nsuffered heavy losses, despite decent economic \\ngrowth and supportive monetary policy. The \\nBrazilian real and Mexican peso were hit especially \\nhard, weighed down by policy uncertainty. \\n Though some investors worry US dollar \\nstrength may falter, we expect another year of \\ngains in 2025. Higher US interest rates relative \\nto other developed markets should continue to \\nattract capital inflows, while US growth is expected \\nto outpace that of offshore peers. Combined \\nwith the potential for rising trade tensions, these \\nfactors support our forecast for low-single-digit US \\ndollar returns. \\n Against this backdrop, we continue to \\nrecommend that clients fully hedge their offshore \\nfixed income. We also recommend that clients hedge \\n50% of their non-local developed market equity \\nholdings to reduce portfolio volatility and provide \\ndiversification. Tactically, we are long the dollar \\nversus both the Swiss franc and the Indian rupee. \\nUS Dollar\\nThe dollar once again demonstrated its dominance \\namong global currencies in 2024. Not only did \\nit outperform every developed market peer, but \\nit also extended an impressive 15-year record. \\nIncluding last year’s 7% gain, the greenback has \\nnow posted 11 winning years over that stretch, \\ndelivering a staggering 50% advance in the process.\\n Several tailwinds suggest another year of gains \\nis likely. US growth, while moderating toward \\ntrend, is still expected to outpace that of the \\nEurozone, Japan and the UK (see Exhibit 137). \\nMeanwhile, the Federal Reserve is expected to \\nend its cutting cycle at a higher terminal rate than \\nother foreign central banks (see Exhibit 138). \\nThe comparatively higher US rates should favor \\nUS capital inflows. Dollar demand could also be \\nbolstered by new US tariffs. \\n Of course, risks to the dollar are not entirely \\none-sided. After such an impressive rally, its \\nvaluation stands well above historical averages, \\nleaving the greenback more exposed to shifts in \\nsentiment (see Exhibit 139). This vulnerability \\nis magnified by today’s elevated long-dollar \\npositioning, with market participants expecting \\nfurther dollar strength. \\n Additionally, the dollar has already advanced \\n4% since the 2024 election—broadly in line with \\nits 6% gain after the 2016 election—suggesting \\ninvestors have already priced in some of today’s \\ntailwinds. As a result, the risk of disappointment \\nhas grown, especially if US economic growth \\nExhibit 138: Policy Rates and Expected Cuts by \\nMajor Central Banks\\nUS policy rates are expected to remain near the upper-end \\namong developed markets. \\nPolicy Rate at Year-End 2024\\nISG Base Case Forecast at Year-End 2025\\n0\\n1\\n2\\n3\\n4\\n5\\nUS UK Eurozone Japan Switzerland\\n%\\nData as of December 31, 2024. \\nNote: Light blue area denotes ISG’s forecasts for the federal funds target range at year-end 2025.  \\nSource: Investment Strategy Group, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 137: Real GDP in Developed \\nMarket Economies\\nThe US remains the fastest-growing major \\ndeveloped economy. \\nUS\\nEurozone\\nUK\\nSwitzerland\\nJapan\\n70\\n75\\n80\\n85\\n90\\n95\\n100\\n105\\n110\\n115\\n120\\n114\\n110\\n106\\n105\\n104\\n2019 2020 2021 2022 2023 2024 2025\\nIndex (4Q 2019 = 100)\\nForecast\\nData through Q3 2024. Forecast through Q4 2025. \\nSource: Investment Strategy Group, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.  ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='77c8bd08-fe73-4f28-9bca-c03617ce3306', embedding=None, metadata={'page_label': '88', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='88 Goldman Sachs january 2025\\nfalters, the Federal Reserve eases more aggressively \\nthan expected or US tariff policy proves more \\nbenign than expected. \\n Still, we believe the balance of risks favors a \\nstronger dollar. Our forecast calls for low-single-digit \\nappreciation in 2025. Accordingly, we enter the year \\ntactically positioned for the dollar to outperform \\nboth the Swiss franc and the Indian rupee. \\nEuro\\nThe euro’s struggles continued in 2024. The \\ncurrency fell 6% against the US dollar, extending \\na multiyear decline that now totals 30% since the \\nonset of the European debt crisis. Part of its recent \\nweakness followed the US presidential election, as \\nexpectations of fresh tariffs on European exports \\nweighed on sentiment. Even before the election, \\nhowever, the euro had been under pressure from \\ntepid growth across the region, prompting the ECB \\nto cut rates earlier than other developed market \\ncentral banks.  \\n With the euro declining in five of the last seven \\nyears, its valuation now sits below its historical \\naverages. Even so, a swift rebound appears unlikely \\ngiven the persistent nature of the currency’s \\nheadwinds. Higher US policy rates, relative to the \\nECB’s more accommodative stance, continue to \\nfavor demand for US dollars over euros. Moreover, \\nlingering uncertainty surrounding global trade \\npolicies is expected to weigh further on the \\ncurrency, adding to the risk premium already \\nreflected in its price (see Exhibit 140).\\n The Eurozone’s capital flow profile underscores \\nits vulnerability. Domestic investors continue to \\nfavor higher yielding offshore alternatives and \\nare selling euro-denominated assets to fund these \\nportfolios. Similarly, foreign buyers have reduced \\ndemand for lower yielding euro assets. These trends \\nhave left Eurozone net fixed income flows struggling \\nto recover despite the end of the ECB negative rate \\npolicy in 2022. Even as the ECB has raised rates \\nsince then, investors remain reluctant to rebuild \\neuro-denominated portfolios (see Exhibit 141). \\n Nevertheless, several factors could help limit \\nthe extent of further euro weakness. Sentiment \\nis already quite dour, reflecting a near universal \\nexpectation that Eurozone growth will lag that \\nof the US and already bearish positioning among \\nExhibit 140: Expected Full Tariff Impact on the \\nEuro vs. Realized Impact Since US Election  \\nThe euro has already weakened in anticipation of higher \\ntariffs, yet our base case suggests room for further downside.  \\n-6\\n-4\\n-2\\n0\\nChange in EUR/USD (%)\\nEstimated Range in \\nISG Tariff Base Case\\nData as of December 31, 2024.  \\nNote: The estimated range is based on different methodologies, including one described in \\nJeanne, Olivier & Son, Jeongwon, 2023. “To what extent are tariffs offset by exchange rates?”. \\nThe potential tariff impact accounts for our base case assumptions for US tariffs on China, Europe \\nand the rest of the world. The estimated tariff impact does not account for potential second-order \\neffects through changes in GDP growth and monetary policy or the impact from rate differentials.  \\nSource: Investment Strategy Group, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can \\nbe no assurance forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook.\\nExhibit 139: US Dollar Real Effective \\nExchange Rate\\nThe dollar’s valuation now stands well above its long-term \\naverage, making the greenback more vulnerable.\\n80\\n90\\n100\\n110\\n120\\n130\\n140\\n150\\n160\\n170\\n1980 19961992 2008 20161984 1988 2000 2004 2012 2020 2024\\nIndex Level\\nCurrent\\n134\\nAverage\\n114\\nData through November 2024. \\nSource: Investment Strategy Group, Haver Analytics. \\n \\n \\n \\n \\n \\n \\n \\n \\n ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='82d94020-6f25-46a1-a90d-b6996736e89b', embedding=None, metadata={'page_label': '89', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='89Outlook Investment Strategy Group\\nmarket participants (see Exhibit 142). The euro \\nalso reflects a fair amount of pessimism related to \\ntariffs. If these expectations are proven wrong, the \\neuro’s yield disadvantage relative to the US dollar \\ncould ease—especially if the Federal Reserve cuts \\nrates more aggressively or US trade policy turns \\nout to be less punitive than feared. \\n While these factors may limit the euro’s losses, \\nthey are unlikely to prevent them. Weighing the \\nconsiderations mentioned, we still expect a low- to \\nmid-single-digit loss for the euro relative to the \\ndollar this year. \\nYen\\nFew would fault yen investors for wanting to \\nforget 2024. The currency tumbled 15% by \\nmidyear, hitting its weakest level against the US \\ndollar since 1986. Although a partial rebound \\ntrimmed its losses, the yen still ended the year \\ndown 10%. This marked its fourth consecutive \\nannual decline and left it among the worst-\\nperforming developed market currencies last year. \\n Having lost half its value since the introduction \\nof Abenomics 13 years ago, the yen certainly has \\nscope to rebound. A potential catalyst could emerge \\nfrom the Bank of Japan’s ongoing policy shift. Last \\nyear, the BOJ raised rates above zero for the first \\ntime since 2016. With further tightening expected \\nin 2025 as other central banks ease, interest rate \\ndifferentials may continue to shift in favor of the yen. \\n Indeed, we see scope for modest yen \\nappreciation as the gap between our 10-year \\nfixed income targets in Japan and the US narrows \\n(see Exhibit 143). A shrinking rate differential \\ncould also reduce the cost of protecting domestic \\nportfolios from currency fluctuations. In turn, \\nmajor Japanese life insurers—holding ¥59 trillion \\nExhibit 141: Eurozone Cumulative Debt \\nPortfolio Flows\\nEurozone net debt portfolio flows have failed to recover \\nfollowing the ECB’s decision to end negative interest rates.\\n-2,500\\n-2,000\\n-1,500\\n-1,901\\n-1,000\\n-500\\n0\\n500\\n1,000\\n1,500\\n1999 2003 2007 2011 2015 2019 2023\\nCumulative Flows (€ Billions)\\nNegative Interest\\nRates\\nData through October 2024. \\nNote: Data shown is foreign net inflows minus domestic net ouflows. \\nSource: Investment Strategy Group, Haver Analytics.\\nExhibit 142: 5-Year Percentile Rank of Euro \\nPositioning vs. US Dollar\\nNet long euro positioning has declined significantly.  \\nPercentile\\n69\\n52\\n3\\n7\\n0\\n20\\n40\\n60\\n80\\nNoncommercials Leveraged Accounts\\nNet long euro positioning\\nDecember 2023 December 2024\\nData as of December 24, 2024.  \\nNote: Percentile is based on positioning data over the past five years.  \\nSource: Investment Strategy Group, CFTC, Bloomberg.\\nExhibit 143: USD/JPY Performance vs. Change in \\nUS-Japan 10-Year Rate Differential\\nThere is a strong positive correlation between the difference \\nin US and Japanese 10Y rates and yen performance. \\n-8\\n-4\\n0\\n4\\n8\\n-0.8 -0.4 0 0.4 0.8\\nChange in 10-Year Rate Differential (US-Japan) (pp)\\nMonthly Performance of USD vs. JPY (%)\\nR2 = 63%\\nData through December 2024.  \\nNote: Data since January 2021. \\nSource: Investment Strategy Group, Bloomberg.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f072cf86-c793-4dd0-a04c-db94369a769c', embedding=None, metadata={'page_label': '90', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='90 Goldman Sachs january 2025\\nof foreign currency assets—may increase their \\nhedge ratios, which remain near multiyear lows \\n(see Exhibit 144). This hedging process involves \\nselling foreign currency to buy the yen and creates \\nsupportive inflows to the currency. \\n Of course, risks to the yen remain. While \\ninstitutional hedging flows are supportive, ongoing \\noutflows from Japanese corporations selling yen to \\nfund higher yielding offshore investments continue \\nto weigh on the currency. At the same time, the \\nthreat of new US tariffs targeting Japanese auto \\nexports adds pressure to Japan’s trade balance, \\nweighing on the yen. \\n Overall, however, we expect the yen to \\nstrengthen in 2025. Our forecast envisions a mid-\\nsingle-digit rise in the currency, with opportunities \\nfor both long and short tactical trades throughout \\nthe year. \\nPound\\nThe pound held its ground better than most \\ncurrencies last year. Instead of suffering the steep \\nlosses seen elsewhere, it ended the year down just \\n2% against the US dollar. Yet that modest decline \\nmasked significant volatility, as the currency had \\nclimbed more than 5% earlier in the year before \\nforfeiting those gains in the fourth quarter. \\n The selloff primarily reflected US tariff fears \\nand weaker domestic growth. Even so, the pound \\nstill managed to finish the year relatively unscathed \\non the back of the UK’s comparatively higher \\ninterest rates and renewed optimism about the \\nLabour government’s push for closer ties with the \\nEuropean Union.\\n Despite its resilience, the pound still faces \\nheadwinds. The UK economy is expected to \\nregister another year of below-trend growth in \\n2025, which may push the Bank of England to \\ncut rates more aggressively than expected. Such \\na move risks eroding the pound’s relative rate \\nadvantage, making sterling-denominated assets \\nless attractive. At the same time, investors enter the \\nyear overweight the pound, leaving the currency \\nvulnerable to any adverse developments. \\n Yet the risks to the pound cut both ways. \\nThe UK’s narrow basic balance of -4% of GDP \\nleaves room for long-term investment flows that \\ncould potentially reverse a current source of \\ncapital outflows (see Exhibit 145). Moreover, any \\ncombination of improving global growth, stronger-\\nthan-expected domestic activity or easing of \\nBrexit-related uncertainties could help brighten the \\npound’s outlook. \\n Given these competing dynamics, we expect the \\npound to remain range-bound versus the US dollar \\nthroughout 2025. \\nEmerging Market Currencies \\nOptimism about emerging market currencies quickly \\ngave way to disappointment last year. Despite the \\nExhibit 144: Major Japanese Life Insurance FX \\nHedge Ratios \\nJapanese life insurers’ FX hedge ratios remain historically low.\\nUSD\\nEUR\\n33\\n45\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n2005 2008 2011 2014 2017 2020 2023\\nHedge Ratio (%)\\nHistorical \\nAverage:  63\\nHistorical \\nAverage:  53\\nData through September 2024. \\nSource: Investment Strategy Group, Nomura. \\n \\n \\nExhibit 145: UK Narrow Basic Balance\\nThe UK’s narrow basic balance has recovered from \\nhistorically weak levels.\\nCapital Inﬂow = \\nTailwind to Pound\\n-15\\n-10\\n-5\\n0\\n5\\n10\\n15\\nCurrent Account\\nNet FDI\\nNarrow Basic Balance\\n4-Quarter Rolling Sum (% of GDP)\\n-4\\n2008 2010 2012 2014 2016 2018 2020 2022 2024\\nData through Q3 2024. \\nNote: The narrow basic balance measures the country’s ability to fund its debts without relying \\non short-term or speculative capital flows. It combines the country’s current account balance \\nwith its net foreign direct investment.  \\nSource: Investment Strategy Group, Haver Analytics.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f8625ea4-c908-4af5-ab0e-77d43403f9d0', embedding=None, metadata={'page_label': '91', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='91Outlook Investment Strategy Group\\npromise of easier US monetary policy and aggressive \\nChinese stimulus, a series of countervailing forces \\nultimately drove an 8% decline in the asset class, \\nwith nearly all components posting losses. \\n Weakness in individual large EM countries \\nwas a key driver. In Mexico, controversial \\nconstitutional reforms undermined investor \\nconfidence, while fiscal instability rattled Brazil. \\nChina’s stimulus efforts also fell short, delivering \\nlittle positive spillover to either commodity prices \\nor regional growth. \\n These challenges were compounded by broader \\nmacroeconomic dynamics. Resilient global growth \\nmainly reflected strength in the US economy, \\nwhich reinforced US dollar outperformance at the \\nexpense of foreign currencies. The outcome of the \\nUS presidential election added another layer of \\nuncertainty, as expectations of higher US tariffs \\ndisproportionately impacted export-dependent \\nemerging markets. \\n EM currencies are likely to remain under \\npressure in the year ahead. Continued US economic \\noutperformance should reinforce dollar strength at \\nthe expense of non-US counterparts. History also \\nreminds us that additional pressure could arise \\nif the US translates threatened tariffs into formal \\ntrade policy. When tariffs were formally announced \\nin early 2018 during President-elect Trump’s last \\nadministration, EM currencies depreciated by an \\naverage of around 10% over the subsequent year \\n(see Exhibit 146). \\n Considering the above, we expect EM currencies \\nto weaken by low- to mid-single-digits, close to the \\naverage annual decline observed over the last 15 \\nyears. Consistent with this view, we hold a long US \\ndollar versus Indian rupee tactical position, expecting \\ngradual depreciation into the first half of 2025.  \\n2025 Global Fixed Income Outlook \\nLast year saw a notable about-face for bonds. Ten-\\nyear yields in developed markets plunged over the \\nsummer as recession fears gripped investors, only \\nto reverse course in the final months of 2024. This \\nsharp turnaround reflected both receding recession \\nconcerns—supported by Federal Reserve rate cuts \\nand a stabilizing US labor market—and rising fears \\nabout inflationary US tariffs and fiscal largesse \\nfollowing the Republican sweep in the US election. \\nGiven the backup in yields, shorter-duration assets \\nand credit outperformed government bonds in \\n2024 (see Exhibit 147).\\n While ongoing policy uncertainty is likely \\nto sustain this volatility, we ultimately expect \\nmodestly lower government bond yields by \\nExhibit 146: EM Currency Performance Around \\nMajor Tariff Announcements in 2018–19\\nEM currencies underperformed after China tariff \\nannouncements.\\n98\\n85\\n91\\n92\\n70\\n80\\n90\\n100\\n110\\nJan-18 Apr-18 Jul-18 Oct-18 Jan-19 Apr-19 Jul-19 Oct-19\\nEmerging Market Performance Rebased\\nAverage EM Asia\\nAverage CEEMEA\\nAverage Latam\\nAverage EM\\nData through December 31, 2019. \\nNote: Data rebased to January 2018. Shaded areas denote major tariff announcements. Analysis \\nincludes the Brazilian real, Mexican peso, Colombian peso, Chilean peso, Peruvian sol (all Latam), \\nHungarian forint, Czech koruna, Polish zloty, South African rand, Turkish lira (all CEEMEA), \\nChinese renminbi, Korean won, Taiwanese dollar, Indian rupee, Thai baht, Malaysian ringgit, \\nIndonesian rupiah and Philippine peso (all EM Asia). \\nSource: Investment Strategy Group, Bloomberg.\\nExhibit 147: 2024 Fixed Income Returns by \\nAsset Class \\nReturns were mixed across global fixed income, with credit \\noutperforming duration.\\nUS CPI Inﬂation (% YoY)*\\nBank Loans\\nUS Muni 1–10 Year\\nUS Corporate High Yield\\nEM Local Debt\\nUS TIPS\\nMuni High Yield\\nUS Cash\\nUS 7–10 Year\\nUS Corporate Investment\\n Grade\\nGlobal Aggregate Index\\nUK 7–10 Year (Local)\\nEM US Dollar Debt\\nGermany 7–10 Year (Local)\\nTotal Return (%)\\n9.1 8.2\\n6.5 6.3\\n5.3\\n2.7 2.1 1.8\\n0.9 0.2\\n-0.6\\n-2.4 -3.0\\n-1.7\\n-4\\n-2\\n0\\n2\\n4\\n6\\n8\\n10\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg. \\n* Inflation data as of November 2024. \\n \\n \\n ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='52abffb0-52c4-4eb0-94b8-f0ecf8cfe687', embedding=None, metadata={'page_label': '92', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='92 Goldman Sachs january 2025\\nyear-end. Our forecast is supported both by our \\nexpectation of faster-than-priced cuts in central \\nbank policy rates and by a small reduction in the \\ncombined privately available net government bond \\nsupply of the US, Europe and UK (see Exhibits 148 \\nand 149). We expect the greatest bond strength in \\nthe Eurozone and UK, where our forecasts call for \\nfalling core inflation, another year of below-trend \\ngrowth and above-average recession risks. \\n Although net government bond supply is \\nprojected to decline slightly, it remains elevated \\ndue to wide fiscal deficits and limited prospects for \\nfiscal consolidation. As a result, we expect term \\npremiums—the additional yield demanded for the \\nuncertainty of holding long-maturity bonds instead \\nof a series of shorter-term securities—to stay high \\nin Europe and the UK and rise for a fifth straight \\nyear in the US. Elevated term premiums, combined \\nwith faster-than-expected policy rate cuts, are \\nlikely to steepen bond yield curves and constrain \\nthe extent to which yields at the long end might \\notherwise decline. \\n Given this backdrop, we expect intermediate-\\nduration bonds to outperform cash in our base \\ncase. Lower global policy rates have already eroded \\nexpected returns on cash, while bonds again \\noffer positive rolldown returns67 now that yield \\ncurves have dis-inverted. For these reasons, we \\nrecommend client portfolios be at their strategic \\nduration benchmark, which is four years for a \\nUS taxable moderate portfolio. We also remain \\noverweight high-quality fixed income in Europe \\nand the UK, given our expectation of greater bond \\nstrength in these regions. \\n In the sections that follow, we review the \\nspecifics of each major fixed income market.\\nUS Treasuries \\nUS Treasuries were not immune to last year’s sharp \\nreversal in global bonds. From their September \\nlow, 10-year Treasury yields climbed more than \\n90 basis points into year-end. Such a rapid move \\nhas been seen less than 4% of the time historically \\nand was large enough to turn the 10-year Treasury \\nbond’s gain for 2024 into a 2% loss.  \\n The rise in bond yields also marked a sharp \\ndeparture from past cutting cycles. As shown in \\nExhibit 150, yields typically fell initially as the \\nFederal Reserve eased policy but ultimately rose \\nif lower policy rates helped the economy avoid \\nrecession. Today, yields have already risen above \\nlevels seen in past non-recessionary cycles, while \\nExhibit 148: Rate Cuts Implied by Market Pricing \\nvs. ISG Forecasts \\nMarkets are underpricing the number of policy rate cuts we \\nexpect this year.\\nAlready Delivered\\n-1.0 -1.0 -1.0 -1.0\\n-0.5 -0.5\\n-0.42\\n-0.75\\n-1.15 -1.25\\n-0.60\\n-1.25\\n-1.42\\n-1.75\\n-2.15\\n-2.25\\n-1.10\\n-1.75\\nUS Market\\nPricing\\nISG Fed\\nExpectations\\nEUR Market\\nPricing\\nISG ECB\\nExpectations\\nGBP Market\\nPricing\\nISG BOE\\nExpectations\\n2025 TotalPercentage Points\\n-2.5\\n-2.0\\n-1.5\\n-1.0\\n-0.5\\n0.0\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved. \\n \\n \\n \\nExhibit 149: Privately Available Net Bond Supply \\nAcross US, Eurozone and UK\\nThe privately available net bond supply is set to fall \\nslightly in 2025.\\nEurozone\\nTotal\\nUS\\nUK\\nForecast\\n781\\n1,422 1,302\\n443\\n687 723\\n195\\n240\\n184\\n1,419\\n2,349\\n2,209\\n0\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n2023 2024 2025\\nUS$ Billions\\nData through December 2024. Forecast through 2025.  \\nNote: Based on data from Goldman Sachs Banking and Markets; adjusted for ISG central \\nbank, bond auction and syndication expectations, and converted using year-end 2024 spot \\nexchange rates.  \\nSource: Investment Strategy Group, Bloomberg, Goldman Sachs Banking and Markets,  \\nUS Treasury, European Treasuries, UK DMO.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e164fb46-3652-4063-b54f-5207b38c8126', embedding=None, metadata={'page_label': '93', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='93Outlook Investment Strategy Group\\nthe number of expected Federal Reserve cuts \\nis also consistent with past cycles that avoided \\nrecessions (see Exhibit 151). Given the extent of \\nthe repricing already seen, we believe the skew \\nof outcomes favors modestly lower bond yields \\nfrom here. \\n That view is reinforced by our expectation \\nthat the Federal Reserve will deliver more cuts \\nthan currently priced, aiming to prevent further \\ncooling in the labor market and realign policy \\nrates closer to neutral. As highlighted in last \\nyear’s Outlook, market pricing of the nominal \\nneutral rate—a key component of long-term bond \\nvaluations—is closely tied to Federal Reserve \\npolicy rate expectations. Further cuts are therefore \\nlikely to drive nominal neutral rate pricing lower. \\nExhibit 152 corroborates this point, showing that \\nthe entire decline in neutral rate pricing between \\n1989 and 2021 happened in the three-day window \\naround FOMC meetings.68 \\n The modest decline in yields we expect would \\nbe larger were it not for the offsetting impact of \\nUS term premium, which is being pushed higher by \\nboth demand and supply factors. On the demand \\nside, fading recession fears are dampening bond-\\nhedging demand. This is especially true now as the \\nbuyer base for Treasuries is becoming more levered \\nand more price-sensitive, as we discussed in last \\nyear’s Outlook. \\nExhibit 150: Change in 10-Year US Government \\nBond Yield Around First Central Bank Cut\\nUS yields tended to fall initially as the Fed cut rates but later \\nrose if the economy avoided a recession.  \\n48\\n-86\\n-32\\n87\\n-150\\n-100\\n-50\\n0\\n50\\n100\\n150\\n-126 -84 -42 0 42 84 126 168 210 252 294\\nBasis Points\\nDay of Central Bank Cut = 0\\nNon-Recession\\nCurrent Cycle\\nRecession\\nAverage Since 1989\\nData through December 31, 2024. \\nNote: We define recessionary vs. non-recessionary cutting cycles as whether a recession starts \\nwithin one year of the first cut. Based on data since 1989. The FOMC cut rates in September \\n2024, and hence we look at the subsequent 15-months which is equivalent to the time period \\nfrom the start of the cutting cycle to year-end 2025. \\nSource: Investment Strategy Group, Bloomberg.  \\nPast performance is not indicative of future results. \\n \\nExhibit 151: Average Number of FOMC Cuts in \\nFirst Year of Past Cutting Cycles\\nThe number of Fed cuts implied by market pricing is \\nconsistent with past cycles that avoided recessions. \\nMarket Pricing\\nat Start of\\nCurrent Cycle\\nAll\\nCutting\\nCycles\\nRecessionary\\nCutting\\nCycles\\nNon-\\nRecessionary\\nCutting Cycles\\nMarket\\nPricing\\nToday\\nISG\\nForecast\\nHistorically Realized Cuts Market Pricing ISG ForecastBasis Points\\n-100 -100\\n-37\\n-75\\n-175\\n-235 -250\\n-342\\n-113\\n-137\\n-400\\n-350\\n-300\\n-250\\n-200\\n-150\\n-100\\n-50\\n0\\nData through December 31, 2024. \\nNote: We define recessionary vs. non-recessionary cutting cycles as whether a recession starts \\nwithin one year of the first cut. Based on data since 1989. \\nSource: Investment Strategy Group, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can \\nbe no assurance forecasts will be achieved. Past performance is not \\nindicative of future results.\\nExhibit 152: Market Pricing of US Nominal Neutral \\nRate Around FOMC Meetings\\nAll of the secular decline in neutral rate pricing occurred \\naround FOMC meetings. \\n-500\\n-400\\n-300\\n-200\\n-100\\n0\\n100\\n200\\n300\\n1989 1994 1999 2004 2009 2014 2019\\nBasis Points\\nCumulative Move in Neutral Rate Pricing\\nMoves Around FOMC Meetings Only\\nMoves Outside FOMC Meetings\\nData through December 31, 2021. \\nNote: Based on data between 1989 and 2021. \\nSource: Investment Strategy Group, Bloomberg, Haver Analytics. \\nPast performance is not indicative of future results.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e88bc6b2-cd82-40b1-9cef-51591b85d53e', embedding=None, metadata={'page_label': '94', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='94 Goldman Sachs january 2025\\n Elevated bond supply is also lifting term \\npremiums. Already sizable Treasury borrowing \\ncould expand even further if the TCJA tax cuts \\nare extended (see Exhibit 85 in Section II). Bond \\nauction sizes are likely to increase by no later \\nthan November to accommodate government \\nfinancing needs. At the same time, the extra \\nissuance could be skewed toward longer-duration \\nsecurities given nominated Treasury Secretary \\nScott Bessent’s preference to extend the maturity \\nof Treasury debt. \\n While the end of the Federal Reserve’s \\nquantitative tightening program around March \\nand the likely reinvestment of maturing mortgage-\\nbacked securities into Treasuries may temporarily \\noffset some of this supply, we do not expect it \\nto fully counteract the upward pressure on term \\npremiums. \\n Accounting for all these factors, our models \\nsuggest term premium is likely to rise by another \\n30 basis points in 2025. However, we expect that \\nimpact to be offset by Federal Reserve rate cuts \\nand the associated decline in market pricing of the \\nnominal neutral rate. As a result, we expect the \\nUS 10-year Treasury yield to decline modestly in \\nour base case, with our year-end target range at \\n4.10–4.60%.\\n The midpoint of this range implies mid-single-\\ndigit bond returns, which should exceed returns of \\ncash (see Exhibit 153). Since higher-term premiums \\nweigh more heavily on long-duration bonds, we do \\nnot advise extending duration in US fixed income \\nbeyond our recommended four-year duration \\ntarget. As shown in Exhibit 154, intermediate-\\nduration fixed income has a more appealing risk/\\nreward profile and is still likely to provide a \\nportfolio hedge in the event of a US recession. \\nTreasury Inflation-Protected Securities (TIPS) \\nTIPS were a relative bright spot for fixed income \\ninvestors last year, outperforming Treasuries as \\nbreakeven inflation expectations (BEIs) rose. The \\nincrease was particularly pronounced around \\nthe US election, reflecting investor concerns over \\ninflationary policies under a unified Republican \\ngovernment. \\n While ongoing inflation risks may make an \\noverweight position in TIPS tempting, we see \\nseveral arguments against it. First, short-term \\ninflation expectations are broadly consistent with \\nour CPI forecast (see Exhibit 155), indicating that \\nmarkets embed some risk premium for tariffs. \\nSecond, longer-dated BEIs at 2.34% are broadly \\nconsistent with the FOMC’s 2% inflation target \\nExhibit 154: 2025 US Fixed Income \\nReturn Scenarios\\nThe attractive asymmetry of returns for 5-year bonds \\nsupports our 4-year target duration recommendation.\\nBase Case—50% Probability\\nBull Case—20% Probability\\nBear Case—30% Probability\\n3.2\\n11.0\\n6.3\\n14.6\\n8.0\\n4.0\\n5.8 5.7\\n6.5 6.5\\n4.4\\n2.8\\n4.6\\n0.3\\n3.2\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nCash 5-Year Nominal Bonds 5-Year TIPS 10-Year Nominal Bonds 10-Year TIPS\\nTotal Return (%)\\nData as of December 31, 2024. \\nSource: Investment Strategy Group.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can \\nbe no assurance forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook.\\nExhibit 153: US Treasury and Municipal Bond \\nReturn Projections\\nWe expect Treasuries to outperform cash in 2025. \\n2024—Actual\\n2025—Expected\\n5.3\\n3.8\\n1.2\\n-1.7\\n4.0\\n4.7\\n5.8\\n6.5\\n0.9\\n3.9\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nCash 2-Year Treasury 5-Year Treasury 10-Year Treasury Muni 1–10\\nTotal Return (%)\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can \\nbe no assurance forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='52958f76-bdaf-4038-941e-d9199a0140fd', embedding=None, metadata={'page_label': '95', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='95Outlook Investment Strategy Group\\nand the fair value estimates from our models. \\nThird, we wouldn’t expect long-term BEIs to \\nmove materially higher even in response to \\nmore aggressive tariffs. This is because tariffs \\ngenerally cause a one-time hike in prices rather \\nthan ongoing increases and represent a drag \\non economic growth. Indeed, growth concerns \\ndominated inflationary worries during the previous \\ntrade war, with BEIs typically falling after tariff \\nannouncements (see Exhibit 156). \\n Given these dynamics, we do not expect \\nTIPS to outperform Treasuries again this year. \\nMoreover, we see a less favorable distribution \\nof returns for TIPS relative to Treasuries (see \\nExhibit 154), reflecting their lower liquidity and \\nthe tendency for BEIs to decline sharply during \\neconomic downturns. Both considerations make \\nTIPS a less effective portfolio hedge, reinforcing \\nour recommendation for US clients with taxable \\naccounts to use municipal bonds for their strategic \\nallocation.\\nUS Municipal Bonds\\nMunicipal bonds delivered comparable \\nperformance to 5-year Treasuries in 2024, with \\nboth returning around 1% (see Exhibit 157). The \\nyear was a tale of two halves, however, as the \\nspread widening and broader bond selloff that \\nweighed on returns in the first half gave way to a \\nrecovery later in the year. \\n Looking ahead, municipal fundamentals remain \\na bright spot for the asset class. During fiscal \\n2024, 35 states reported increases in their budget \\nstabilization or “rainy day” funds.69 Aggregate \\nExhibit 156: Performance of Breakeven Inflation \\nExpectations (BEIs) During Previous Trade War\\nBreakeven inflation expectations did not rise around tariff \\nthreats and fell after formal tariff announcements.\\n2y BEIs (Around Tariff Threats)\\n5y BEIs (Around Tariff Announcements)\\n2y BEIs (Around Tariff Announcements)\\n5y BEIs (Around Tariff Threats)\\n-5.7\\n-0.1\\n-18.8\\n-10.5\\n-20\\n-15\\n-10\\n-5\\n0\\n5\\n10\\n-21 -14 -7 0 7 14 21 28 35 42 49\\nBasis Points\\nDay of Tariff Announcement/Threat = 0\\nData through 2023. \\nSource: Investment Strategy Group, Bloomberg.  \\nPast performance is not indicative of future results. \\n \\n \\nExhibit 155: Market-Based Inflation Fixings vs. ISG \\nInflation Forecast\\nMarket pricing for short-term inflation is close to our \\nexpectation and incorporates some tariff risk premium.\\nForecast\\n2.7\\n2.5\\n2.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0\\n6.5\\nJan-23 May-23 Sep-23 Jan-24 May-24 Sep-24 Jan-25 May-25 Sep-25\\n%\\nUS Headline CPI (% YoY)\\n2% Inﬂation Target\\nMarket-Based Inﬂation Fixings\\nISG CPI Forecast (% YoY)\\nData through December 2024. Forecast through November 2025.  \\nNote: Based on ISG calculations.  \\nSource: Investment Strategy Group, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 157: 2024 Performance of Cash and Fixed \\nIncome Indices\\nMost fixed income lagged cash returns last year but high \\nyield municipal bonds outperformed.\\nTotal Return (%)\\nCash 5-Year\\nTreasury\\n10-Year\\nTreasury\\nMuni 1–10 Muni IG Muni HY\\n5.3\\n1.2\\n-1.7\\n0.9 1.1\\n6.3\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg, Barclays.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d82c0aee-203e-46ba-adb2-5eb9d0ef7147', embedding=None, metadata={'page_label': '96', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='96 Goldman Sachs january 2025\\nrainy day balances reached $153 billion last year, \\nclose to all-time highs. This trend is expected to \\ncontinue based on fiscal 2025 budgets, with the \\nmedian rainy day fund balance representing more \\nthan 14% of estimated general fund spending—an \\nall-time high (see Exhibit 158). \\n States’ general fund balances have also \\ngrown substantially in recent years, fueled by \\nrevenue surpluses. While states are spending \\nthese balances, the combined reserves of \\ngeneral and rainy day funds are expected to \\nreach 24% of expenditures in 2025, well above \\nthe 14% realized in 2019. 70 Income trends are \\nequally healthy, with state tax revenues up \\n7.1% year-over-year as of the third quarter last \\nyear (see Exhibit 159). Moreover, 2025 budget \\nassumptions seem conservative, as spending \\nis expected to decline by 0.3% despite 1.9% \\nrevenue growth.\\n Credit agency trends have mirrored these \\npositive fundamental developments. Upgrades \\nsignificantly outpaced downgrades last year, with \\nmunicipal bond issuers receiving 525 upgrades \\nand 180 downgrades. The relative upgrade ratio \\nwas even more pronounced at 4.3x based on the \\npar value of debt (see Exhibit 160). While some \\nsectors—higher education, hospitals and senior \\nliving—saw more downgrades, overall ratings \\ntrends remained positive. \\n Amid these supportive fundamentals, retail \\ninvestors have returned as a key source of demand \\nfor the asset class. Last year’s $32 billion of \\ninflows partially reversed two consecutive years of \\noutflows, including record outflows of $148 billion \\nin 2022 (see Exhibit 161). \\nExhibit 158: Budget Stabilization or Rainy Day \\nFund Balances Among State Governments\\nThe median rainy day fund balance as a share of \\nexpenditures rose to an all-time high last year.\\nAggregate RDF Balance\\nMedian RDF Balance\\n12.7\\n14.4\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n1988 1994 2000 2006 2012 2018 2024\\nRainy Day Fund (RDF) Balance as a Share of General Fund Spending (%)\\nData through 2024. Forecast through 2025. \\nSource: Investment Strategy Group, National Association of State Budget Officers. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.\\nExhibit 160: Upgrade-to-Downgrade Ratios for \\nMunicipal Bond Issuers\\nUpgrades outpaced downgrades by a significant margin \\nduring 2024.\\nDCG\\nPar Value of Upgrades/Downgrades\\nNumber of Upgrades/Downgrades\\n2.9\\n4.3\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n1989 1994 1999 2004 2009 2014 2019 2024\\nUpgrade-to-Downgrade Ratio (x)\\nData through Q3 2024. \\nSource: Investment Strategy Group, Moody’s Investor Service.  \\n \\n \\nExhibit 159: Year-Over-Year Growth in State \\nTax Revenues\\nState tax revenues are growing at healthy levels relative \\nto history.\\nDCF\\n7.1\\n-20\\n-10\\n0\\n10\\n20\\n30\\n40\\nQ1-10 Q1-12 Q1-14 Q1-16 Q1-18 Q1-20 Q1-22 Q1-24\\n% YoY\\nData through Q3 2024. \\nSource: Investment Strategy Group, Census Bureau Quarterly Summary of State and Local \\nTax Revenue.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='00f893b7-f9e0-4383-97a7-a983597692ba', embedding=None, metadata={'page_label': '97', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='97Outlook Investment Strategy Group\\n While this supportive backdrop has rekindled \\ninvestor interest, it has also lifted valuations. That \\nis evident in the ratio of AAA-rated municipal \\nbond yields to Treasuries, which now stands \\nbelow long-term averages across maturities (see \\nExhibit 162). Similarly, the spread of 1- to 10-year \\nmunicipal bond yields to after-tax Treasury yields \\nremains slightly below the long-term median (see \\nExhibit 163). \\n In our view, there is a low risk that municipal \\nbonds could lose their tax exemption in a fiscal \\noverhaul. The cost of the municipal tax shield \\nrepresents less than 4% of the projected $8 \\ntrillion increase in the deficit over the next decade. \\nMoreover, the interest deductibility of outstanding \\nbonds is likely grandfathered in based on their \\ncontractual clauses. \\n Balancing healthy fundamentals against \\nelevated valuations, we expect the Muni 1–10 \\nindex to deliver a nominal return of 4% in the \\nbase case. This outlook reflects a modest decline in \\n5-year Treasury yields offset by a slight widening in \\ncredit spreads. \\nUS High Yield Municipal Bonds\\nHigh yield municipal bonds returned 6.3% last \\nyear, significantly outperforming their investment \\ngrade counterparts. While overall performance was \\nstrong, returns across the asset class were uneven. \\nIssuers in the hospital and transportation sectors \\nled gains, while tobacco, Puerto Rico and the \\nleasing sector lagged behind. \\n Last year’s performance was partly driven by \\nnarrowing spreads, which declined 46 basis points \\nfor the asset class even as investment grade spreads \\nwidened by a similar amount. This left valuations \\nmore stretched, with high yield municipal spreads \\nnow at levels that have been lower only 14% of \\nExhibit 161: Annual Flows Into Municipal Bond \\nMutual Funds and ETFs\\nMunicipal bond funds saw inflows in 2024 following two \\nconsecutive years of outflows. \\n-150\\n-200\\n-100\\n-50\\n0\\n50\\n100\\n150\\nUS$ Billions\\n-148\\n32\\n1984 1988 1992 1996 2000 2004 2008 2012 2016 2020 2024\\nData through December 25, 2024. \\nNote: Data prior to 2006 is composed of mutual funds only. For 2024, monthly data is used \\nthrough October 2024 and weekly fund flows afterward.  \\nSource: Investment Strategy Group, Haver Analytics, Investment Company Institute.\\nExhibit 162: Ratio of AAA Municipal Bond Yields \\nto Treasury Yields\\nThe yield of AAA municipal bonds compared to Treasuries \\nstands below long-run averages.\\nAverage Ratio Since 2000\\nCurrent\\nAverage Ratio Since 1987\\n65 67\\n8281\\n88\\n98\\n79\\n85\\n93\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n5-Year 10-Year 30-Year\\nRatio of Muni AAA Yields to Treasuries (%)\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Municipal Market Monitor, Bloomberg. \\n \\nExhibit 163: After-Tax Spread of 1- to 10-Year \\nMunicipal Bond Yields to Treasuries\\nThe incremental after-tax yield of municipal bonds compared \\nto Treasuries is slightly below its long-term median.\\nMuni 1–10 Yield Spread to After-Tax Treasuries\\nLong-Term Median   \\n78\\n80\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n1993 2008 20231999 20142005 20201996 20112002 2017\\nBasis Points\\nData through December 2024.  \\nSource: Investment Strategy Group, Bloomberg, Barclays.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='db070081-b942-4874-b3bf-1546ff7829ee', embedding=None, metadata={'page_label': '98', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='98 Goldman Sachs january 2025\\nthe time since 1995 (see Exhibit 164). Over the \\nsame period, the bonds’ 5.5% yield has been lower \\nonly a quarter of the time. \\n The combination of tight spreads and low \\nyields reflects the sector’s low default rates. \\nThrough November 2024, only 1.1% of the high \\nyield index defaulted, well below the peaks in \\n2020 and 2023 (see Exhibit 165). Low default risk \\nis also seen in the percentage of these bonds that \\nare trading at distressed spreads. That share also \\nstands at 1.1%, well below the nearly 4% median \\nsince 2005 (see Exhibit 166). \\n Against the backdrop of sound fundamentals \\nbut already tight spreads, we expect US high yield \\nmunicipal bonds to return 5% in nominal terms this \\nyear. Our forecast envisions slightly wider spreads \\nthat erode a portion of these bonds’ 5.5% yield. \\nUS Corporate High Yield Credit\\nCorporate credit outperformed duration again last \\nyear, defying concerns over restrictive borrowing \\nrates and tight spreads. Resilient economic growth, \\nrising profits and low default rates supported \\nrobust returns. For the year, high yield bonds \\nand leveraged loans returned 8.2% and 9.1%, \\nrespectively, outperforming comparable-maturity \\nTreasuries and cash by three to five percentage \\npoints (see Exhibit 167). \\n The strong fundamentals that underpinned \\nlast year’s gains remain intact. Interest coverage \\nfor the median high yield issuer stands at 2.7x,71 \\na level exceeded just 11% of the time since 1999 \\n(see Exhibit 168). Leverage metrics are similarly \\nhealthy. At 3.4x, high yield leverage has been lower \\nonly a quarter of the time historically (see Exhibit \\n169).72 Reflecting these metrics, rating upgrades \\noutpaced downgrades by 1.5x last year based on \\nthe par value of high yield debt outstanding (see \\nExhibit 170). \\nExhibit 164: High Yield Municipal Bond Spread\\nThe incremental after-tax yield of high yield municipal bonds \\nremains below its long-term average. \\nDCH\\nHY Spread to After-Tax Treasuries\\nMedian\\n0\\n200\\n400\\n600\\n800\\n1,000\\nBasis Points\\n1995 1999 2003 2007 2011 2015 2019 2023\\n379\\n280\\nData through December 2024.  \\nSource: Investment Strategy Group, Bloomberg, Barclays. \\nExhibit 166: Distress Rate in the High Yield \\nMunicipal Bond Universe\\nThe share of distressed bonds in the high yield municipal \\nbond universe remains historically low.  \\nDCI\\nUS HY Muni Distress Rate Long-Term Median\\n1.1\\n3.8\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n2005 2011 2017 20232009 2015 20212007 2013 2019\\n%\\nData through December 2, 2024. \\nNote: Distressed bonds are those with spreads above 750bps.  \\nSource: Investment Strategy Group, Barclays.\\nExhibit 165: High Yield Municipal Bond Defaults\\nMunicipal bond defaults remain consistent with \\nrecent years. \\nDCJ\\n2.0\\n1.5\\n2.2\\n1.8\\n1.3\\n2.3\\n1.7\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n2018 2019 2020 2021 2022 2023 2024\\n(11 Months)\\nUS$ Billions\\nData through November 2024. \\nSource: Investment Strategy Group, Barclays.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='702e34bc-9ae6-4223-a0bb-3a0e9509d601', embedding=None, metadata={'page_label': '99', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='99Outlook Investment Strategy Group\\n Within corporate high yield, bonds continue to \\nexhibit stronger credit fundamentals than leveraged \\nloans. Bank loans saw upgrades fall below the pace \\nof downgrades last year, with a ratio of just 0.62 \\nbased on the par value of loans as of November \\n2024. Moreover, only 17% of the leveraged loan \\nuniverse is rated at or above BB—the highest \\nspeculative grade rating—compared to 51% in the \\nhigh yield index.73\\n Given this weaker credit profile, bank loans \\nhave experienced higher default activity than \\nbonds. Over the last year, par-weighted default \\nrates on high yield bonds stood at just 1.1%, \\nsignificantly below their 3.4% long-term average. \\nExhibit 167: Performance of Corporate Fixed \\nIncome Assets in 2024\\nCorporate credit generated healthy absolute and excess \\nreturns last year.\\nDDA\\nExcess Return\\nTotal Return\\n2.1\\n8.2\\n9.1\\n2.5\\n5.0\\n3.8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nInvestment Grade Bonds High Yield Bonds Leveraged Loans\\n%\\nData as of December 31, 2024. \\nNote: Excess returns are calculated relative to matched-duration Treasuries for bonds and cash \\nfor loans.  \\nSource: Investment Strategy Group, Bloomberg, Barclays.\\nExhibit 169: Median Leverage for Investment \\nGrade and High Yield Issuers\\nLeverage for high yield issuers remains at historically \\nlow levels.\\nHigh Yield\\nInvestment Grade\\n3.4\\n2.6\\n0.0\\n1.0\\n2.0\\n3.0\\n4.0\\n5.0\\n1999 2004 2009 2014 2019 2024\\nMedian Leverage (x)\\nData through Q3 2024. \\nNote: Leverage measured by the ratio of net debt to LTM EBITDA.  \\nSource: Investment Strategy Group, Goldman Sachs Global Investment Research. \\nExhibit 170: Trailing 12-Month Ratio of Upgrades \\nto Downgrades Among US High Yield Issuers\\nUpgrades outpaced downgrades among high yield issuers \\nover the past year.\\nBy Number of Issuers\\nBy Par Amount\\n1-to-1 Upgrades to Downgrades\\n1.2\\n1.5\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n2001 2007 2013 20192004 2010 2016 2022\\nUpgrade-to-Downgrade Ratio (x)\\nData through November 2024. \\nSource: Investment Strategy Group, JP Morgan. \\nExhibit 168: Median Interest Coverage for \\nInvestment Grade and High Yield Issuers\\nInterest coverage for investment grade and high yield \\nissuers is stabilizing at historically healthy levels.\\nHigh Yield\\nInvestment Grade\\n2.7\\n5.8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n1999 2004 2009 2014 2019 2024\\nMedian Interest Coverage (x)\\nData through Q3 2024. \\nNote: Interest coverage is measured using the ratio of LTM EBIT to net interest expense.  \\nSource: Investment Strategy Group, Goldman Sachs Global Investment Research.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='faf34166-d74f-4370-9843-8ac9189db7f0', embedding=None, metadata={'page_label': '100', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='100 Goldman Sachs january 2025\\nBy contrast, leveraged loan default rates reached \\nabout 4%. While that figure sounds alarming, the \\ndefault rate drops to just 1.5% when distressed \\nexchanges are excluded.74\\n We expect default rates to remain subdued in \\n2025. Past periods with easing lending conditions \\nand Federal Reserve rate cuts were associated with \\nbenign default environments. Moreover, our model, \\nbased on four separate macro variables,75 projects \\na low 2.5% par-weighted default rate this year (see \\nExhibit 171). \\n Issuance trends are equally benign across \\nthe high yield credit market. While net issuance \\nfor bonds and loans was up 16% and 102%, \\nrespectively, last year, it remained well below five-\\nyear averages. Moreover, 43% of issuance was used \\nfor refinancing, in line with the 44% average during \\n2010–23. At the same time, only 10% was used \\nfor leveraged buyout and M&A purposes, while \\nthe share of low-rated company issuance remained \\nmodest relative to history (Exhibit 172). We expect \\ncredit quality to hold up in 2025, even as issuance \\npicks up around M&A and refinancing activities. \\n Of course, investors are not oblivious to \\nthis generally supportive backdrop. High yield \\nspreads—which compensate investors for the risk \\nof default losses—ended the year at levels that have \\nbeen lower only 5% of the time in the last 30 years \\n(see Exhibit 173). This narrower margin of safety \\nis also visible in our models, which imply the credit \\nrisk premium—or incremental return in excess of \\nrisk-free Treasuries after accounting for default \\nlosses—is significantly below its median and at \\na level that has been lower only 7% of the time \\nhistorically (see Exhibit 174). \\nExhibit 171: Trailing and Projected 12-Month \\nDefault Rates for US High Yield \\nThe default rate for US high yield remained low last year, \\nwhile it trended higher for leveraged loans.\\nHigh Yield\\nBank Loans\\nPar-Weighted Default\\n1.1\\n4.0\\n2.5\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\n1998 2001 2004 2007 2010 2013 2016 2019 2022 2025\\nTrailing 12-Month Default Rate (%)\\nForecast\\nData through November 2024. Forecast through 2025. \\nNote: Includes distressed exchanges.  \\nSource: Investment Strategy Group, JP Morgan, Moody’s Investor Service, Haver Analytics. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved. \\nExhibit 173: Spreads for Investment Grade and \\nHigh Yield Bonds \\nSpreads ended 2024 at very tight levels relative to history. \\nDDB\\nHigh Yield\\nInvestment Grade (Right)\\n287 \\n80\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\n1,600\\n1,800\\n2,000\\n1987 20222002 20171997 20121992 2007\\nBasis Points Basis Points\\nData through December 2024.  \\nSource: Investment Strategy Group, Bloomberg, Barclays. \\n \\n \\n \\nExhibit 172: Share of High Yield New Issuance\\nThe quality of high yield issuance remains healthy relative \\nto history.\\n52\\n28\\n23\\n28\\n44\\n910\\n43\\n3\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\nUsed for LBO and M&A Used for Reﬁnancing Issued by\\nLow-Rated Companies\\n%\\n2010–23 Average\\n2005–07 Average\\n2024 (11 Months)\\nData through November 2024. \\nSource: Investment Strategy Group, JP Morgan.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='15712127-812d-4181-a857-3507d801a86e', embedding=None, metadata={'page_label': '101', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='101Outlook Investment Strategy Group\\n Despite already tight spreads and our \\nexpectation of some widening this year, we believe \\ncurrent yields—7.5% for bonds and 8.8% for \\nloans—remain sufficient to drive mid-single-digit \\nreturns. While bank loans suffer from poorer credit \\nquality than bonds, they also offer an incremental \\nspread of nearly 200 basis points, which provides \\nadditional compensation for their higher default \\nrisk (see Exhibit 175). \\nEuropean Bonds\\nEuropean bonds exhibited a notable divergence \\nlast year. While German bonds outperformed those \\nin the US and UK for a second consecutive year, \\nUK bonds significantly underperformed. Disparate \\neconomic fundamentals drove this performance \\ngap—Germany faced weak growth and gradually \\nfalling inflation, whereas the UK experienced \\nstronger-than-expected growth, sticky \\ninflation and mounting fiscal concerns \\nfollowing the Autumn Budget. The \\nresulting differences in central bank \\npolicy also played a role. While the ECB \\ndelivered 100 basis points of interest rate \\ncuts in 2024, the BOE cut half that much. \\n Despite these differences, we expect \\nboth German and UK bonds to generate \\nattractive returns in 2025. In Europe, \\nanother year of below-trend growth and \\nreceding inflation should allow the ECB \\nto cut rates by at least 125 basis points, supporting \\nlower bond yields. Economic surprise indicators \\nhave already turned negative, historically a signal \\nfor declining rates. Europe also faces higher \\nrecession risk—estimated at 40% over the next \\nyear—given weak underlying growth and the \\nthreat of tariffs. Our colleagues in Goldman Sachs \\nGlobal Investment Research estimate that a 10% \\nuniversal tariff—if implemented—would reduce \\nEurozone growth by approximately one percentage \\npoint, enough to tip the economy into technical \\nrecession. \\n Given these factors, we forecast German 10-\\nyear bond yields will end the year at 1.75–2.25% \\nwith a favorable skew of returns (see Exhibit 176). \\nIn turn, we recommend a small overweight to high-\\nquality European fixed income. At the same time, \\nwe are closely monitoring the upcoming German \\nExhibit 174: Incremental Risk Premium of High \\nYield Bonds in Excess of Estimated Default Losses\\nThe incremental risk premium offered by high yield bonds \\nafter accounting for expected defaults is near all-time lows.\\nHY Credit Risk Premium\\nMedian\\n156\\n284\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\n1,600\\n1987 1991 1995 1999 2003 2007 2011 2015 2019 2023\\nBasis Points\\nData through December 2024.  \\nSource: Investment Strategy Group, Bloomberg, Moody’s Investor Service, Barclays, Haver \\nAnalytics, Federal Reserve.\\nExhibit 175: Spreads for High Yield Bonds and \\nLeveraged Loans\\nBank loans offer higher yield spreads compared to high yield \\nbonds to account for their weaker credit quality.\\nHigh Yield\\nLeveraged Loans\\n287\\n475\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\n1,600\\n1,800\\n2,000\\nBasis Points\\n1987 1993 1999 2005 2011 2017 2023\\nData through December 2024.  \\nSource: Investment Strategy Group, Bloomberg, Barclays, Standard & Poor’s, UBS. \\nDespite already tight spreads and our \\nexpectation of some widening this \\nyear, we believe current yields—7.5% \\nfor HY bonds and 8.8% for loans—\\nremain sufficient to drive mid-single-\\ndigit returns.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f40634fd-7ea7-4b27-84e2-b7622b2b10c1', embedding=None, metadata={'page_label': '102', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='102 Goldman Sachs january 2025\\nfederal elections in February 2025. Potential \\nreforms to the debt brake could increase fiscal \\nuncertainty and bond market volatility. \\n We are more cautious on Europe’s semi-core and \\nperipheral bond markets. The spreads of peripheral \\nbonds tightened significantly last year, leaving little \\nbuffer against adverse developments. Additionally, \\nbonds in these areas are likely to face pressure from \\nincreased supply, particularly in France and Italy (see \\nExhibit 177). At the same time, political instability \\nin France could further strain its already weak \\nfiscal position. Both France and Italy are currently \\nunder the European Commission’s excessive deficit \\nprocedure. Further deterioration in their fiscal \\ndiscipline could jeopardize their eligibility for the \\nECB’s Transmission Protection Instrument—a critical \\nsafeguard for peripheral bond markets. \\n In the UK, we expect lower yields to support \\nbond returns in 2025. Our forecast reflects a \\ncombination of below-trend growth, falling core \\ninflation, recent economic data underperforming \\nexpectations and tariff-related uncertainty. UK \\nbonds also offer attractive valuations across the \\nyield curve. At the front end, markets appear to \\nbe significantly underpricing our expected BOE \\ncutting path (see Exhibit 148), particularly given \\nthe recent deterioration in labor market indicators. \\n We also see value further out the curve, as the \\nmarket pricing of the nominal neutral rate—at \\n3%—has scope to reach our 2.50% estimate. Term \\npremium is also above our estimates and the levels \\nimplied by our suite of models. Although term \\npremiums are unlikely to converge completely back \\nto fair value in the face of ongoing budget-related \\nuncertainty, today’s large valuation buffer should \\nensure that the strong demand for UK fixed income \\nseen in 2024 persists. \\n Given this confluence of supportive factors, we \\nexpect 10-year gilt yields to decline to 3.50–4.00% \\nin 2025, delivering double-digit returns in our base \\ncase (see Exhibit 178). Accordingly, we recommend \\ninvestors overweight UK fixed income (see Section \\nI, Our Tactical Tilts).\\nEmerging Market Local Debt \\nLast year underscored how currency fluctuations \\ncan dominate total returns in emerging market \\nlocal debt. While local returns of 5% outpaced the \\n10-year average, currency depreciation ultimately \\nreduced this return to a disappointing 2% loss. With \\nlocal returns being no better than the current yield \\nof the index in 2024, markets appeared to have \\nbegun the year already fully pricing in the sizable \\n2,000 basis points of rate cuts that followed.76\\nExhibit 176: 2025 German Bunds Total \\nReturn Scenarios\\nGerman bunds have a particularly attractive  \\nrisk/reward profile.\\nBase Case—50% Probability\\nBull Case—40% Probability\\nBear Case—10% Probability\\n2.2\\n6.7\\n10.5\\n2.5\\n3.3\\n5.9\\n2.7\\n0.4\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nEUR Cash 5-Year Bund 10-Year Bund\\nTotal Return (%)\\n0.3\\nData as of December 31, 2024. \\nSource: Investment Strategy Group.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can \\nbe no assurance forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook. \\nExhibit 177: 2025 Privately Available Net Bond \\nSupply Forecast Across Germany, France and Italy\\nThe increase in Eurozone net privately available bond supply \\nis concentrated in France and Italy.\\n2024\\n2023\\n2025\\n151\\n134\\n94\\n120\\n142\\n112111\\n162\\n141\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\n180\\n200\\nGermany France Italy\\n€ Billions\\nData through 2024. Forecast through 2025. \\nSource: Investment Strategy Group, Bloomberg, Goldman Sachs Banking and Markets. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.  \\n ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c0df97c3-f002-4c4a-8599-f2cd42330228', embedding=None, metadata={'page_label': '103', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='103Outlook Investment Strategy Group\\n For the coming year, the scope for further \\nmonetary easing appears limited in several EM \\neconomies (see Exhibit 179), as core services \\ninflation remains sticky, particularly outside of \\nAsia (see Section II). Additionally, the real yield \\ndifferential between EM local debt and US yields \\nremains in the bottom quartile of its 20-year range, \\nleaving investors with little risk premium (see \\nExhibit 180). \\n Against this backdrop, we expect EM \\nlocal debt to deliver mid-single-digit returns in \\n2025, with risks skewed to the downside. That \\nsaid, wide performance dispersion across local \\nmarkets should continue to offer selective tactical \\nopportunities this year. \\nEmerging Market Dollar Debt \\nIn contrast to local market debt, EM dollar debt \\nperformed well last year, with an attractive 7% gain \\nthat nearly matched that of US high yield. Returns \\nwere aided by spread tightening, particularly in the \\nmost distressed segments of the market. EM high \\nyield spreads narrowed by approximately 140 basis \\npoints (see Exhibit 181), while credits rated CCC \\nand below tightened by more than 1,000 basis \\npoints. In turn, countries such as Argentina, Ecuador \\nand Lebanon posted returns exceeding 70%. \\n For the year ahead, we view an encore of \\nthis strong performance as unlikely. The spread \\ntightening seen in 2024 has left valuations \\nstretched. Current spreads across ratings segments \\nExhibit 179: Policy Rate Changes in \\nEmerging Markets\\nThe scope for further monetary easing appears limited in \\nseveral EM economies\\nExpected Policy Rate Change in 2025\\nPolicy Rate Change in 2024\\nTotal Change\\n-11.4\\n-6.3 -5.5\\n-3.8 -3.6 -2.8 -2.4 -1.5 -1.3 -1.2 -1.0 -0.8 -0.8 -0.6\\n0.00.0\\n0.9 2.0\\n-20\\n-15\\n-10\\n-5\\n0\\n5\\n10\\nTürkiye\\nColombia\\nHungary\\nChile\\nCzechia\\nMexico\\nPeru\\nPhilippines\\nSouth Africa\\nSouth Korea\\nPoland\\nIndonesia\\nIndia\\nThailand\\nTaiwan\\nMalaysia\\nRussia\\nBrazil\\nPercentage Points\\nData as of December 31, 2024. \\nNote: Expected policy rates are based on Bloomberg consensus expectations. \\nSource: Investment Strategy Group, Bloomberg. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved.  \\n \\nExhibit 178: 2025 UK Gilts Total Return Scenarios\\nWe expect UK gilts to perform strongly in 2025. \\n \\nBase Case—50% Probability\\nBull Case—30% Probability\\nBear Case—20% Probability\\n3.9\\n13.1\\n17.1\\n4.3\\n8.1\\n11.8\\n4.8\\n2.2\\n-0.6-2\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\nGBP Cash 5-Year Gilt 10-Year Gilt\\nTotal Return (%)\\nData as of December 31, 2024. \\nSource: Investment Strategy Group.  \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can \\nbe no assurance forecasts will be achieved. Indices are gross of fees and \\nreturns can be significantly varied. Please see additional disclosures at the \\nend of this Outlook.\\nExhibit 180: Real Yield Differential: Emerging \\nMarkets Local Debt Less US 5-Year Treasury\\nThe real yield difference between EM local debt and US \\nTreasuries remains historically low.\\nReal Yield Differential (Trailing Headline Inﬂation)\\nReal Yield Differential (Trailing Core Inﬂation)\\n1.3\\n2.2\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n2003 2006 2009 2012 2015 2018 2021 2024\\nPercentage Points\\nData through November 2024. \\nNote: Adjusted for realized inflation. \\nSource: Investment Strategy Group, Bloomberg, JP Morgan, Haver Analytics.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='1e1eb8eb-6d66-430f-b396-25ae9d250ba3', embedding=None, metadata={'page_label': '104', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='104 Goldman Sachs january 2025\\nare near historic lows, standing in the bottom \\nfifth percentile over the last five years and in the \\nbottom 20th percentile over the last 10 years. At \\nthe same time, the strong US dollar environment \\nwe expect has historically been challenging for EM \\nfundamentals and credit spreads. \\n Given these headwinds, we expect EM spreads \\nto widen by 50–75 basis points, returning to levels \\ncloser to the decade’s median. While this spread \\nwidening will erode part of the index’s current 6% \\nyield, we still anticipate low- to mid-single-digit \\nreturns for EM dollar debt in 2025. \\n2025 Global Commodities Outlook\\nCommodity markets often change the locks just \\nwhen investors think they have found the keys. \\nRecent years have been no exception: investors \\ngained confidence after two consecutive years of \\nexcess returns in 2021 and 2022—fueled by supply \\nchain disruptions, rising inflation and Russia’s \\ninvasion of Ukraine—only to be caught off guard \\nby losses in 2023 and just a modest rebound last \\nyear. The GSCI’s 4% return in 2024 also masked \\nuneven performance across subsectors. Gains in \\nprecious metals and livestock weren’t enough to \\noffset losses in agriculture and industrial metals. \\nEnergy prices were rangebound (see Exhibit 182).\\n The relative stability in energy prices last year \\nstood in stark contrast to pervasive geopolitical \\nrisks and moderating demand growth. This \\nresilience reflected historically high spare capacity \\nin oil markets, which helped balance declining \\ninventories and modest periodic disruptions. \\nLooking ahead, we expect steady global economic \\ngrowth to support demand, while ample spare \\ncapacity should again keep prices in check, barring \\na major supply disruption.\\n Gold emerged as a standout performer last \\nyear, reaching a new all-time high in October and \\noutperforming the S&P 500 with a 27% spot \\nreturn. Its strength was driven by a combination of \\nfactors, including elevated central bank purchases, \\nmonetary easing, rising national debt concerns and \\ngeopolitical risks.\\n While we remain neutral on commodities \\noverall, we continue to recommend a small \\noverweight to uranium, which we believe offers \\nattractive asymmetry given an ongoing structural \\ndeficit (see Section I, Our Tactical Tilts).\\nExhibit 182: Commodity Returns in 2024\\nCommodities as a whole delivered modest gains last year.\\nS&P GSCI Energy Agriculture Industrial Metals Precious Metals Livestock\\nPrice Average, 2024 vs. 2023 -3% -5% -12% 5% 23% 7%\\nSpot Price Return 3% -1% -1% 4% 27% 16%\\nExcess Return* 4% 4% -5% -2% 20% 14%\\nData as of December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg. \\n* Excess return corresponds to the actual return from being invested in the front-month contract and differs from spot price return, depending on the shape of the forward curve. An upward-sloping \\ncurve (contango) is negative for returns, while a downward-sloping curve (backwardation) is positive. \\nPast performance is not indicative of future results. Investing in commodities involves substantial risk and is not suitable for all investors.\\nExhibit 181: Emerging Market Sovereign Bond \\nSpread to Worst\\nEM high yield spreads narrowed significantly last year.\\nOverall\\nInvestment Grade\\nHigh Yield\\n325\\n119\\n559\\n0\\n200\\n400\\n600\\n800\\n1,000\\n1,200\\n1,400\\n2019 2020 2021 2022 2023 2024\\nBasis Points\\nData through December 31, 2024. \\nSource: Investment Strategy Group, Bloomberg, JP Morgan.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='246bd32d-f8df-4223-bb0b-1bca0d0a0f22', embedding=None, metadata={'page_label': '105', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='105Outlook Investment Strategy Group\\nOil: Uneasy Equilibrium\\nOil did not live up to its reputation for volatility \\nin 2024. Prices traded within their tightest range \\nsince 2013 in percentage terms and within their \\nnarrowest range since 1998 in real dollar terms. \\nWTI crude oil spent nearly 80% of 2024 within \\nour expected trading range of $60–80 and ended \\nthe year flat. This uncharacteristic stability reflected \\na balanced oil market, where ample spare capacity, \\ndisciplined production and moderating demand \\ngrowth combined to offset geopolitical risks.  \\n Supply disruptions remained modest despite \\nactive military conflicts in the Middle East \\nand Russia’s ongoing war in Ukraine. OPEC+ \\nmaintained remarkable discipline, extending \\nproduction cut agreements through 2026 and \\npostponing plans to unwind cuts. This resulted \\nin historically high spare capacity, providing a \\nbuffer against unexpected supply shocks (see \\nExhibit 183). Meanwhile, global oil demand \\ngrowth slowed for a third straight year, even as \\nconsumption demand recovered from pandemic \\ndistortions and reached a new all-time high.\\n Looking ahead to 2025, we expect trend-like \\nglobal economic activity to drive oil demand \\nhigher by around 1 million b/d, similar to last \\nyear. However, Chinese demand growth is likely to \\nremain subdued at around 0.1–0.2 million b/d due \\nto its slower economic growth (see Section II) and \\na shift away from petroleum-consuming passenger \\nand freight transportation vehicles. This would \\nleave China’s demand growth trailing India’s for a \\nsecond year (see Exhibit 184).\\n To keep the market in balance, oil supply will \\nneed to increase to meet demand growth. Increased \\noil production is most likely to come from among \\nnon-OPEC+ countries, which now contribute the \\nmajority of global oil supply (see Exhibit 185). \\nIndeed, the approximately 1.2 million b/d in supply \\ngrowth we expect from countries such as the US, \\nGuyana, Brazil and Canada is sufficient to satisfy \\nglobal demand growth on its own. Moreover, much \\nof this expected growth arises from large-scale \\noffshore developments whose production is not \\nsensitive to market oil prices. \\n Given this non-OPEC+ growth, the burden of \\nmanaging supply relative to demand rests with \\nOPEC+. We expect the group to maintain its recent \\nsupply discipline this year unless prices move \\nsustainably higher. This should ensure ample spare \\ncapacity to absorb unexpected disruptions. \\n Given a roughly balanced oil market, we \\nexpect WTI crude oil prices to end 2025 in a range \\nof $60–80, consistent with the current forward \\ncurve and last year’s range. That said, risks to our \\noutlook remain. OPEC’s spare capacity provides a \\ntangible buffer, but the oil market’s other buffer—\\nobservable global oil inventories—is near multiyear \\nExhibit 183: OPEC Spare Capacity\\nOPEC spare capacity is historically high, providing a buffer in \\ncase of supply disruptions.\\n4.6\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n2012 2014 2016 2018 2020 2022 2024\\nMillion b/d\\nData through December 2024.  \\nSource: Investment Strategy Group, Energy Aspects. \\n \\n \\n \\nExhibit 184: Global Oil Demand Growth\\nIndia is expected to lead China in oil demand growth for a \\nsecond consecutive year, based on consensus forecasts.\\nMillion b/d\\n-10\\n-8\\n-6\\n-4\\n-2\\n0\\n2\\n4\\n6\\n8\\n2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025c\\nIndia\\nChina\\nOther\\nData through 2024. Forecast through 2025. \\nSource: Investment Strategy Group, IEA, EIA, OPEC, Energy Aspects, S&P Global Commodity \\nInsights, Goldman Sachs Global Investment Research, Morgan Stanley, JP Morgan, Citigroup. \\nForecasts are estimated, based on assumptions, are subject to revision \\nand may change as economic and market conditions change. There can be \\nno assurance forecasts will be achieved. ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='51c05655-5d9d-4d93-9b2c-1ad9f404c46e', embedding=None, metadata={'page_label': '106', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='106 Goldman Sachs january 2025\\nlows (see Exhibit 186). Additionally, OPEC+ could \\nchoose not to respond swiftly to a supply shock, \\nwhile weaker global demand, a lapse in OPEC+ \\nproduction discipline and larger-than-expected \\nnon-OPEC+ production growth could also test \\nmarket stability. \\n Policy uncertainty also looms. The incoming \\nTrump administration may tighten sanctions on \\nIran, Russia and Venezuela, but is likely to prioritize \\nlow energy prices—balancing geopolitical objectives \\nwith domestic economic considerations. Meanwhile, \\nwhile deregulation could increase oil drilling \\npermits, logistical bottlenecks and US producers’ \\ncapital discipline make a production surge unlikely. \\n Given these crosscurrents, we do not hold an \\nactive tactical position in oil currently. Instead, \\nwe continue to recommend a small overweight to \\nthe US midstream sector, which offers strong cash \\nflows, benefits from growing energy volumes and \\nhas less direct exposure to oil price volatility.\\nGold: Golden Crossroads\\nNo other commodity shined as brightly as gold \\nlast year, with its 27% surge marking the best \\nannual performance in a decade. Gold’s strength \\nwas notable for reflecting familiar drivers—\\nsizable central bank purchases, easier monetary \\npolicy, rising national debt levels and pervasive \\ngeopolitical risks—while also defying traditional \\nheadwinds, including rising real interest rates and a \\nstronger US dollar. Last year marked only the third \\ntime since 1997 in which gold rallied despite these \\nheadwinds, although its inverse correlation with \\nUS real interest rates has weakened in recent years \\n(see Exhibit 187). \\n This breakdown of once-reliable relationships \\nunderscores a notable shift in the appeal of gold \\namong key buyers. Central banks—particularly in \\ncountries such as China, India and Türkiye—have \\nsharply increased gold purchases in recent years \\nExhibit 185: OPEC+ vs. Non-OPEC+  \\nOil Production\\nThe majority of global oil production now comes from  \\nnon-OPEC+ countries.\\n50\\n54\\n35\\n40\\n45\\n50\\n55\\n60\\n2015 2017 2019 20212016 2018 2020 2022 2023 2024\\nMillion b/d\\nOPEC+\\nNon-OPEC+\\nData through November 2024. \\nSource: Investment Strategy Group, IEA.\\nExhibit 187: Gold Price and US 10-Year Real \\nInterest Rates\\nThe historical relationship between gold and real rates has \\nweakened in recent years.\\nGold Price\\n10-Year Real Interest Rate (Right, Inverted)\\n-2.0\\n-1.0\\n0.0\\n1.0\\n2.0\\n3.0\\n4.00\\n500\\n1,000\\n1,500\\n2,000\\n2,500\\n3,000\\n2006 2008 2010 2012 2014 2016 2018 2020 2022 2024\\nUS$/Ounce %\\nData through December 27, 2024. \\nSource: Investment Strategy Group, Bloomberg.\\nExhibit 186: Observable Global Petroleum \\nInventories\\nInventories remain near multiyear lows.\\n7.1\\n7.3\\n7.0\\n7.5\\n6.5\\n8.0\\n8.5\\nBillion Barrels\\n2017 2018 2019 2020 2021 2022 2023 2024\\nGlobal Onshore and Seaborne Crude Oil and Products\\n2017–19 Average\\nData through December 27, 2024. \\nSource: Investment Strategy Group, Kpler, S&P Global Platts, IEA, EIA, PJK, PAJ, International \\nEnterprise Singapore.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='376d166e-fc54-43a0-a5cc-7cd9e1124da0', embedding=None, metadata={'page_label': '107', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='107Outlook Investment Strategy Group\\nas part of broader efforts to diversify foreign \\nexchange reserves and reduce reliance on the US \\ndollar. Heightened trade uncertainty, geopolitical \\ntensions, and the US dollar’s use in sanctions \\nhave further bolstered gold’s appeal in these \\nbuyers’ eyes. \\n Global central bank purchases of gold topped \\n1,000 tonnes in 2022–23 and were on track \\nto reach similar levels again late last year (see \\nExhibit 188). Consumer demand in emerging \\nmarkets has also surged, driven by gold’s enduring \\nreputation as both a store of value and a hedge \\nagainst geopolitical risk. Collectively, this \\ndemand has supported prices even amid volatile \\nmacroeconomic conditions. \\n The renewed enthusiasm for gold is also visible \\nin investor positioning. While ETF holdings of \\ngold in volume terms actually declined in 2024, \\ntheir value in dollars reached an all-time high. \\nSimilarly, net speculative positioning in the futures \\nand options markets remains elevated, standing \\nin its 88th percentile since 1995. All told, global \\ninvestment demand accounted for about a quarter \\nof gold purchases through the third quarter of last \\nyear, roughly in line with long-run averages but \\nhigher than levels seen in 2021–23.\\n The macroeconomic backdrop presents a \\nmore mixed outlook for gold. We forecast positive \\nbut falling real interest rates this year, which \\nhave historically been neutral for gold relative \\nto unconditional returns (see Exhibit 189). We \\nalso expect a modestly stronger dollar, which has \\nhistorically limited meaningful gold appreciation. \\nThat said, the reliability of both of these macro \\ndrivers has weakened in recent years, as discussed. \\n On balance, these crosscurrents leave us neutral \\non gold prices from current levels. Central banks \\nhave been net buyers of gold for over a decade, \\nsuggesting their demand may be structural. \\nHowever, with gold already near all-time highs \\nand investors positioned for further upside, price \\nsensitivity among buyers is likely to increase. \\nExhibit 188: Gold Price Performance vs. Central \\nBank Purchases\\nCentral bank gold purchases have been elevated for some \\ntime, providing support for gold.\\nNet Demand\\nNet Supply\\n-1,200\\n-1,000\\n-800\\n-600\\n-400\\n-200\\n0\\n200\\n400\\n600\\n8000\\n500\\n1,000\\n1,500\\n2,000\\n3,000\\n2,500\\n1977 1982 1987 1992 1997 2002 2007 2012 2017 2022\\nUS$/Ounce Tonnes\\nReal Gold Price\\nCentral Bank Supply/Demand (Right, Inverted)\\nData through 2024. \\nNote: 2024 data is annualized. \\nSource: Investment Strategy Group, Bloomberg, World Gold Council. \\n \\nExhibit 189: Average Gold Returns by Real Interest \\nRate Regime\\nPositive and falling real interest rates have historically been \\nneutral for gold relative to unconditional returns.\\n1.6\\n1.2\\n0.0 0.3\\n2.5\\n0.8\\n0.3\\n-0.8\\n3.2\\n0.6\\n0.2\\n-0.3\\n-2\\n-1\\n0\\n1\\n2\\n3\\n4\\n5\\nNegative, Falling Positive, Falling Positive, Rising Negative, Rising\\nReal Interest Rate Regime: Level, Direction\\nGold Average Monthly Return (%)\\nUnconditional: 0.9%\\nReal 5-Year Treasury\\nReal Policy Rate\\nReal 10-Year Treasury\\nData through December 31, 2024. \\nNote: Data begins in 1971. \\nSource: Investment Strategy Group, Bloomberg. \\nPast performance is not indicative of future results.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='36f3c126-542a-4c23-8d5c-cbb9907a597e', embedding=None, metadata={'page_label': '108', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Notes\\n1. Model portfolio performance is \\nshown for illustrative purposes \\nonly and has certain limitations. \\nIt does not represent actual \\ntrading and thus may not \\nreflect material economic and \\nmarket factors, such as liquidity \\nconstraints, that may have had \\nan impact on actual decision-\\nmaking. It is based on indices \\nand public proxies for private \\nassets. The moderate strategic \\nportfolio refers to a diversified \\nlong-term asset allocation \\nwith a level of risk similar to \\nthat of the moderate reference \\nportfolio. The indices and public \\nproxies used in this calculation \\nare: US Investment Grade \\nMunicipal Bonds:  Barclays \\nCapital Municipal 1-10 (Feb-09 \\nto Dec-24); US Dollar Debt:  \\nBarclays Capital US Aggregate \\n(Feb-09 to Jun-22), Bloomberg \\nIntermediate US Government/\\nCredit (Jul-22 to Dec-24); US \\nTreasury Inflation Protected \\nSecurities:  Barclays Capital \\nUS TIPS (Feb-09 to Dec-14), \\nBarclays Capital US TIPS \\n1-10 (Jan-15 to Oct-19); US \\nMunicipal High Yield:  \\nBarclays Capital Municipal High \\nYield (Feb-09 to Dec-24); US \\nHigh Yield Bonds:  Barclays \\nCapital US Corporate High Yield \\n(Feb-09 to Dec-24); Emerging \\nMarkets Local Debt:  JPM GBI \\nEmerging Markets Global (Jan-\\n13 to Jan-16); US Large Cap \\nValue Equity: Russell 1000 \\nValue (Feb-09 to Dec-24); US \\nLarge Cap Growth Equity: \\nRussell 1000 Growth (Feb-09 \\nto Dec-24); US Small Cap \\nEquity: Russell 2000 (Feb-09 to \\nDec-12); US Small Cap Value \\nEquity:  Russell 2000 Value \\n(Jan-13 to Dec-24); US Small \\nCap Growth Equity: Russell \\n2000 Growth (Jan-13 to Dec-\\n24); Infrastructure Master \\nLimited Partnerships:  Alerian \\nInfrastructure MLPs (Jan-13 \\nto Dec-17); Global Public \\nREITs: S&P Global REITs Local \\nTotal Return (Jan-13 to Dec-14), \\nS&P Global REITs USD Total \\nReturn (Jan-15 to Dec-17); \\nNon-US Developed Equity:  \\nMSCI EAFE USD Total Return \\n(Feb-09 to Dec-14), 50% MSCI \\nEAFE USD Total Return / 50% \\nMSCI EAFE Local Total Return \\n(Jan-15 to Sep-23), 50% MSCI \\nEAFE Unhedged Total Return / \\n50% MSCI EAFE Hedged Total \\nReturn (Oct-23 to Dec-24); \\nEmerging Markets Equity: \\nMSCI Emerging Markets USD \\nTotal Return (Feb-09 to Dec-\\n24); Income-Oriented Equity: \\n33% S&P Global REITs USD \\nTotal Return / 33% Alerian \\nInfrastructure MLPs / 33% \\nMSCI World Infrastructure USD \\nTotal Return (Dec-17 to Dec-24); \\nRelative Value Hedge Funds: \\n20% CS/Tremont Convertible \\nArbitrage / 40% CS/Tremont \\nEquity Market Neutral / 40% \\nCS/Tremont FI Arbitrage \\n(Feb-09 to Dec-12); Event \\nDriven Hedge Funds:  CS/\\nTremont Event Driven (Feb-09 \\nto Dec-24); Equity Long/Short \\nHedge Funds:  CS/Tremont \\nEquity Long/Short (Feb-09 to \\nDec-24); Macro/Tactical \\nTrading Hedge Funds:  50% \\nCS/Tremont Global Macro / \\n50% CS/Tremont Managed \\n(Feb-09 to Dec-24); Buyout / \\nBuyout and Secondaries:  \\n60% MSCI World All Country \\n/ 40% S&P Global Small Cap \\n(Feb-09 to Dec-12), MSCI \\nWorld Developed USD Total \\nReturn (Jan-13 to Dec-24); \\nMezzanine:  80% Barclays \\nCapital US High Yield Corporate \\n/ 20% S&P Global Small Cap \\n(Feb-09 to Dec-12), Barclays \\nCapital Global High Yield (Jan-\\n13 to Dec-17); Distressed:  \\n50% HFRI Distressed / 50% \\nMSCI World Developed (Feb-09 \\nto Dec-12), 50% CS/Tremont \\nDistressed / 50% MSCI World \\nDeveloped (Jan-13 to Dec-24); \\nPrivate Credit: Barclays Capital \\nGlobal High Yield (Jan-18 to \\nDec-24); Venture:  Nasdaq \\nPrice Return (Feb-09 to Dec-14), \\nNasdaq Total Return (Jan-15 \\nto Dec 24); Growth Private \\nEquity:  66.75% S&P 500 / \\n22.25% Russell 3000 Growth \\n/ 8.25% MSCI EAFE / 2.75% \\nMSCI EAFE Growth (Jul-22 \\nto Dec-24); Energy Private \\nEquity:  35% S&P 600 Energy \\n/ 15% S&P 600 Utilities / 35% \\nMSCI EAFE Energy / 15% MSCI \\nEAFE Utilities (Feb-09 to Dec-\\n12), Dow Jones World Oil & Gas \\n(Jan-13 to Jun-21); Emerging \\nMarkets Private Equity: \\nMSCI Emerging Markets USD \\nTotal Return (Jan-13 to Jan-16); \\nPrivate Real Estate:  GPR 250 \\nGlobal Index (Feb-09 to Dec-12); \\nCore Private Real Estate: \\nS&P Global REITs Local Total \\nReturn (Jan-13 to Dec-14), S&P \\nGlobal REITs USD Total Return \\n(Jan-15 to Dec 24); Private \\nInfrastructure:  MSCI World \\nInfrastructure USD Total Return \\n(Jan-18 to Dec-24).\\n2. David Kostin et al., 2024 \\nPortfolio Passport: Analyzing \\nForeign Sales Exposure of US \\nFirms, Goldman Sachs Global \\nInvestment Research, June \\n17, 2024. \\n3. Katie Martin, “Year in a Word: \\nMemecoin,” Financial Times , \\nDecember 23, 2024.\\n4. Jennifer B. Nuzzo and Jorge \\nR. Ledesma, “Why Did the \\nBest Prepared Country in the \\nWorld Fare So Poorly During \\nCOVID?” Journal of Economic \\nPerspectives, Fall 2023. \\n5. Tim Callen, “Purchasing Power \\nParity: Weights Matter,” \\nInternational Monetary Fund. \\n6. Bradley C. Parks et al., \"Belt \\nand Road Reboot: Beijing’s \\nBid to De-Risk Its Global \\nInfrastructure Initiative,\" \\nAidData at William & Mary, \\nNovember 2023. \\n7. Sharmin Mossavar-Rahmani \\net al., Middle Kingdom: Middle \\nIncome, Goldman Sachs Wealth \\nManagement, December 2022. \\n8. Robert J. Gordon, The Rise \\nand Fall of American Growth , \\nPrinceton University Press, \\n2016. \\n9. Based on Preqin data.\\n10. Daron Acemoglu, “The Fall and \\nRise of American Democracy,” \\nProject Syndicat e, December \\n3, 2024. \\n11. Pippa Norris, “Trump’s Threat \\nto American Democracy,” U.S. \\nElection Analysis 2024: Media, \\nVoters and the Campaig n. Ed. \\nDaniel Jackson et al., University \\nof Bournemouth, 2024.\\n12. Eswar Prasad, “The Ironies \\nof Trump’s Tantrums About \\nthe Dollar,” Financial Times , \\nDecember 24, 2024.\\n13. Harlan Ullman, “Will America’s \\nChecks and Balances Survive \\nthe Trump ‘Politburo?’” The Hill , \\nDecember 23, 2024. \\n14. Charles Krauthammer, \\n“American Democracy: Not \\nSo Decadent After All,” \\nWashington Post, March 23, \\n2017. \\n15. Charles Krauthammer, “The \\nGuardrails Hold,” Washington \\nPost, August 3, 2017. \\n16. Lionel Barber, “Lunch With the \\nFT: James Baker,” Financial \\nTimes, June 2, 2016. \\n17. Alexis de Tocqueville, \\nDemocracy in America, Volume \\nI, 1835.\\n18. David Kostin et al., 2024 \\nPortfolio Passport: Analyzing \\nForeign Sales Exposure of US \\nFirms, Goldman Sachs Global \\nInvestment Research, June \\n17, 2024.\\n19. Katie Martin, “Tina Is Back: \\nInvestors Say There Is No \\nAlternative to US Equities,” \\nFinancial Times, November 22, \\n2024.\\n20. The broad technology \\nsector includes information \\ntechnology, broadline retail, \\nand interactive media and \\nservices.\\n21.  Joseph Briggs et al., Upgrading \\nOur Longer-Run Global Growth \\nForecasts to Reflect the Impact \\nof Generative AI (Briggs/\\nKodnani), Goldman Sachs \\nGlobal Investment Research, \\nOctober 29, 2023. \\n22. Alison Nathan et al., Gen AI: \\nToo Much Spend, Too Little \\nBenefit? Goldman Sachs Global \\nInvestment Research, June \\n25, 2024. \\n23. David Kostin et al., 2024 \\nPortfolio Passport: Analyzing \\nForeign Sales Exposure of US \\nFirms, Goldman Sachs Global \\nInvestment Research, June \\n17, 2024.\\n24. Paul Krugman, “The Myth of \\nAsia’s Miracle: A Cautionary \\nFable,” Foreign Affairs, \\nNovember/December 1994. \\n25. Lawrence H. Summers and Lant \\nPritchett, “Asiaphoria Meets \\nRegression to the Mean,” \\nNational Bureau of Economic \\nResearch, October 2014. \\n26. Joseph Nye, Is the American \\nCentury Over? Policy Press, \\n2015.\\n27. Kenneth Rogoff and Yuanchen \\nWang, “China’s Real Estate \\nChallenge,” International \\nMonetary Fund Finance & \\nDevelopment Magazine , \\nDecember 2024. \\n28. Lawence H. Summers, in \\nconversation with Sharmin \\nMossavar-Rahmani at Goldman \\nSachs Alternatives Summit, \\nNovember 1, 2023.\\n29. Christopher Matthews, \\n“What’s Behind the Crash in \\nthe Gold Market?” Time , April \\n16, 2013.\\n30. Peter L. Bernstein, The Power \\nof Gold: The History of an \\nObsession, John Wiley & Sons, \\n2000.\\n31. Alessio Farhadi, “Part 1: \\nOutlook 2025 | When Big Egos \\non Wall Street Capitulate,” \\nSpeevr Intelligence, December \\n30, 2024.\\n32. Bill Gates, in an interview with \\nCNBC, June 17, 2022.\\n33. Marc Verdonk, “Quantum \\nComputers and the Bitcoin \\nBlockchain,” Deloitte, https://\\nwww.deloitte.com/nl/en/\\nservices/risk-advisory/\\nperspectives/quantum-\\ncomputers-and-the-bitcoin-\\nblockchain.html.\\n34. Hartmut Neven, “Meet Willow, \\nOur State-of-the-Art Quantum \\nChip,” Google, https://blog.\\ngoogle/technology/research/\\ngoogle-willow-quantum-chip.\\n35. Wences Casares (CEO and ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='91dd7230-23c5-4bd9-a587-ea90f42aed57', embedding=None, metadata={'page_label': '109', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='co-founder of Xapo and PayPal \\nboard member), in an email \\nexchange with the Investment \\nStrategy Group, June 4, 2021.\\n36. Aswath Damodaran, “The \\nBitcoin Boom: Asset, Currency, \\nCommodity or Collectible?” \\nOctober 24, 2017, https://\\naswathdamodaran.blogspot.\\ncom/2017/10/the-bitcoin-\\nboom-asset-currency.html\\n37. These forecasts have \\nbeen generated by ISG for \\ninformational purposes as of \\nthe date of this publication. \\nTotal return targets are based \\non ISG’s framework, which \\nincorporates historical valuation, \\nfundamental analysis and \\ntechnical analysis. They are \\nbased on proprietary models and \\nthere can be no assurance that \\nthe forecasts will be achieved. \\nThe following indices were used \\nfor each asset class: Barclays \\nMunicipal 1-10Y Blend (Muni \\n1-10); BAML US T-Bills 0-3M \\nIndex (Cash); JPM Government \\nBond Index; Emerging Markets \\nGlobal Diversified (Emerging \\nMarket Local Debt); Barclays \\nHigh Yield Municipal Bond Index \\n(Muni High Yield); HFRI Fund of \\nFunds Composite (Hedge Funds); \\nBarclays US Corporate High Yield \\n(US High Yield); MSCI EM US$ \\nIndex (Emerging Market Equity); \\nFTSE 100 (UK Equities); MSCI \\nEAFE Local Index (EAFE Equity); \\nEuro Stoxx 50 (Eurozone Equity); \\nTOPIX Index (Japan Equity); S&P \\n500 (US Equity); FTSE Germany \\nGBI 7-10 Year Local (10-Year \\nGermany); FTSE UK GBI 7-10 Year \\nLocal (10-Year UK); the reference \\nbenchmark for a taxable \\nmoderate portfolio assumes \\n50% Munis (Bloomberg Muni \\n1-10) and 50% Global Equity \\n(MSCI ACWI). The reference \\nbenchmark for a tax-exempt \\nmoderate portfolio assumes \\n50% Intermediate Bonds \\n(Intermediate Government/\\nCredit) and 50% Global Equity \\n(MSCI ACWI).\\n38. Karen Elliott House, “The \\nMiddle East Is Up for Grabs,” \\nWall Street Journal, December \\n15, 2024.\\n39. NATO Secretary-General Mark \\nRutte, “To Prevent War, NATO \\nMust Spend More,” speech at \\nthe Concert Noble, Brussels, \\nDecember 12, 2024.\\n40. Chief of Defence Staff Admiral \\nSir Tony Radakin, “Chief of the \\nDefence Staff RUSI Lecture \\n2024,” annual RUSI lecture, \\nDecember 4, 2024.\\n41. Brookings Institution, “A \\nConversation With Commander \\nof US Indo-Pacific Command \\nAdmiral Samuel Paparo,” \\nNovember 19, 2024.\\n42. The Cipher Brief, “Top U.S. \\nCybersecurity Official: \\nChina Attacks on American \\nInfrastructure ‘Tip of the \\nIceberg,’” December 11, 2024.\\n43. Jordan Fabian and Ellen M. \\nGilmer, “Mayorkas Detects \\nUptick in Foreign Terrorist \\nThreats to US,” Bloomberg, \\nJune 17, 2024.\\n44. Tom Cole, “How to Avoid \\nthe Coming Federal Debt \\nAvalanche,” Wall Street \\nJournal, December 19, 2024.\\n45. Mitch Daniels, “‘The Day the \\nDollar Died’ Is Coming. What’s \\nthe Plan?” Washington Post , \\nSeptember 19, 2024.\\n46. Committee for a Responsible \\nFederal Budget, “Government \\nSpending Just Keeps On \\nGrowing,” June 21, 2024.\\n47. Jagadeesh Gokhale and Kent \\nSmetters, “When Does Federal \\nDebt Reach Unsustainable \\nLevels?,” Penn Wharton, \\nUniversity of Pennsylvania, \\nOctober 6, 2023.\\n48. Jonathan Anderson, “Why \\nChina Can’t Lead the Global \\nSouth,” Emerging Advisors \\nGroup, December 12, 2024.\\n49. US Department of Defense, \\n“Annual Report to Congress: \\nMilitary and Security \\nDevelopments Involving the \\nPeople’s Republic of China \\n2024, December 18, 2024.\\n50. Dzirhan Mahadzir, “China \\nTargets Taiwan in Major \\nMilitary Exercise, Pentagon \\nCondemns ‘Irresponsible’ \\nAction,” US Naval Institute \\nNews, October 14, 2024.\\n51. House of Representatives, \\nSelect Committee on the \\nStrategic Competition Between \\nthe United States and the \\nChinese Communist Party, \\n“The CCP Cyber Threat to \\nthe American Homeland and \\nNational Security,” January \\n31, 2024.\\n52. House of Representatives, \\nSelect Committee on the \\nStrategic Competition Between \\nthe United States and the \\nChinese Communist Party, \\n“The CCP Cyber Threat to \\nthe American Homeland and \\nNational Security,” January \\n31, 2024.\\n53. CBS News, “Transcript: \\nRep. Michael Waltz on ‘Face \\nthe Nation with Margaret \\nBrennan,’” December 15, 2024.\\n54. Economist , “Bertrand Russell \\nand ‘The Problem of China,’” \\nDecember 20, 2022.\\n55. US Helsinki Commission, \\n“Spotlight on the Shadow \\nWar: Inside Russia’s Attacks \\non NATO Territory,” December \\n12, 2024.\\n56. Bojan Pancevski, “Brush With \\nRussia in Baltic Points to New \\nFlashpoint in NATO-Moscow \\nShadow War,” Wall Street \\nJournal, December 15, 2024.\\n57. Chantal da Silva, “Finland \\nSeizes Russia-Linked Tanker \\nSuspected of Cutting Vital \\nUndersea Cables,” NBC News, \\nDecember 27, 2024.\\n58. Hyung-Jin Kim and Kim Tong-\\nHyung, “Russia Supplied Air \\nDefense Missiles to North \\nKorea in Return for Its Troops, \\nSouth Korea Says,” Associated \\nPress, November 22, 2024.\\n59. IBM, “Cost of a Data Breach \\nReport 2024,” July 2024.\\n60. Ani Petrosyan, “Estimated \\nCost of Cybercrime Worldwide \\n2018–2029,” Statista, July \\n30, 2024.\\n61. Gartner, “Gartner Forecasts \\nGlobal Information Security \\nSpending to Grow 15% in \\n2025,” August 28, 2024.\\n62. Adam Greenberg, “Emerging \\nThreats: Cybersecurity Forecast \\n2025,” Google Cloud, November \\n13, 2024.\\n63. Sean Morrow and Don \\nRassler, “A View from the \\nCT Foxhole: General Bryan \\nFenton, Commander, U.S. \\nSpecial Operations Command,” \\nCombating Terrorism Center \\nSentinel, November 2024.\\n64. Bank of America Securities, \\n“Investment Strategy: The \\nThundering Word: Year Ahead \\n2025: Go BIG in ’25,” November \\n26, 2024.\\n65. International Monetary Fund, \\nWorld Economic Outlook , \\nOctober 2024, box 1.2.\\n66. Mario Draghi, The Future of \\nEuropean Competitiveness , \\nEuropean Commission, \\nSeptember 2024.\\n67. In an environment where yield \\ncurves are positively sloped, \\nthe yield on a government bond \\nwill fall as time passes and \\nthe bond approaches maturity, \\nassuming that market pricing \\nremains unchanged. This \\nrolldown return is an important \\ndriver of fixed income returns.\\n68. Sebastian Hillenbrand, “The \\nFed and the Secular Decline in \\nInterest Rates,” working paper, \\nJanuary 2022.\\n69. National Association of State \\nBudget Officers, Fiscal Survey \\nof States, Fall 2024.\\n70. National Association of State \\nBudget Officers, Fiscal Survey \\nof States, Fall 2024.\\n71. Interest coverage is measured \\nas the ratio of earnings before \\ninterest and taxes (EBIT) to \\nnet interest expense, both \\nmeasured over the trailing 12 \\nmonths.  \\n72. Leverage is measured by the \\nratio of net debt to earnings \\nbefore interest, taxes, \\ndepreciation and amortization \\n(EBITDA), both measured over \\nthe trailing 12 months. \\n73. The leveraged loan universe \\nis based on the S&P/UBS \\nLeveraged Loan Index, while \\nthe high yield universe is based \\non the Bloomberg US High Yield \\nIndex. \\n74. During 2024, 36 companies \\ndefaulted ($4.4 billion in bonds \\nand $21.7 billion in loans), \\nwhile 41 companies completed \\ndistressed exchanges ($9.9 \\nbillion in bonds and $33.7 billion \\nin loans).\\n75. To predict default rates, the \\nmodel uses corporate debt-to-\\nGDP versus trend, corporate \\ninterest coverage four-quarter \\nchange, one-year expectation \\nof a decline in real GDP, and \\nmanufacturing capacity \\nutilization versus trend. \\n76. The amount of total rate cuts \\nis based on central banks in \\nthe JPMorgan GBI-EM index, \\nexcluding Türkiye.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='80d27fbb-fb2a-47a7-9f31-3b416f132c0f', embedding=None, metadata={'page_label': '110', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Abbreviations Glossary\\nAI: artificial intelligence \\nb/d: barrels per day \\nBEI: breakeven inflation \\nBOE: Bank of England \\nBOJ: Bank of Japan\\nbps: basis points \\nBRICS: Brazil, Russia, India, China and South Africa\\nCAPE:  cyclically adjusted price-to-earnings\\nCapex: capital expenditure(s) \\nCBO: Congressional Budget Office\\nCEEMEA:  Central and Eastern Europe, Middle East and Africa\\nCPI: Consumer Price Index\\nDXY: US Dollar Index \\nEAFE:  Europe, Australasia and the Far East \\nEBIT: earnings before interest and taxes \\nEBITDA:  earnings before interest, taxes, depreciation and \\namortization \\nECB: European Central Bank  \\nEM: emerging market\\nEMEA:  Europe, Middle East and Africa \\nEPS: earnings per share \\nETF:  exchange-traded fund \\nEU: European Union \\nEUR: euro\\nFDA: [US] Food and Drug Administration\\nFDI: foreign direct investment\\nFOMC: Federal Open Market Committee\\nFX: foreign exchange \\nGDP: gross domestic product \\nGFC: global financial crisis\\nHICP: Harmonised Index of Consumer Prices \\nHY: high yield\\nIG: investment grade\\nIMF: International Monetary Fund \\nIPO: initial public offering\\nIT: information technology\\nLBO: leveraged buyout \\nLTM: last 12 months \\nM&A:  mergers and acquisitions \\nMLP: master limited partnership\\nMSCI ACWI:  MSCI All Country World Index\\nMSCI EAFE:  MSCI Europe, Australasia and the Far East \\nMSCI EM: MSCI Emerging Markets \\nMSCI UK:  MSCI United Kingdom \\nNATO: North Atlantic Treaty Organization\\nNBER: National Bureau of Economic Research \\nOECD: Organisation for Economic Co-operation and Development  \\nOPEC+: Organization of the Petroleum Exporting Countries and 11 \\nnon-OPEC members\\nPCE: Personal Consumption Expenditures [price index] \\npp: percentage point\\nPPP: purchasing power parity\\nR&D: research and development \\nROE: return on equity \\nSNB: Swiss National Bank \\nSOE: state-owned enterprise  \\nTCJA:  Tax Cuts and Jobs Act\\nTIPS: Treasury Inflation-Protected Securities  \\nWTI:  West Texas Intermediate [oil price] \\nWWII:  World War II\\nYTD:  year to date\\nYoY: year-over-year', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='05e396a1-26fc-4533-9885-1f95cf740932', embedding=None, metadata={'page_label': '111', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Important Information\\nOur Relationship with Clients.  \\nGoldman Sachs & Co. LLC (“we,” “us,” \\nand “GS&Co.,” and together with its \\naffiliates, “Goldman Sachs” or “GS”) \\nis registered with the Securities and \\nExchange Commission (“SEC”) as both \\na broker-dealer and an investment \\nadviser and is a member of the \\nFinancial Industry Regulatory Authority \\n(“FINRA”) and the Securities Investor \\nProtection Corporation (“SIPC”). \\nWe predominantly offer investment \\nadvisory and brokerage services to \\nretail investors through our Wealth \\nManagement business unit, which \\nincludes Private Wealth Management \\n(“PWM”). How we are compensated \\nby you may change over time and will \\ndepend on various factors.  Please \\nask questions and review the GS&Co. \\nForm CRS and GS&Co. Relationship \\nGuide/Regulation Best Interest \\ndisclosures (available at: https://\\nwww.goldmansachs.com/disclosures/\\ncustomer-relationship-summary-\\nform-crs/index.html) for important \\ninformation, including the difference \\nbetween advisory and brokerage \\naccounts, compensation, fees, conflicts \\nof interest, and our obligations to you. \\nWe are part of a full-service, integrated \\ninvestment banking, investment \\nmanagement, and brokerage firm. \\nOther firm businesses may implement \\ninvestment strategies that are \\ndifferent from the strategies used or \\nrecommended for your portfolio.  \\nIntended Audience.  This material is \\ngenerally intended for clients of PWM \\nand/or prospective clients who would \\nmeet the eligibility requirements to \\nbe clients of PWM. If you have any \\nquestions on whether this material is \\nintended for you, please contact your \\nPWM Team. Materials that discuss \\nadvisory services are generally \\nintended for individuals who are \\nQualified Clients as defined under \\nRule 305-3 of the Investment Advisers \\nAct of 1940.  Materials that discuss \\nalternative investment products are \\ngenerally intended for recipients who \\nqualify as Accredited Investors as \\ndefined in the Securities Act of 1933.  \\nGS&Co. considers client suitability, \\neligibility, and sophistication when \\ndistributing marketing materials; not \\nall materials are appropriate for all GS \\nclients. Distribution is premised on the \\nreasonable belief that the recipient has \\nsufficient financial expertise and/or \\naccess to resources to independently \\nanalyze the information presented. \\nIf you do not believe you meet these \\ncriteria, please disregard and contact \\nyour PWM Team.  \\nEntities Providing Services. \\nInvestment advisory and/or financial \\ncounseling services may be provided \\nby GS&Co., an affiliate, or an external \\nmanager under the wrap program \\nsponsored by GS&Co. Affiliates may \\ninclude but are not limited to The \\nAyco Company, L.P. (“Goldman Sachs \\nAyco”) (a wholly-owned subsidiary \\nof The Goldman Sachs Group, Inc. \\nor “GS Group”) or another affiliate. \\nBrokerage services are provided by \\nGS&Co. Banking and payment services \\n(including check-writing, ACH, direct \\ndebit, and margin loans) are provided or \\nfacilitated by GS&Co. Over-The-Counter \\n(“OTC”) derivatives, foreign exchange \\nforwards, and related financing are \\noffered by GS&Co. Trust services \\nare provided by The Goldman Sachs \\nTrust Company, N.A. or The Goldman \\nSachs Trust Company of Delaware. \\nDeposit products, mortgages, and bank \\nloans are offered by Goldman Sachs \\nBank USA, member Federal Deposit \\nInsurance Corporation (“FDIC”) and an \\nEqual Housing Lender. GS&Co. and its \\npresent and future affiliates may offer \\nand provide through the GS Family \\nOffice (“GSFO”) offering—or through \\na client referral to third parties—a \\nsuite of personal family office services \\n(“GSFO Services”) specifically designed \\nfor certain Wealth Management \\n(“WM”) clients of GS. As part of GSFO \\nServices, GSFO may discuss with you \\nvarious aspects of financial planning, \\nincluding but not necessarily limited to \\nthe potential income tax consequences \\nof your investments, estate planning, \\nphilanthropic endeavors, and certain \\nother activities that may affect your \\nincome tax, gift tax and estate tax.  \\nGSFO Services vary among clients, \\nare provided based on individual \\nclient needs and preferences, and \\nare generally limited to educational \\nconsultations that should not be \\nviewed as tax or legal advice. GSFO \\ndoes not provide investment advice, \\ninvestment management services, or \\nadvise on or offer the sale of insurance \\nproducts. GSFO Services are offered \\nin the United States through GS&Co. \\nbut may also be provided in part by \\nGoldman Sachs Ayco. Goldman Sachs \\nAyco may, separately and distinctly \\nfrom GSFO Services, provide tax and \\ninsurance advice in addition to personal \\nfamily office services (“Ayco Family \\nOffice Services”). We encourage you \\nto clearly establish your set of services \\nwith your advisory team. \\nInvestment Strategy Group (“ISG”).  \\nThe Investment Strategy Group, part \\nof the Asset & Wealth Management \\nbusiness (“AWM”) of GS, focuses on \\nasset allocation strategy formation \\nand market analysis for GS Wealth \\nManagement. Any information that \\nreferences ISG, including their model \\nportfolios, represents the views of \\nISG, is not financial research and is \\nnot a product of GS Global Investment \\nResearch (“GIR”) and may vary \\nsignificantly from views expressed by \\nindividual portfolio management teams \\nwithin AWM, or other groups at GS. \\nReferences to ISG Model Portfolios \\nare provided for illustrative purposes \\nonly. Your actual asset allocation may \\nlook significantly different based on \\nyour particular circumstances and risk \\ntolerance. Model portfolio performance \\nis not included and any performance \\nreferenced is to a 50/50 reference \\nbenchmark.\\nInvestment Risks and Information.  \\nGS&Co. offers a range of products that \\nyou should carefully consider for their \\nunique terms and risks prior to investing \\nto ensure they are appropriate for \\nyour individual circumstances.  Below \\nare descriptions of major risks for our \\nmore complex products; please review \\nthe offering documents and product \\nprospectuses for particular products, \\nas well as additional information about \\nthe nature and risks of these and other \\nproducts in GS&Co.’s ADV Part 2A \\nBrochure and PWM Relationship Guide. \\nInvesting involves the risk of loss.  \\nAlternative Investments (“AI”). AIs may \\ninvolve a substantial degree of risk, \\nincluding the risk of total loss of capital, \\nuse of leverage, lack of liquidity, and \\nvolatility of returns. Private equity, \\nprivate credit, private real estate, \\nhedge funds, and AI investments \\nstructured as private investment funds \\nare subject to less regulation than other \\ntypes of pooled vehicles. Review the \\nOffering Memorandum, Subscription \\nAgreement, and any other applicable \\noffering documents for risks, potential \\nconflicts of interest, terms and \\nconditions and other disclosures. \\nCommodities. The risk of loss in trading \\ncommodities can be substantial due, \\nbut not limited, to lack of liquidity, \\nvolatile political, market, and economic \\nconditions, and abrupt changes in price \\nwhich may result from unpredictable \\nfactors including weather, labor strikes, \\ninflation, foreign exchange rates, etc. \\nDue to the use of leverage, a small \\nmove against your position may result \\nin a loss that may be larger than your \\ninitial deposit.\\nCurrencies. Currency exchange rates \\ncan be extremely volatile, particularly \\nduring times of political or economic \\nuncertainty. There is a risk of loss when \\nan investor has exposure to foreign \\ncurrency or holds foreign currency \\ntraded investments.\\nDigital Assets/Cryptocurrency. \\nDigital assets regulation is still \\ndeveloping across all jurisdictions \\nand governments may in the future \\nrestrict the use and exchange of any \\nor all digital assets. Digital assets are \\ngenerally not backed nor supported by \\nany government or central bank, are not \\nFDIC insured and do not have the same \\nprotections that U.S. or other countries’ \\nbank deposits may have and are more \\nvolatile than traditional currencies. \\nTransacting in digital assets carries \\nthe risk of market manipulation and \\ncybersecurity failures such as the risk \\nof hacking, theft, programming bugs, \\nand accidental loss. Differing forms of \\ndigital assets may carry different risks.  \\nThe volatility and unpredictability of \\nthe price of digital assets may lead to \\nsignificant and immediate losses.\\nOver-the-Counter (“OTC”) Derivatives. \\nOTC derivatives are illiquid as there is \\nno public market. The price or valuation \\nof each OTC derivative transaction \\nis individually negotiated between \\nGS&Co. and each counterparty, and \\nGS&Co. does not represent or warrant \\nthat the prices for which it offers OTC \\nderivative transactions are the best \\nprices available. You may therefore \\nhave trouble establishing whether \\nthe price you have been offered for a \\nparticular OTC derivative transaction \\nis fair. OTC derivatives may trade \\nat a value that is different from the \\nlevel inferred from interest rates, \\ndividends, and the underlier due to \\nfactors including expectations of future \\nlevels of interest rates and dividends, \\nand the volatility of the underlier prior \\nto maturity. The market price of the \\nOTC derivative transaction may be \\ninfluenced by many unpredictable \\nfactors, including economic conditions, \\nGS creditworthiness, the value of any \\nunderliers, and certain actions taken \\nby GS. Because GS may be obligated \\nto make substantial payments to you \\nas a condition of an OTC derivative \\ntransaction, you must evaluate the \\ncredit risk of doing business with GS. \\nDepending on the type of transaction, \\nyour counterparty may be GS&Co. or \\nanother GS affiliate. Counterparties \\nmay be subject to different rules \\ndepending on whether they are a \\nregistered U.S. broker dealer. OTC \\nderivative transactions with GS \\naffiliates cannot be assigned or \\ntransferred without GS’s prior written \\nconsent. The provisions of an OTC \\nderivative transaction may allow for \\nearly termination and, in such cases, \\neither you or GS may be required \\nto make a potentially significant \\ntermination payment depending \\nupon whether the OTC derivative \\ntransaction is in-the-money at the time \\nof termination. You should carefully \\nreview the Master Agreement, \\nincluding any related schedules, credit \\nsupport documents, addenda, and \\nexhibits. You may be requested to post \\nmargin or collateral at levels consistent \\nwith the internal policies of GS to \\nsupport written OTC derivatives\\nEmerging Markets and Growth \\nMarkets. Emerging markets and \\ngrowth markets investments involve \\ncertain considerations, including \\npolitical and economic conditions, \\nthe potential difficulty of repatriating \\nfunds or enforcing contractual or other \\nlegal rights, and the small size of the \\nsecurities markets in such countries \\ncoupled with a low volume of trading, \\nresulting in potential lack of liquidity \\nand price volatility.\\nNon-US Securities. Non-US securities \\ninvestments are subject to differing \\nregulations, less public information, \\nless liquidity, and greater volatility in \\nthe countries of domicile of the security \\nissuers and/or the jurisdiction in which \\nthese securities are traded. In addition, \\ninvestors in securities such as ADRs/\\nGDRs, whose values are influenced by \\nforeign currencies, effectively assume \\ncurrency risk.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6d922ea4-5fd9-4002-af96-85afd5537062', embedding=None, metadata={'page_label': '112', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Options. The purchase of options can \\nresult in the loss of an entire investment \\nand the risk of uncovered options is \\npotentially unlimited. You must read \\nand understand the current Options \\nDisclosure Document before entering \\ninto any options transactions. The \\nbooklet entitled Characteristics and \\nRisk of Standardized Options can be \\nobtained from your PWM team or at \\nhttp://www.theocc.com/components/\\ndocs/riskstoc.pdf. A secondary market \\nmay not be available for all options. \\nTransaction costs may be significant in \\noption strategies that require multiple \\npurchases and sales of options, such as \\nspreads. Supporting documentation for \\nany comparisons, recommendations, \\nstatistics, technical data, or other \\ninformation will be supplied upon \\nrequest\\nReal Estate. Real estate investments, \\nincluding real estate investments \\ntrusts (“REITS”) and non-traded REITS, \\ninvolve additional risks not typically \\nassociated with other asset classes. \\nSuch investments (both through public \\nand private markets) may be subject \\nto changes in broader macroeconomic \\nconditions, such as interest rates, and \\nsensitivities to temporary or permanent \\nreductions in property values for the \\ngeographic region(s) represented. Non-\\ntraded REITS may carry a higher risk of \\nilliquidity, incomplete or nontransparent \\nvaluations, dilution of shares, and \\nconflicts of interest.\\nStructured Investments. Structured \\ninvestments are complex and investors \\nassume the credit risk of the issuer or \\nguarantor. If the issuer or guarantor \\ndefaults, you may lose your entire \\ninvestment, even if you hold the product \\nto maturity. Structured investments \\noften perform differently from the \\nasset(s) they reference. Credit ratings \\nmay pertain to the credit rating of \\nthe issuer and are not indicative of \\nthe market risk associated with the \\nstructured investment or the reference \\nasset. Each structured investment is \\ndifferent, and for each investment you \\nshould consider 1) the possibility that \\nat expiration you may be forced to own \\nthe reference asset at a depressed \\nprice; 2) limits on the ability to share in \\nupside appreciation; 3) the potential for \\nincreased losses if the reference asset \\ndeclines; and 4) potential inability to sell \\ngiven the lack of a public trading market.\\nTactical Tilts. Tactical tilts may involve \\na high degree of risk. No assurance can \\nbe made that profits will be achieved \\nor that substantial losses will not be \\nincurred. For various reasons, GS may \\nimplement a tactical tilt, invest in an \\naffiliated fund that may invest in tactical \\ntilts, or unwind a position for its client \\nadvisory accounts or on its own behalf \\nbefore your advisor does on behalf \\nof your account, or may implement a \\ntactical tilt that is different from the \\ntactical tilt implemented by advisors \\non client accounts, which could have \\nan adverse effect on your account and \\nmay result in poorer performance by \\nyour account than by GS or other client \\naccounts.\\nU.S. Registered Mutual Funds / Exchange \\nTraded Funds (“ETFs”) or Exchange Traded \\nNotes (“ETNs”). You should consider a \\nfund’s investment objectives, risks, and \\ncosts, and read the summary prospectus \\nand/or the Prospectus (which may \\nbe obtained from your PWM Team) \\ncarefully before investing. You may \\nobtain documents for ETFs or ETNs for \\nfree by 1) visiting EDGAR on the SEC \\nwebsite at http://www.sec.gov/; 2) \\ncontacting your PWM Team; or 3) calling \\ntoll-free at 1-866-471-2526. Unlike \\ntraditional mutual funds, ETFs can trade \\nat a discount or premium to the net asset \\nvalue and are not directly redeemable \\nby the fund. Leveraged or inverse ETFs, \\nETNs, or commodities futures-linked \\nETFs may experience greater price \\nmovements than traditional ETFs and \\nmay not be appropriate for all investors. \\nMost leveraged and inverse ETFs or \\nETNs seek to deliver multiples of the \\nperformance (or the inverse of the \\nperformance) of the underlying index \\nor benchmark on a daily basis. Their \\nperformance over a longer period of \\ntime can vary significantly from the \\nstated daily performance objectives \\nor the underlying benchmark or index \\ndue to the effects of compounding. \\nPerformance differences may be \\nmagnified in a volatile market. \\nCommodities futures-linked ETFs may \\nperform differently than the spot price \\nfor the commodity itself, including due \\nto the entering into and liquidating \\nof futures or swap contracts on a \\ncontinuous basis to maintain exposure \\n(i.e., “rolling”) and disparities between \\nnear term future prices and long \\nterm future prices for the underlying \\ncommodity. You should not assume that \\na commodity-futures linked ETF will \\nprovide an effective hedge against other \\nrisks in your portfolio.\\nSecurity-Specific References. \\nReferences to a specific company or \\nsecurity are intended solely as examples \\nor for context and are not research or \\ninvestment advice; do not rely upon them \\nin making an investment decision. GS may \\nhave a relationship with such companies \\nand/or its securities that may present \\nconflicts of interest. Contact your PWM \\nTeam for further information on any \\nsecurities mentioned. \\nOff-Platform Investments. If you ask \\nus for guidance on external investment \\nopportunities not offered by GS, any \\ninformation we may provide is as an \\naccommodation only and we will not \\nbe acting as your advisor. We assume \\nno obligation to determine whether \\nthe opportunity is suitable for you \\nin connection with such investment \\ndecisions and will not assume any liability \\nfor such investment decisions. Our Form \\nADV has information on conflicts of \\ninterest we may have in connection with \\nany such requests.\\nISG/GIR Forecasts. Economic and \\nmarket forecasts presented (“forecasts”) \\ngenerally reflect either ISG’s either ISG’s \\nviews or, where indicated, Goldman \\nSachs Global Investment Research’s \\n(“GIR”) views and are subject to change \\nwithout notice. Forecasts do not consider \\ninvestment objectives, restrictions, tax \\nand financial situations or other needs \\nof any specific client. Forecasts are \\npresented for educational purposes and \\nare subject to high levels of uncertainty \\nthat may affect actual performance and \\nrepresents only one of a broad range of \\npossible outcomes. Forecasts and any \\nreturn expectations are as of the date of \\nthis material, and do not project returns \\nof any given investment or strategy. \\nForecasts are estimated based on capital \\nmarket assumptions using historical \\nanalysis of applicable underlying relevant \\nindices taking into consideration variables \\nthat may impact the sub-asset class \\nincluding but not limited to geopolitical \\nfactors, potential for recession, and/or \\nrevenue growth. Estimates are subject \\nto significant revision and may change \\nmaterially as economic and market \\nconditions change. Any case studies and \\nexamples are for illustrative purposes \\nonly. If applicable, a copy of the GIR \\nReport used for GIR forecasts is available \\nupon request. Forecasts do not reflect \\nadvisory fees, transaction costs, and \\nother expenses a client would have paid, \\nwhich would reduce return.\\nClient Specific Markets. Investments \\nheld in your name with a subcustodian \\nin the local market where traded in \\norder to comply with local law will be \\nindicated on your statements.\\nPerformance / Estimated Income \\n/ Estimated Cash Flow. Past \\nperformance is not a guide of future \\nresults and may include investments \\nno longer owned in current or closed \\naccounts. Current performance may be \\nlower or higher than the performance \\ndata quoted. Where not relevant \\nor representative, outliers may be \\nexcluded. To request the most current \\nor historical performance data, or asset \\nclassification schema information, \\nplease contact your PWM team at \\nthe number provided on your monthly \\nstatement or toll-free in the U.S. at \\n1-800-323-5678. \\nPerformance reports, where shown, \\ngenerally present the relevant \\ntime weighted performance, which \\nis a combination of daily returns \\ncompounded over a specified time \\nperiod with the removal of the deposit \\nand withdrawal impacts, and may show \\ninternal rate of return calculations \\nwhere requested. Aggregate \\nperformance may not equal the sum of \\nreturns at an investment level. Where \\nperformance is shown net of fees, \\nactual fees may differ. Net performance \\nfor advisory accounts is calculated net \\nof fees and expenses that were or would \\nhave been paid in connection with GS’s \\nservices, including management fees, \\nand might include investments for which \\nactual market prices are not currently \\navailable. If included, estimated income \\nfigures and estimated private equity \\nfuture cash flows are estimates of \\nfuture activity, and actual results may \\nvary substantially. GS&Co. has adjusted \\nperformance calculations for certain \\nasset classes or strategies and may do \\nso in the future. Performance of net cash \\n(i.e., cash less margin debit) is generally \\nincluded in the total performance \\ncalculation but not displayed separately. \\nOption performance is included in the \\nperformance of the asset class of the \\nunderlier \\nIndices/Benchmarks. References to \\nindices, benchmarks, or other measures \\nof relative market performance over a \\nspecified period are informational only \\nand are not predictions or guarantees of \\nperformance. comparative or Reference \\nbenchmark returns may reflect different \\nperiods. \\nIndices are unmanaged and investors \\ncannot directly invest in them. The \\nfigures for the index reflect the \\nreinvestment of all income or dividends, \\nas applicable, but may not always reflect \\nthe deduction of any fees or expenses \\nwhich would reduce returns. Where \\nappropriate, relevant index trademarks \\nor index information has been licensed \\nor sub-licensed for use. Inclusion of \\nindex information does not mean the \\nrelevant index or its affiliated entities \\nsponsor, endorse, sell, or promote the \\nreferenced securities, or that they \\nmake any representation or warranty \\nregarding either the advisability of \\ninvesting in securities or the ability of \\nthe index to track market performance. \\nIndices are unmanaged and investors \\ncannot directly invest in them. The \\nfigures for the index reflect the \\nreinvestment of all income or dividends, \\nas applicable, but may not always reflect \\nthe deduction of any fees or expenses \\nwhich would reduce returns. Where \\nappropriate, relevant index trademarks \\nor index information has been licensed \\nor sub-licensed for use. Inclusion of \\nindex information does not mean the \\nrelevant index or its affiliated entities \\nsponsor, endorse, sell, or promote the \\nreferenced securities, or that they \\nmake any representation or warranty \\nregarding either the advisability of \\ninvesting in securities or the ability of \\nthe index to track market performance. \\nPricing and Valuations. Prices \\ndo not necessarily reflect realizable \\nvalues and are based on information \\nconsidered to be reliable but are not \\nguaranteed for accuracy, currency, or \\nas realizable values. Certain positions \\nmay be provided by third parties or may \\nappear without a price if GS is unable to \\nobtain a price and/or the security is not \\nactively traded for a certain amount of \\ntime. Pricing sources and methods are \\navailable upon request and are subject \\nto change\\nTax Information. GS does not provide \\nlegal, tax or accounting advice, unless \\nexplicitly agreed in writing between you \\nand GS, and does not offer the sale of \\ninsurance products. You should obtain \\nyour own independent tax advice based \\non your circumstances. The information \\nincluded in this presentation, including, \\nif shown, in the Tax Summary section, \\ndoes not constitute tax advice, has not \\nbeen audited, should not be used for \\ntax reporting, and is not a substitute for \\nthe applicable tax documents, including \\nyour Form 1099, Schedule K-1 for private ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='717f34b3-32ec-4dcf-afb0-af292ee13d3d', embedding=None, metadata={'page_label': '113', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='investments, which we will provide \\nto you annually, or your monthly GS \\naccount statement(s). The cost basis \\nincluded in this presentation may differ \\nfrom your cost basis for tax purposes. \\nInformation regarding your AIs and \\ntransactions for retirement accounts are \\nnot included in the Tax Summary section \\nNotice to ERISA / Qualified \\nRetirement Plan / IRA / Coverdell \\nEducation Savings Account \\n(collectively, “Retirement \\nAccount”) Clients : Information \\nregarding your Retirement Account(s) \\nincluded in this presentation is for \\ninformational purposes only and does \\nnot constitute investment or other advice \\nor a recommendation relating to any \\ninvestment or other decisions, and GS is \\nnot a fiduciary or advisor with respect to \\nany person or plan by reason of providing \\nthe presentation including under the \\nEmployee Retirement Income Security \\nAct of 1974 or Department of Labor \\nRegulations. Unless GS agrees otherwise, \\nany target allocation shown for such \\nRetirement Account represents decisions \\nyou have communicated to GS regarding \\nsuch asset allocation, without any advice \\nor recommendations from GS, after \\nconsidering your financial circumstances, \\nobjectives, risk tolerance and goals.\\nOther Services.  Any provided financial \\nplanning services, including cash flow \\nanalyses based on information you \\nprovide, are hypothetical illustrations of \\nmathematical principles and are not a \\nprediction or projection of performance \\nof an investment or investment strategy. \\nCertain illustrations may be predicated \\non an Investment Analysis tool, an \\ninteractive technological tool that \\nproduces simulations and statistical \\nanalyses that present the likelihood of \\nvarious investment outcomes based \\non client input. Such services may not \\naddress every aspect of a client’s financial \\nlife; topics that were not discussed with \\nyou may still be relevant to your financial \\nsituation. In providing financial services, \\nGS relies on information provided by you \\nand is not responsible for the accuracy or \\ncompleteness of any such information, nor \\nfor any consequences related to the use of \\nany inaccurate or incomplete information. \\nWhere materials and/or analyses are \\nprovided to you, they are based on the \\nassumptions stated therein, which are \\nlikely to vary substantially from the \\nexamples shown if they do not prove to be \\ntrue. These examples are for illustrative \\npurposes only and do not guarantee that \\nany client will or is likely to achieve the \\nresults shown. Assumed growth rates are \\nsubject to high levels of uncertainty and \\ndo not represent actual trading and may \\nnot reflect material economic and market \\nfactors that may have an impact on actual \\nperformance. GS has no obligation to \\nprovide updates to these rates.\\nNot a Municipal Advisor. Except \\nwhere GS expressly agrees otherwise, \\nGS is not acting as a municipal advisor \\nand the opinions or views contained in \\nthis presentation are not intended to be, \\nand do not constitute, advice, including \\nwithin the meaning of Section 15B of the \\nSecurities Exchange Act of 1934.\\nAdditional Information for Ayco \\nClients: . Your GS team may include \\nindividuals from your Goldman Sachs \\nAyco team. Goldman Sachs Ayco may \\nprovide tax advice or other Ayco Family \\nOffice Services to certain clients. \\nGoldman Sachs Ayco does not provide \\nbrokerage services. As part of its financial \\ncounseling services, Goldman Sachs Ayco \\nmay provide you with certain reports \\nwhere similar information contained \\nherein is presented differently. You should \\nview each report independently and raise \\nany questions with your Goldman Sachs \\nAyco team. \\nNo Distribution; No Offer or \\nSolicitation. This material may not, \\nwithout GS’ prior written consent, \\nbe (i) duplicated by any means, or (ii) \\ndistributed to any person that is not an \\nemployee, officer, director, or authorized \\nagent of the recipient. This material is \\nnot an offer or solicitation with respect \\nto the purchase or sale of any security \\nin any jurisdiction in which such offer or \\nsolicitation is not authorized, or to any \\nperson to whom it would be unlawful to \\nmake such offer or solicitation. We have \\nno obligation to provide any updates or \\nchanges to this material\\nArgentina: The information has been \\nprovided at your request. \\nAustralia:  This material is being \\ndisseminated in Australia by Goldman \\nSachs & Co (“GSCo”); Goldman Sachs \\nInternational (“GSI”); Goldman Sachs \\n(Singapore) Pte (“GSSP”) and/or \\nGoldman Sachs (Asia) LLC (“GSALLC”). \\nIn Australia, this document, and any \\naccess to it, is intended only for a \\nperson that has first satisfied Goldman \\nSachs that: \\n•  The person is a Sophisticated or \\nProfessional Investor for the purposes \\nof section 708 of the Corporations Act \\n2001 (Cth) (“Corporations Act”); or\\n•  The person is a wholesale client for \\nthe purposes of section 761G of the \\nCorporations Act \\nNo offer to acquire any financial product \\nor interest in any securities or interests \\nof any kind is being made to you in \\nthis document. If financial products or \\ninterests in any securities or interests \\nof any kind do become available in the \\nfuture, the offer may be arranged by an \\nappropriately licensed Goldman Sachs \\nentity in Australia in accordance with \\nsection 911A(2) (b) of the Corporations \\nAct. Any offer will only be made in \\ncircumstances where disclosures and/or \\ndisclosure statements are not required \\nunder Part 6D.2 or Part 7.9 of the \\nCorporations Act (as relevant). \\nTo the extent that any financial service \\nis provided in Australia by GSCo, GSI, \\nGSSP and/or GSALLC, those services \\nare provided on the basis that they \\nare provided only to “wholesale \\nclients”, as defined for the purposes \\nof the Corporations Act. GSCo, GSI, \\nGSSP and GSALLC are exempt from \\nthe requirement to hold an Australian \\nFinancial Services Licence under the \\nCorporations Act and do not therefore \\nhold an Australian Financial Services \\nLicence. GSCo is regulated by the \\nSecurities and Exchange Commission \\nunder US laws; GSI is regulated by the \\nFinancial Conduct Authority and the \\nPrudential Regulation Authority under \\nlaws in the United Kingdom; GSSP is \\nregulated by the Monetary Authority of \\nSingapore under Singaporean laws; and \\nGSALLC is regulated by the Securities \\nand Futures Commission under Hong \\nKong laws; all of which differ from \\nAustralian laws. Any financial services \\ngiven to any person by GSCo, GSI, \\nand/or GSSP in Australia are provided \\npursuant to ASIC Class Orders 03/1100; \\n03/1099; and 03/1102 respectively.\\nBahrain:  : GSI represents and warrants \\nthat it has not made and will not make \\nany invitation to the public in the \\nKingdom of Bahrain to subscribe for \\nthe fund. This presentation has not \\nbeen reviewed by the Central Bank of \\nBahrain (CBB) and the CBB takes no \\nresponsibility for the accuracy of the \\nstatements or the information contained \\nherein, or for the performance of the \\nsecurities or related investment, nor \\nshall the CBB have any liability to any \\nperson for damage or loss resulting from \\nreliance on any statement or information \\ncontained herein. This presentation \\nwill not be issued, passed to, or made \\navailable to the public generally. \\nBrazil. These materials are provided \\nat your request and solely for your \\ninformation, and in no way constitutes \\nan offer, solicitation, advertisement \\nor advice of, or in relation to, any \\nsecurities, funds, or products by any \\nof Goldman Sachs affiliates in Brazil \\nor in any jurisdiction in which such \\nactivity is unlawful or unauthorized, or \\nto any person to whom it is unlawful \\nor unauthorized. This document has \\nnot been delivered for registration to \\nthe relevant regulators or financial \\nsupervisory bodies in Brazil, such as \\nthe Brazilian Securities and Exchange \\nCommission (Comissão de Valores \\nMobiliários – CVM) nor has its \\ncontent been reviewed or approved \\nby any such regulators or financial \\nsupervisory bodies. The securities, \\nfunds, or products described in this \\ndocument have not been registered \\nwith the relevant regulators or financial \\nsupervisory bodies in Brazil, such as \\nthe CVM, nor have been submitted \\nfor approval by any such regulators \\nor financial supervisory bodies. The \\nrecipient undertakes to keep these \\nmaterials as well as the information \\ncontained herein as confidential and not \\nto circulate them to any third party. \\nChile: Fecha de inicio de la oferta: \\n(i) La presente oferta se acoge a la \\nNorma de Carácter General N° 336 \\nde la Superintendencia de Valores \\ny Seguros de Chile; (ii) La presente \\noferta versa sobre valores no inscritos \\nen el Registro de Valores o en el \\nRegistro de Valores Extranjeros que \\nlleva la Superintendencia de Valores y \\nSeguros, por lo que los valores sobre \\nlos cuales ésta versa, no están sujetos \\na su fiscalización; (iii) Que por tratarse \\nde valores no inscritos, no existe la \\nobligación por parte del emisor de \\nentregar en Chile información pública \\nrespecto de estos valores; y (iv) Estos \\nvalores no podrán ser objeto de oferta \\npública mientras no sean inscritos en el \\nRegistro de Valores correspondiente. \\nDubai: Goldman Sachs International \\n(“GSI”) is authorised and regulated by \\nthe Dubai Financial Services Authority \\n(“DFSA”) in the DIFC and the Financial \\nServices Authority (“FSA”) authorised \\nby the Prudential Regulation Authority \\nand regulated by the Financial Conduct \\nAuthority and Prudential Regulation \\nAuthority in the UK. Registered address \\nof the DIFC branch is Level 5, Gate \\nPrecinct Building 1, Dubai International \\nFinancial Centre, PO Box 506588, Dubai, \\nUAE and registered office of GSI in the \\nUK is Plumtree Court, 25 Shoe Lane, \\nLondon, EC4A 4AU, United Kingdom. \\nThis material is only intended for use by \\nmarket counterparties and professional \\nclients, and not retail clients, as defined \\nby the DFSA Rulebook. Any products \\nthat are referred to in this material will \\nonly be made available to those clients \\nfalling within the definition of market \\ncounterparties and professional clients. \\nIsrael:  Goldman Sachs is not licensed to \\nprovide investment advice or investment \\nmanagement services under Israeli law. \\nKorea:  No Goldman Sachs entity, \\nother than Goldman Sachs (Asia) L.L.C, \\nGoldman Sachs Asset Management \\nInternational and Goldman Sachs Asset \\nManagement Korea Co., Ltd., is currently \\nlicensed to provide discretionary \\ninvestment management services and \\nadvisory services to clients in Korea \\nand nothing in this material should \\nbe construed as an offer to provide \\nsuch services except as otherwise \\npermitted under relevant laws and \\nregulations. Goldman Sachs (Asia) \\nL.L.C. is registered as a Cross-Border \\nDiscretionary Investment Management \\nCompany and a Cross-Border Investment \\nAdvisory Company with the Korean \\nFinancial Supervisory Commission, and \\nas a licensed corporation for, amongst \\nother regulated activities, advising on \\nsecurities and asset management with \\nthe Hong Kong Securities & Futures \\nCommission. Goldman Sachs Asset \\nManagement International is licensed \\nas a Cross-Border Discretionary \\nInvestment Management Company and \\na CrossBorder Investment Advisory \\nCompany with the Korean Financial \\nSupervisory Commission, as an \\ninvestment adviser with the Securities \\nand Exchange Commission of the United \\nStates and for Managing Investments \\nwith the Financial Services Authority \\nof the United Kingdom. Goldman Sachs \\nAsset Management Korea Co., Ltd. \\nis licensed as an Asset Management \\nCompany in Korea and is also registered \\nas an Investment Advisory Company and \\nDiscretionary Investment Management \\nCompany with the Korean Financial \\nSupervisory Commission. Details of \\ntheir respective officers and major \\nshareholders can be provided upon \\nrequest. \\nOman: The information contained in \\nthese materials neither constitutes \\na public offer of securities in the ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a2715503-295f-4b92-bcc8-d84664c79586', embedding=None, metadata={'page_label': '114', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Sultanate of Oman as contemplated \\nby the Commercial Companies Law \\nof Oman (Sultani Decree 4/74) or the \\nCapital Market Law of Oman (Sultani \\nDecree 80/98) nor does it constitute an \\noffer to sell, or the solicitation of any \\noffer to buy Non-Omani securities in the \\nSultanate of Oman as contemplated by \\nArticle 6 of the Executive Regulations \\nto the Capital Market Law (issued \\nvide Ministerial Decision No. 4/2001). \\nAdditionally, these materials are not \\nintended to lead to the conclusion of any \\ncontract of whatsoever nature within \\nthe territory of the Sultanate of Oman. \\nPanama:  These Securities have not \\nbeen and will not be registered with the \\nnational Securities Commission of the \\nRepublic of Panama under Decree Law \\nNo. 1 of July 8, 1999 (the “Panamanian \\nSecurities Act”) and may not be \\noffered or sold within Panama except \\nin certain limited transactions exempt \\nfrom the registration requirements of \\nthe Panamanian Securities Act. These \\nSecurities do not benefit from the tax \\nincentives provided by the Panamanian \\nSecurities Act and are not subject to \\nregulation or supervision by the National \\nSecurities Commission of the Republic of \\nPanama. This material constitutes generic \\ninformation regarding Goldman Sachs and \\nthe products and services that it provides \\nand should not be construed as an offer \\nor provision of any specific services or \\nproducts of Goldman Sachs for which a \\nprior authorization or license is required \\nby Panamanian regulators. \\nPeru: The products or securities \\nreferred to herein have not been \\nregistered before the Superintendencia \\ndel Mercado de Valores (SMV) and are \\nbeing placed by means of a private offer. \\nSMV has not reviewed the information \\nprovided to the investor. \\nQatar: The investments described in \\nthis document have not been, and will \\nnot be, offered, sold or delivered, at any \\ntime, directly or indirectly in the State of \\nQatar in a manner that would constitute \\na public offering. This document has not \\nbeen, and will not be, registered with \\nor reviewed or approved by the Qatar \\nFinancial Markets Authority, the Qatar \\nFinancial Centre Regulatory Authority \\nor Qatar Central Bank and may not be \\npublicly distributed. This document is \\nintended for the original recipient only \\nand must not be provided to any other \\nperson. It is not for general circulation \\nin the State of Qatar and may not \\nbe reproduced or used for any other \\npurpose. \\nSingapore: This document has not been \\ndelivered for registration to the relevant \\nregulators or financial supervisory bodies \\nin Hong Kong or Singapore, nor has its \\ncontent been reviewed or approved \\nby any financial supervisory body or \\nregulatory authority. The information \\ncontained in this document is provided at \\nyour request and for your information only. \\nIt does not constitute an offer or invitation \\nto subscribe for securities or interests of \\nany kind. Accordingly, unless permitted \\nby the securities laws of Hong Kong or \\nSingapore, (i) no person may issue or \\ncause to be issued this document, directly \\nor indirectly, other than to persons who \\nare professional investors, institutional \\ninvestors, accredited investors or other \\napproved recipients under the relevant \\nlaws or regulations (ii) no person may \\nissue or have in its possession for the \\npurposes of issue, this document, or any \\nadvertisement, invitation or document \\nrelating to it, whether in Hong Kong, \\nSingapore or elsewhere, which is directed \\nat, or the contents of which are likely to \\nbe accessed by, the public in Hong Kong \\nor Singapore and (iii) the placement of \\nsecurities or interests to the public in \\nHong Kong and Singapore is prohibited. \\nBefore investing in securities or interests \\nof any kind, you should consider whether \\nthe products are suitable for you \\nSouth Africa: Goldman Sachs does \\nnot provide tax, accounting, investment \\nor legal advice to our clients, and all \\nclients are advised to consult with their \\nown advisers regarding any potential \\ninvestment/transaction. This material is \\nfor discussion purposes only, and does \\nnot purport to contain a comprehensive \\nanalysis of the risk/ rewards of any \\nidea or strategy herein. Any potential \\ninvestment/transaction described \\nwithin is subject to change and Goldman \\nSachs Internal approvals. Goldman \\nSachs International is an authorised \\nfinancial services provider in South \\nAfrica under the Financial Advisory and \\nIntermediary Services Act, 2002. \\nUkraine:  Goldman Sachs & Co. LLC is \\nnot registered in Ukraine and carries out \\nits activity and provides services to its \\nclients on a purely cross-border basis \\nand has not established any permanent \\nestablishment under Ukrainian law. The \\ninformation contained in this document \\nshall not be treated as an advertisement \\nunder Ukrainian law. \\nUnited Arab Emirates: The \\ninformation contained in this document \\ndoes not constitute, and is not \\nintended to constitute, a public offer of \\nsecurities in the United Arab Emirates \\nin accordance with the Commercial \\nCompanies Law (Federal Law No. 8 of \\n1984, as amended) or otherwise under \\nthe laws of the United Arab Emirates. \\nThis document has not been approved \\nby, or filed with the Central Bank of the \\nUnited Arab Emirates or the Securities \\nand Commodities Authority. If you do \\nnot understand the contents of this \\ndocument, you should consult with \\na financial advisor. This document is \\nprovided to the recipient only and should \\nnot be provided to or relied on by any \\nother person. \\nUnited Kingdom:  This material has \\nbeen approved for issue in the United \\nKingdom solely for the purposes of \\nSection 21 of the Financial Services \\nand Markets Act 2000 by GSI, Plumtree \\nCourt, 25 Shoe Lane, London, EC4A \\n4AU, United Kingdom. Authorised by \\nthe Prudential Regulation Authority \\nand regulated by the Financial Conduct \\nAuthority and the Prudential Regulation \\nAuthority. \\n© 2025 Goldman Sachs. All rights reserved.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='aa9e9eec-3557-4f7c-977f-9b2811099b1d', embedding=None, metadata={'page_label': '115', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Additional contributors from the \\nInvestment Strategy Group include:\\nAnais Boussie  \\nVice President\\nCharles Deng  \\nVice President\\nRick Harrell \\nVice President\\nCarol Jiang  \\nVice President\\nHoward Spector  \\nVice President\\nOlivia Xia \\nVice President\\nNathan Chen  \\nAssociate\\nNeerja Talreja \\nAssociate\\nMayra Vialette  \\nAssociate', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d84bb6ba-e9e8-4bfb-9ae9-988b763e59e0', embedding=None, metadata={'page_label': '116', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Goldman Sachs\\nAmsterdam\\nAtlanta\\nAustin\\nBeijing\\nBoston\\nChicago\\nDallas\\nDenver\\nDetroit\\nDubai\\nDublin\\nFrankfurt\\nGeneva\\nHong Kong\\nHouston\\nLondon\\nLos Angeles\\nLuxembourg\\nMadrid\\nMelbourne\\nMiami\\nMilan\\nMonaco\\nNashville / Brentwood\\nNew York\\nParis\\nPhiladelphia\\nSan Francisco\\nSeattle\\nShanghai\\nShenzhen\\nSingapore\\nStockholm\\nSydney\\nTel Aviv\\nWashington, D.C.\\nWest Palm Beach\\nZurich\\nwww.gs.com', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[10] # see page 10"
      ],
      "metadata": {
        "id": "W1VHK7ksauVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b57b27c-6b8f-4d02-ac37-3772adf00e57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='956e8a91-4464-4f1d-9e66-1f5e0814776b', embedding=None, metadata={'page_label': '9', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9Outlook Investment Strategy Group\\n SECTION II: DIVERGING PATHS\\n58  2025 Global  \\nEconomic Outlook\\n  Despite divergent paths, most economies will \\nascend toward higher elevations this year.  \\n60  United States\\n64  Eurozone\\n66  United Kingdom\\n67 Japan\\n68  Emerging Markets\\n SECTION III: EXTENDING THE SEASON\\n74  2025 Financial \\nMarkets Outlook\\n  Ongoing economic growth will lift markets this \\nyear, albeit with a few icy patches along the way.\\n76  US Equities\\n82  Non-US Developed Market Equities\\n83  Eurozone Equities\\n84 UK Equities\\n84  Japanese Equities\\n85  Emerging Market Equities\\n86  Global Currencies\\n91  Global Fixed Income\\n104 Global Commodities', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "pp_mZfXrbF_R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine() # this will convert query and document to numerical representation and pass it to model"
      ],
      "metadata": {
        "id": "4AgWRP4PbKBa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the outlook for 2025 for US GDP?\")"
      ],
      "metadata": {
        "id": "puFqfflTbtmh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1_gQiupb6Dh",
        "outputId": "2e765cea-681e-4f3f-d8cc-165aec70468c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The outlook for 2025 for US GDP is expected to show solid growth, with real GDP expanding by 2.3%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the recipe of apple pie?\")"
      ],
      "metadata": {
        "id": "KjuspUJNeBme"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUO7tLrneKou",
        "outputId": "087b7531-1069-4361-8482-ccffa6b3c26a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm unable to provide the recipe for apple pie as it is not mentioned in the provided context information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "eXURoX5eeRT1",
        "outputId": "08a8f248-77b6-4345-e296-4172275dad49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response=\"I'm unable to provide the recipe for apple pie as it is not mentioned in the provided context information.\", source_nodes=[NodeWithScore(node=TextNode(id_='e394c0b4-2741-42ec-8aaa-c739797ef8a1', embedding=None, metadata={'page_label': '6', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f556fa0c-b5e7-439c-9d70-226af985f683', node_type='4', metadata={'page_label': '6', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, hash='ff711678520d73b47fcca81c7d4b34acf78e2f32753a4120cf92bb432c32d5f2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='6 Goldman Sachs january 2025\\n Importantly, we argue that the recent barrage of commentary from the media \\nciting the end of American exceptionalism is misplaced. We believe the incoming \\nTrump administration will not derail US preeminence—the system of checks and \\nbalances is alive and well. \\n We also explain why the record cheapness of most non-US equity markets does \\nnot warrant a tactical shift away from US equities. \\n Next, we put forth our one- and five-year expected returns, which underpin \\nour view not to underweight US equities in favor of bonds or cash. We also \\nreview our opportunistic tactical tilts going into 2025.\\n Along the way, we dispel several key myths that have become common lore. \\nWe show that:\\n• There is no evidence of mean reversion in equity valuations.\\n• Valuations are not an effective timing signal to exit the markets.\\n• Equity concentration is not an effective timing signal for forward returns.\\nMyth Busters', mimetype='text/plain', start_char_idx=0, end_char_idx=946, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.712080603516999), NodeWithScore(node=TextNode(id_='b2323d7a-225e-49af-9192-1237aea9e183', embedding=None, metadata={'page_label': '16', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='481e6084-94eb-4fb6-aab1-a74e89811575', node_type='4', metadata={'page_label': '16', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, hash='ff3caf97ed5ad48e808333ead48b5658b126aabab9e87a0ea39c35d1a45730fc')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='16 Goldman Sachs january 2025\\n1. Number of patents. Japan and the US are world \\nleaders, with 16,000 and 14,000 triadic patent \\nfamilies, respectively, registered as of 2021. A \\ntriadic patent family is a set of patents filed at \\nthree major patent offices: the US Patent and \\nTrademark Office, the European Patent Office \\nand the Japan Patent Office (see Exhibit 17). \\n2. The Modern Innovation System Composite \\nIndex. Designed by the Atlantic Council \\nGeoEconomics Center and Rhodium Group’s \\nChina Pathfinder, this metric is comprehensive \\nand captures national R&D spending as a \\nshare of GDP, venture capital attractiveness, \\nprivate-sector versus state-funded innovation, \\ntriadic patent families filed, international \\nattractiveness of a nation’s intellectual \\nproperty, and strength of the intellectual \\nproperty regime. According to this metric, \\nthe US is ranked third, after South Korea \\nand Japan. China stands at 2.5, which is well \\nExhibit 15: Annual Hours Worked per Worker\\nWorkers in the US work 34% more than workers in Germany \\nand 20% more than French workers.\\n2.4 2.3\\n2.0 1.9 1.8\\n1.6\\n1.5 1.5 1.5\\n1.3\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\nIndia China Taiwan Korea US Japan Eurozone UK France Germany\\nThousands\\nData as of 2024. \\nSource: Investment Strategy Group, Total Economy Database™, Conference Board. \\n \\nExhibit 17: Triadic Patent Families Registered \\nJapan and the US have the most triadic patent families \\nregistered.\\n16,102\\n14,341\\n6,106\\n4,364\\n1,991 1,868\\n0\\n2,000\\n4,000\\n6,000\\n8,000\\n10,000\\n12,000\\n14,000\\n16,000\\n18,000\\nJapan US China Germany France UK\\nTriadic Patent Families Registered\\nData as of 2021. \\nNote: Triadic patent families are a set of patents filed at the European Patent Office, the United \\nStates Patent and Trademark Office, and the Japan Patent Office. \\nSource: Investment Strategy Group, OECD.\\nExhibit 16: Top 10 Countries Ranked by \\nR&D Spending\\nThe US spends the most on R&D globally.\\n819\\n434\\n166 137\\n92 90 66 34 32 17\\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n900\\nUS China Japan Germany UK Korea France Canada Italy India\\nUS$ Billions\\nData as of 2021. \\nNote: India’s R&D spending is as of 2020. \\nSource: Investment Strategy Group, Haver Analytics. \\nExhibit 18: Modern Innovation Composite Index\\nThe US is ranked third, after South Korea and Japan, while \\nChina ranks well below the average open economy. \\n6.5\\n5.5\\n4.8 4.8 4.6\\n4.0\\n3.5\\n2.5\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n Japan Korea  US  UK  Germany Avg. Open\\nEconomy\\n France  China\\nModern Innovation System Composite Index\\nData as of 2023. \\nSource: Investment Strategy Group, China Pathfinder, Atlantic Council, Rhodium Group.', mimetype='text/plain', start_char_idx=0, end_char_idx=2587, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7119872270921039)], metadata={'e394c0b4-2741-42ec-8aaa-c739797ef8a1': {'page_label': '6', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}, 'b2323d7a-225e-49af-9192-1237aea9e183': {'page_label': '16', 'file_name': '2025-isg-outlook.pdf', 'file_path': '/content/2025-isg-outlook.pdf', 'file_type': 'application/pdf', 'file_size': 7209986, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-19'}})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Memory"
      ],
      "metadata": {
        "id": "tiaR0dMuq_0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the outlook for developping markets?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ifZ88JrC_-",
        "outputId": "fb332913-a44b-4306-f172-abf4d73ad784"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Developing markets are expected to have mid-single-digit returns, with the volatility in this market being the greatest among all markets. There is a 55% probability assigned to the base case scenario, a 25% probability to the upside scenario, and a 20% probability to the downside scenario for emerging market equities. On a probability-weighted basis, emerging market equities have the second-lowest expected return after the UK.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Models don't have memory, so if we want a chatbot or chat engine that can respond to multiple related questions, we need to give the system memory\n",
        "\n",
        "response = query_engine.query(\"What did I just ask you? Give me the exact question\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYCUDvIjrMxq",
        "outputId": "23963fd0-786b-418f-f789-3e9608fb347b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What did I just ask you? Give me the exact question\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to give the model memory, we need to create a different engine\n",
        "\n",
        "from re import VERBOSE\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"openai\", verbose=True) # other chat_mode's include Simple, React, condense, among others\n",
        "response = chat_engine.chat(\"Hello there\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXvSjXwarVja",
        "outputId": "b921a547-0a5c-421a-f9da-e9a79ceefd3c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Hello there\n",
            "Hi! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"What is the likelihood of a recession in 2025?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUqWAstlsS1I",
        "outputId": "70419307-05b1-417b-8af3-db0fcb28a26b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What is the likelihood of a recession in 2025?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"What is the likelihood of a recession in 2025?\"}\n",
            "Got output: The likelihood of a recession in 2025 is 20%, which is close to the long-term average but below the current consensus.\n",
            "========================\n",
            "\n",
            "The likelihood of a recession in 2025 is estimated to be around 20%, which is close to the long-term average but below the current consensus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"Can you expand on that?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsalKyT6vLZ9",
        "outputId": "7e9564d7-7c66-4a2d-bb2a-5392a719f69f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Can you expand on that?\n",
            "The likelihood of a recession in 2025 being estimated at 20% means that there is a moderate chance of an economic downturn occurring that year. This percentage is close to the long-term average probability of a recession but is slightly lower than the current consensus among experts and analysts. Factors such as economic indicators, market trends, and global events can influence the probability of a recession in 2025. It's important to monitor these factors closely to assess the potential risks and prepare accordingly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"What did I just ask you?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL2TI2c8sggc",
        "outputId": "89aa129a-5e65-4e3e-ce4b-2d3883d1ad85"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What did I just ask you?\n",
            "You asked about the likelihood of a recession in 2025.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "QxHwxx3avuaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "\n",
        "Settings.chunk_size = 1000\n",
        "Settings.chunk_overlap = 50\n",
        "\n",
        "# Create a VectorStoreIndex object from the documents. This will involve processing the documents\n",
        "# and creating a vector representation for each of them, suitable for semantic searching.\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Convert the VectorStoreIndex object into a query engine. This query engine can be used to\n",
        "# perform semantic searches on the index, matching natural language queries to the most relevant\n",
        "# documents in the index.\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Use the query engine to search for documents that are relevant to the query\n",
        "# from the indexed documents based on the semantic understanding of the query.\n",
        "response = query_engine.query(\"What is the 2025 outlook for US GDP?\")\n",
        "\n",
        "# Print the response obtained from the query. This will display the result of the semantic search,\n",
        "# showing the information or documents that best match the query about the 2025 outlook.\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91bK6dPVvvsw",
        "outputId": "25ab291a-1456-4910-a0ce-3792e2e86627"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2025 outlook for US GDP is expected to expand by 2.3% according to the provided context information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector DB"
      ],
      "metadata": {
        "id": "X3XvaWI1v3Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-vector-stores-weaviate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbppGMoH0s5Q",
        "outputId": "5ad625ed-9d6f-4c3f-fe65-b755e7adfdcd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-vector-stores-weaviate in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-weaviate) (0.12.19)\n",
            "Requirement already satisfied: weaviate-client<5.0.0,>=4.5.7 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-weaviate) (4.11.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (3.11.12)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.17.2)\n",
            "Requirement already satisfied: validators==0.34.0 in /usr/local/lib/python3.11/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (0.34.0)\n",
            "Requirement already satisfied: authlib<1.3.2,>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.3.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.70.0)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.70.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.70.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.18.3)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib<1.3.2,>=1.2.1->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (43.0.3)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.11/dist-packages (from grpcio-health-checking<2.0.0,>=1.66.2->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (5.29.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-weaviate) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-vector-stores-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlwWnSnxv56O",
        "outputId": "dbcd941e-9064-456d-e431-d112eb1831c3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-vector-stores-chroma in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: chromadb>=0.5.17 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-chroma) (0.6.3)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-chroma) (0.12.19)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.10.6)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.115.8)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.14.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.20.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.30.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.21.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.70.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.2.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.15.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (13.9.4)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.11.12)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (11.1.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.32.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.18.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.45.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2024.11.6)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.67.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.51b0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.28.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.0.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (14.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "chroma_client = chromadb.PersistentClient()\n",
        "chroma_collection = chroma_client.create_collection(\"tech16example\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ],
      "metadata": {
        "id": "n1OYQn37wEa-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./\").load_data(\"2025-isg-outlook.pdf\")\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is the 2025 outlook for US GDP?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1yT_TAziwTfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176622d3-40fa-4c35-d86f-e638f23dd25c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|██████████| 1/1 [00:14<00:00, 14.38s/file]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2025 outlook for US GDP is an expansion of 2.3% according to the provided context information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube\n",
        "\n",
        "## TODO: Debug the youtube import error\n"
      ],
      "metadata": {
        "id": "-BjvWa3RwXNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api"
      ],
      "metadata": {
        "id": "Ra-khXG8wZGs",
        "outputId": "c13d0639-4124-4955-bdf9-26bce81bc510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.11/dist-packages (0.6.3)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
        "\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(\n",
        "    ytlinks=[\"https://www.youtube.com/watch?v=i3OYlaoj-BM\"]\n",
        ")"
      ],
      "metadata": {
        "id": "6_-Y27NBwbzI",
        "outputId": "ade041b6-41bf-4316-c338-076e10ed4541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_index.readers.youtube_transcript'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e808cb918f5d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myoutube_transcript\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYoutubeTranscriptReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYoutubeTranscriptReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m documents = loader.load_data(\n\u001b[1;32m      5\u001b[0m     \u001b[0mytlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"https://www.youtube.com/watch?v=i3OYlaoj-BM\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.readers.youtube_transcript'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Convert the VectorStoreIndex object into a query engine. This query engine can be used to\n",
        "# perform semantic searches on the index, matching natural language queries to the most relevant\n",
        "# documents in the index.\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Use the query engine to search for documents that are relevant to the query\n",
        "# from the indexed documents based on the semantic understanding of the query.\n",
        "response = query_engine.query(\"What is the 2025 outlook for US GDP?\")\n",
        "\n",
        "# Print the response obtained from the query. This will display the result of the semantic search,\n",
        "# showing the information or documents that best match the query about the 2025 outlook.\n",
        "print(response)"
      ],
      "metadata": {
        "id": "i0x2ssapwfOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e26a667-60b9-4ba9-91d9-fae4912cf1fd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2025 outlook for US GDP is an expansion of 2.3% according to the provided context information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *HW3*\n",
        "\n",
        "Uploaded Captions from Lecture 1,2,3 to a folder labelled \"Tech16_Captions\""
      ],
      "metadata": {
        "id": "A26zN_pV32ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('./Tech16_Captions').load_data('Lecture?.txt')\n",
        "documents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fk6Yu0I339L",
        "outputId": "e5843c64-0d84-4196-d781-b8aca4977906"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|██████████| 3/3 [00:00<00:00, 1665.95file/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='89e6ff33-5a33-4f4f-85f9-cf887495f16f', embedding=None, metadata={'file_path': '/content/Tech16_Captions/Lecture1.txt', 'file_name': 'Lecture1.txt', 'file_type': 'text/plain', 'file_size': 135926, 'creation_date': '2025-02-20', 'last_modified_date': '2025-02-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Lecture 1 Captions\\r\\n\\r\\nCathal Flanagan: Okay, share my screen.\\r\\n0:00\\r\\nCathal Flanagan: Excellent. Okay. Hopefully, folks can can see my screen. Welcome. Welcome to the 1st\\r\\n0:17\\r\\nCathal Flanagan: of many classes of tech 16 for this winter.\\r\\n0:28\\r\\nCathal Flanagan: llms for business with python. Really excited to have you here. It's\\r\\n0:35\\r\\nCathal Flanagan: honestly, it's my favorite 8 weeks of the year. So it's\\r\\n0:40\\r\\nCathal Flanagan: this is an exciting time. Never been better. We're going to talk about a lot of stuff that's happening in the news. The challenge of teaching this class, as you'll find out, is\\r\\n0:44\\r\\nCathal Flanagan: I, you know. Of course, I have the content ready in advance, and then I have to change everything all at once, because\\r\\n0:55\\r\\nCathal Flanagan: the AI world is changing constantly. So it's a it's a fun but challenging class to teach because it's\\r\\n1:02\\r\\nCathal Flanagan: you know, there's we're we're living in exciting times, that's for sure.\\r\\n1:10\\r\\nCathal Flanagan: so welcome to everybody as you'll show you. We have people from all around the world, and it's pretty exciting. It's my favorite thing about doing this class virtually. I do a class in\\r\\n1:15\\r\\nCathal Flanagan: on campus in Stanford, and that's why I started. I've been teaching at Stanford in\\r\\n1:27\\r\\nCathal Flanagan: Since 2019 started with a data visualization class where I took a class from my time at Google up to Stanford. And then during Covid, we went online, of course. And then, now, I do a hybrid and, you know, given the demand for this class, we'd like to do it online, because.\\r\\n1:33\\r\\nCathal Flanagan: you know, it's opens it up to a wider community. So it is fantastic to have alumni and and friends all around the world that have come out of this class, so I hope you will, too.\\r\\n1:52\\r\\nCathal Flanagan: To introduce myself my name is everyone calls me Charlie my Irish name is Cahal\\r\\n2:04\\r\\nCathal Flanagan: I was born in Ireland, which is why I have an accent. It's also why I speak too quickly sometimes.\\r\\n2:10\\r\\nCathal Flanagan: I am currently the head of applied AI at Bali. Azni asset management. It's Azni asset management is a\\r\\n2:16\\r\\nCathal Flanagan: 22 billion dollar Multi Strategy fund so large. Hedge fund. Multi-strategy means\\r\\n2:24\\r\\nCathal Flanagan: we do everything from\\r\\n2:30\\r\\nCathal Flanagan: equities to macro to commodities to growth equity. So it's a real range, and everything is set up in different teams. So you get to see a lot of a lot of different ways that people invest and how they're using AI to do so. So I'm in charge of\\r\\n2:33\\r\\nCathal Flanagan: all of the AI efforts at the firm. I have a team, a relatively large team now, of folks were like a center of excellence that goes across the entire firm.\\r\\n2:51\\r\\nCathal Flanagan: And you know, very much building a lot of tools. We're going to talk about retrieval, augmented generation systems in this class. We're going to talk about agents\\r\\n3:00\\r\\nCathal Flanagan: I'm in the trenches every day with my team trying to build these things.\\r\\n3:09\\r\\nCathal Flanagan: But also as importantly, education. We spend about 1 3rd of all of the time within the firm educating folks on what's new how to use the tools that we're building, how to get the most out of the tools that exist today. So\\r\\n3:13\\r\\nCathal Flanagan: you know, it's a journey. It's and I think everyone, including folks in the investment world, are figuring out what it means for them.\\r\\n3:27\\r\\nCathal Flanagan: Prior to that, I was at Google for many years.\\r\\n3:35\\r\\nCathal Flanagan: I was based in the Bay area. I'm now based in New York.\\r\\n3:38\\r\\nCathal Flanagan: at Google, I spend most of my time, leading the data science efforts for a project within searching assistant called Google Duplex, which you probably haven't heard of. But I'll tell you a little bit about it in a moment because it's relevant to Llms.\\r\\n3:42\\r\\nCathal Flanagan: And then before that, I was on Wall Street also. But I was actually more based in London in the Bahamas, working for a quantitative fund doing trend following. So mix of finance and technology. So pretty pretty fun career always gotten to work with\\r\\n3:57\\r\\nCathal Flanagan: people far smarter than I, and learn a lot from them. So it's it's been a been a fun ride so far.\\r\\n4:15\\r\\nCathal Flanagan: Oh, yes. I mentioned duplex. Duplex is\\r\\n4:24\\r\\nCathal Flanagan: a technology that started in Google.\\r\\n4:28\\r\\nCathal Flanagan: and it actually started as a 20% project.\\r\\n4:32\\r\\nCathal Flanagan: Which means, you know, it was kind of people doing this in their spare time. And it was actually the very 1st\\r\\n4:36\\r\\nCathal Flanagan: product within Google to leverage large language models. Introduce practical things. Let me play that in some second. I'm just going to stop my screen and share it again. So we get the audio.\\r\\n4:43\\r\\nCathal Flanagan: It was actually the the 1st product to leverage language models to do practical things. So the practical things were making phone calls to restaurant and hairdressers initially and then, later, it actually evolved to\\r\\n4:58\\r\\nCathal Flanagan: power, Google Search or sorry Google Maps, particularly during the pandemic, when maps needed to understand very quickly what had changed. If doctors were accepting new patients, if stores had Ppe in stock\\r\\n5:12\\r\\nCathal Flanagan: and today it actually powers about 1 3rd of all the information that you see on Google Maps and speak 7 languages. So pretty impressive fun project to be involved in but also, of course, it meant we saw the\\r\\n5:25\\r\\nCathal Flanagan: you know, that both the opportunity and the challenges early since 2018 of kind of building things with language models. Let me play a short demo of\\r\\n5:39\\r\\nCathal Flanagan: duplex notation.\\r\\n5:49\\r\\nCathal Flanagan: progress to the system, or what time are you?\\r\\n5:54\\r\\nCathal Flanagan: Sorry?\\r\\n6:00\\r\\nCathal Flanagan: Let's try this again.\\r\\n6:03\\r\\nCathal Flanagan: The progress with the haircut for our clients. I'm looking\\r\\n6:07\\r\\nCathal Flanagan: to schedule the appointment for you. Let's listen.\\r\\n6:12\\r\\nCathal Flanagan: Oh, how can I help you, Hi.\\r\\n6:17\\r\\nCathal Flanagan: to book a woman's haircut for a client? I'm looking for something on May 3, rd\\r\\n6:23\\r\\nCathal Flanagan: sure. Give me one second\\r\\n6:28\\r\\nCathal Flanagan: Sure. What time are you looking for around at 12 PM.\\r\\n6:34\\r\\nCathal Flanagan: We do not have a 12 Pm. Available. The closest we have to that is a 1 15. Do you have anything between 10 Am. And 12 Pm.\\r\\n6:39\\r\\nCathal Flanagan: Depending on what service she would like? What service is she looking for? Just a woman's haircut for now.\\r\\n6:50\\r\\nCathal Flanagan: Okay, we have a 10 o'clock.\\r\\n6:57\\r\\nCathal Flanagan: 10 Am. Is fine.\\r\\n7:00\\r\\nCathal Flanagan: Okay? What's her 1st name? The 1st name is Lisa.\\r\\n7:01\\r\\nCathal Flanagan: Okay, perfect. So I will see Lisa at 10 o'clock on May 3.rd Okay, great thanks. Great. Have a great day bye.\\r\\n7:06\\r\\nCathal Flanagan: So that was duplex all the way back in 2018. So as you can imagine, it's\\r\\n7:16\\r\\nCathal Flanagan: It has come a long way even since then. The\\r\\n7:21\\r\\nCathal Flanagan: I guess the the lesson that I will be taking from duplex and telling you throughout this classes. It's never about building a language model or a system it's really about.\\r\\n7:28\\r\\nCathal Flanagan: you know, taking\\r\\n7:41\\r\\nCathal Flanagan: small capabilities of where the models are today, and chaining them together to interesting and useful and practical things.\\r\\n7:44\\r\\nCathal Flanagan: You know. Certainly this was very early, but we were able to get us to super interesting and powerful and practical things, but of course it was held together by duct, tape, and baling worm. Right? It's you know, as you might imagine. It's\\r\\n7:51\\r\\nCathal Flanagan: It was an experiment. So\\r\\n8:07\\r\\nCathal Flanagan: a lot of lessons I take from that that I'll be bringing and talking about as part of this class in terms of you know\\r\\n8:10\\r\\nCathal Flanagan: the applied nature of these problems. So you know these, we're on the path to Agi, I do believe.\\r\\n8:16\\r\\nCathal Flanagan: bus, you know, along the way we're we're definitely building these systems in. And they're very imperfect.\\r\\n8:25\\r\\nCathal Flanagan: but it doesn't mean they're not useful, right? And we're spending a lot of time.\\r\\n8:33\\r\\nCathal Flanagan: My team is called the Claudia Team, because we're trying to apply these things, these models to do useful things. Given the limitations. And there are real limitations that they are today.\\r\\n8:38\\r\\nCathal Flanagan: Okay, that's enough about me. Let me introduce our wonderful Tas\\r\\n8:48\\r\\nCathal Flanagan: Dima, do you want to tell us a little bit about yourself.\\r\\n8:54\\r\\nCathal Flanagan: Are you with us, Geema?\\r\\n9:05\\r\\nDima Timofeev: So. Hello, everyone my name is Dean.\\r\\n9:11\\r\\nDima Timofeev: Can you hear me? Well.\\r\\n9:14\\r\\nCathal Flanagan: We can hear you now. Yeah.\\r\\n9:16\\r\\nDima Timofeev: Hey, guys, can you hear me? Oh, okay, and I think the slide disappeared. But let me keep talking, and please let me know if you will lose me\\r\\n9:17\\r\\nDima Timofeev: as I mentioned originally, I'm from Ukraine, and you can hear it from my Eastern European accent.\\r\\n9:26\\r\\nDima Timofeev: And I met Charlie back at Google, which was really cool. Actually, I met him at Stanford and not at Google. And then we realized that we both work at Google.\\r\\n9:33\\r\\nDima Timofeev: And I'm a software engineer, my almost entire career.\\r\\n9:44\\r\\nDima Timofeev: I'm working on machine learning systems. This slide is going to be a bit outdated. Since I recently decided to join AI Robotics Company instead of autonomous vehicles or self-driving companies instead.\\r\\n9:48\\r\\nDima Timofeev: I've been working for the last almost 4 years\\r\\n10:02\\r\\nDima Timofeev: for cruise, which is General motors, autonomous Vehicle Company and Wemo, which is Google, Autonomous Vehicle Company. If you live in\\r\\n10:07\\r\\nDima Timofeev: San Francisco. So in the Us. Or Japan or Dubai, you probably have seen cruise or Waymo cars in San Francisco, Houston, Austin, Dallas, La Los Angeles, Miami as well, Tokyo and Dubai to some lim in in some limited areas. So these are 2 companies that they used to work for.\\r\\n10:17\\r\\nDima Timofeev: And I am all about embodiment of AI. So if you ever want to discuss. If your business or your area of interest, or your area of research is related to how you can move not only bits but also atoms. I'm more than happy to talk to you, discuss with you how you can move your AI to.\\r\\n10:38\\r\\nDima Timofeev: to embodiments, to embedded devices, to do the local inference, etcetera.\\r\\n11:02\\r\\nDima Timofeev: And before I got fully occupied with embodied ais in the form of humanoid robots or self driving cars, I worked for 5 and a half years at Google in Google search and Google ads.\\r\\n11:07\\r\\nDima Timofeev: And this is my quick background. I have very typical software engineering education. So nothing very specific. So whatever questions you have with respect to programming python or, again, embodiments of everything that you're gonna learn in this class.\\r\\n11:21\\r\\nDima Timofeev: I'm here, and I'm available all the time to help you, not only with this, but this is an extension to the class. If you will have any personal interest.\\r\\n11:42\\r\\nCathal Flanagan: Awesome, and as you guys will learn very quickly, we are very lucky to have Dima. He is\\r\\n11:54\\r\\nCathal Flanagan: a wonderful addition, and I hope we'll get him to teach some sections of the class as well. It's I learn a lot from him all the time, and I think expect we'll have some good debates which I like to.\\r\\n12:01\\r\\nCathal Flanagan: Okay. And secondly, Anya, Anya, you're with us.\\r\\n12:12\\r\\nDima Timofeev: I think she will not join us today. Unfortunately.\\r\\n12:20\\r\\nCathal Flanagan: Oh, okay, yeah. Yeah. Okay. Little confusion. Yes. So we also are very lucky to be joined. We have another ta, who you'll start hearing from Anya\\r\\n12:22\\r\\nCathal Flanagan: annually. So she is a product manager, and also teaches a class. Stanford. She's currently agreed to join us as a team this class as well. And you'll see Anya online responding things in slack sending you emails, following up\\r\\n12:31\\r\\nCathal Flanagan: so as you'll come to learn, slack will be the primary use of communications in the class. So we're all going to be busy chatting there over the next next 8 weeks and beyond\\r\\n12:50\\r\\nCathal Flanagan: and then, of course, I want to know who you are. I sent out a pre-class survey. I have a little bit of a sense. So you know, I look forward to getting to know you over the next 8 weeks what I'd like you to do now is or today or tomorrow. Whenever you want, introduce yourself, tell us in the slack channel you know who you are, where you're located.\\r\\n13:02\\r\\nCathal Flanagan: and ideally tell us what you're looking to get from the class and drop your linkedin profile on the Linkedin Channel. If you'd like connect with one another. Yeah, we're\\r\\n13:24\\r\\nCathal Flanagan: We'd like to get to know each other over the next 8 weeks.\\r\\n13:35\\r\\nCathal Flanagan: Awesome. And that is because, yes, we want to learn a lot lot about Llms. Yes, we want to.\\r\\n13:40\\r\\nCathal Flanagan: You know, it really kind of come around away with some practical tools in the form of code that we can actually run and use and understand. But as importantly, and maybe even more importantly, this class is about creating community right?\\r\\n13:50\\r\\nCathal Flanagan: That is very, very important to me, to Dima, to Anya that\\r\\n14:06\\r\\nCathal Flanagan: in this class you get to meet really interesting people.\\r\\n14:11\\r\\nCathal Flanagan: And you know I've I've I've had the opportunity to teach this class a couple of times, and I've gotten to know. Look up the profiles of some folks who are already taking this class, and there's some fascinating folks so take advantage of it.\\r\\n14:15\\r\\nCathal Flanagan: We're gonna be setting up meetups.\\r\\n14:29\\r\\nCathal Flanagan: We want you to self organize.\\r\\n14:31\\r\\nCathal Flanagan: We want you to reach out. You can see that where people are located around the world. Of course Bay Area takes a is a heavy dominance, but we have a large contingent of folks, maybe 10 or 12 people in Brazil. People all around the world. So if you happen to be going somewhere, it's also I certainly will intend to be saying, hey? I'm you know I'm going to\\r\\n14:35\\r\\nCathal Flanagan: Turkey, or the Uk. Or something like that, and then wanna meet up so definitely\\r\\n14:56\\r\\nCathal Flanagan: a community that we want you to be part of, and you will find that you will get a lot more from this class by\\r\\n15:03\\r\\nCathal Flanagan: participating, participating in slack by coming to the office hours, to the socials, to the other events outside of class and really kind of joining the community. So we look forward to to to you doing that.\\r\\n15:10\\r\\nCathal Flanagan: and I already mentioned this. But slack is our preferred, and will be our primary\\r\\n15:23\\r\\nCathal Flanagan: channel of communication if you're not already in the slack group. Then we will drop\\r\\n15:30\\r\\nCathal Flanagan: Do you know, maybe you can actually drop the link that to the slack group. Invite in the zoom chat in case there's anybody who\\r\\n15:38\\r\\nCathal Flanagan: if there's anyone who hasn't\\r\\n15:47\\r\\nCathal Flanagan: signed up already. If you haven't signed up, please sign up it will be the primary way we communicate. I will also send a recap email every week.\\r\\n15:50\\r\\nCathal Flanagan: And just to kind of tell you what we've done in class give you links to the assets.\\r\\n15:59\\r\\nCathal Flanagan: But really slack is where it's gonna be at. So join there, get involved\\r\\n16:04\\r\\nCathal Flanagan: questions, debates, sharing fun, stuff, all that good stuff.\\r\\n16:10\\r\\nDima Timofeev: Yeah, I would like to quickly mention why we use slack instead. 1st of all, everybody who is going to watch it later can read through, and second, one slack supports\\r\\n16:15\\r\\nDima Timofeev: much better threads, so we can continue conversations after even after the end of the class. So please utilize, slack the link to the slack is already in zoom chat. It's 3rd message from the end, so it's very easy to find, and I closed the chat and we will be closing it in all future classes as well.\\r\\n16:27\\r\\nCathal Flanagan: Awesome and then the other thing to call out is within slack at the top of the general channel. We have the bookmarks. That's where you'll find the link to the zoom, to the zoom link that we run the class. We use the same zoom, link every class.\\r\\n16:47\\r\\nCathal Flanagan: or every office hours. You only need one zoom link for the entire 8 weeks.\\r\\n17:02\\r\\nCathal Flanagan: The other thing is create some channels. You know you're certainly not limited to the channels that we've created. So there are other things that are interesting, for you create them, socialize them in the general channel. So other people are aware of them. Yeah, it's really it's it's your community space to make the most out of while you have it.\\r\\n17:08\\r\\nCathal Flanagan: And then, you know, I want to talk about why I started this class. Well.\\r\\n17:27\\r\\nRichard Ryan: I guess.\\r\\n17:36\\r\\nCathal Flanagan: You know. I I'm like you are very. I'm very interested in what\\r\\n17:37\\r\\nCathal Flanagan: particularly advances around language models mean for us.\\r\\n17:43\\r\\nCathal Flanagan: And I read a lot on the topic. I listen to a lot on the topic.\\r\\n17:47\\r\\nCathal Flanagan: and quite frankly, you know, you read the headlines right, and lots of concern lots of doom and gloom. You know, we're going to be automated. We're going to go to take our jobs. You know. It'll displace us all.\\r\\n17:53\\r\\nCathal Flanagan: but I guess for me.\\r\\n18:10\\r\\nRichard Ryan: Don't worry.\\r\\n18:12\\r\\nCathal Flanagan: When I look at the these headlines, these are all from like the 19 forties, the 19 fifties, the 19 eighties\\r\\n18:13\\r\\nCathal Flanagan: right?\\r\\n18:19\\r\\nCathal Flanagan: And it's always been this case. It's always this case that when we have fundamentally changing technology that\\r\\n18:20\\r\\nCathal Flanagan: you know, the media people\\r\\n18:29\\r\\nCathal Flanagan: get nervous, get scared. And you know for me. I'm as you will come to learn. I'm very, very optimistic about this technology.\\r\\n18:32\\r\\nCathal Flanagan: So I really, you know, I start the class, partly because I want to bring that\\r\\n18:41\\r\\nCathal Flanagan: those learnings and the exploration of the technology, and how great it can be and what it can do for us. You know, to an audience.\\r\\n18:47\\r\\nCathal Flanagan: I, of course, couldn't\\r\\n18:58\\r\\nCathal Flanagan: take take on this class this week without talking about deep seek\\r\\n19:01\\r\\nCathal Flanagan: you know. So it's part of, you know, part of the class. Every week\\r\\n19:08\\r\\nCathal Flanagan: there will be a new headline. There'll be something a new model and new development. So we'll definitely discuss it.\\r\\n19:11\\r\\nCathal Flanagan: And you know this is part of it, right? There's like part of the reason reason I wanted to\\r\\n19:17\\r\\nCathal Flanagan: to the classes, because it's great to have a community to start discussing this, to start understanding, you know, beyond the noise beyond, behind the headlines, you know what's actually real like, let's get hands on. Let's test some of these models. Let's form an opinion with a group of like minded individuals.\\r\\n19:20\\r\\nCathal Flanagan: So I guess you know, it's definitely been an interesting\\r\\n19:37\\r\\nCathal Flanagan: interesting week since Deep Seat came out. It feels like everybody's become an expert all of a sudden.\\r\\n19:42\\r\\nCathal Flanagan: I don't diva\\r\\n19:49\\r\\nCathal Flanagan: tell me, what's your opinion of Deepseek? 1st of all, I assume that most people taking this class have\\r\\n19:52\\r\\nCathal Flanagan: are aware of deep seek. But you know, maybe, Dima. Just tell us, what is deep. Seek and tell us your opinion of it.\\r\\n19:59\\r\\nDima Timofeev: Okay, deep seek is the model that dropped Nvidia stock by 3% and wiped out a lot of equity in many 80 companies in many AI companies. I'm sorry. It was created by Chinese, I think wealth management fund or hedge fund. I'm not sure which we didn't\\r\\n20:06\\r\\nDima Timofeev: haven't heard about before deepseek, or the majority of us, and apparently it's in par with the top models from Openai. There is no sauna. There is no any anybody else to compare with, and it performs so well on such a great level that it became the number one app in App store as well. So the chat application around this model.\\r\\n20:31\\r\\nDima Timofeev: and every and the main point. They claim. The authors of the model claim that they used only 6 million dollars, which is like nothing which is like I don't know how to say. It's like the price of maintaining servers for modern AI models when you are usually talking about hundreds of millions.\\r\\n20:56\\r\\nDima Timofeev: and it's in power. Everybody says the dominant theory. Now I will give you 2 different theories, and you buy anyone. One party says that. Oh, no, actually, it hallucinates much more. And actually you cannot train it from scratch from first.st Try Yada Yada. So basically saying, No, it's not true. The model hallucinates a lot. It doesn't have sophisticated tools, and it's the final version they spend significantly more. And they escaped\\r\\n21:15\\r\\nDima Timofeev: trade limits imposed by us on China. It's 1 party, and the second party is like, Whoa! It's cool. We just they nailed it down. You don't actually need crazy amount of resources to trade in such a sophisticated model, and we are too far from China, and we need to catch up. We were too obsessed with compute. And we need to figure out about new algorithms. So here is blue and red pill. Pick your own.\\r\\n21:43\\r\\nTony A: I don't know if you heard, by the way, that Openai is looking into, whether they used their model to distill down and basically used a database of responses from Openai, from from their model to train their model. Supposedly, that's not the latest, so they may not be as far as long as we think, with their\\r\\n22:10\\r\\nTony A: with their new approach, if you will, to training. But who who knows? That's all that's still outstanding, of course.\\r\\n22:34\\r\\nCathal Flanagan: Well.\\r\\n22:43\\r\\nCathal Flanagan: they won't have to look too far, because it is blatantly obvious that they did that particularly with the.\\r\\n22:45\\r\\nTony A: Yeah, yeah.\\r\\n22:50\\r\\nCathal Flanagan: That they released over Christmas. Given the fact that the V 3 model thinks that it's chat gpt so. You know the mystery is in there. It is a little bit ironic, of course, because Openai, like every lab, has been training on data that they haven't had full permission with right.\\r\\n22:51\\r\\nTony A: Right, right.\\r\\n23:10\\r\\nCathal Flanagan: etc. So you know, there's a little pot capital black element to this whole thing.\\r\\n23:10\\r\\nCathal Flanagan: At the same time. What does this tell us, right? The headlines would have you believe that? You know we have a Chinese lab that is.\\r\\n23:17\\r\\nCathal Flanagan: you know, potentially competitive with Openai. I I take a slightly different opinion. I think it means that they're that they are fast followers right?\\r\\n23:26\\r\\nCathal Flanagan: I don't think they are competitive directly. I think they. As for the point.\\r\\n23:39\\r\\nCathal Flanagan: it means that models in the future will get commoditized much faster, potentially right. And you know, not to take anything away the the paper and the work from deep seek is impressive. They have certainly obtained a lot of efficiencies. They've gotten a lot more. Seems if we believe that they're actually using H. 800, which are particular type of chip from Nvidia, which are still\\r\\n23:47\\r\\nCathal Flanagan: ex exportable. You know, they squeeze out a lot more\\r\\n24:12\\r\\nCathal Flanagan: efficiencies from those. Again, there's a lot of debate. Maybe they're not actually using H. 800. But they actually have access to H. 100 s. Which are the more powerful chips.\\r\\n24:17\\r\\nCathal Flanagan: So there's a lot of conspiracy going on around that\\r\\n24:26\\r\\nCathal Flanagan: but at the same time, you know.\\r\\n24:30\\r\\nCathal Flanagan: they? They deserve a decent amount of credit for some innovation, but\\r\\n24:33\\r\\nCathal Flanagan: quite honestly, if they hadn't open source this, I don't think it would have made news. It's an impressive, it's impressive, but it's not as impressive that. We know that O 3 is coming out.\\r\\n24:39\\r\\nCathal Flanagan: They're also.\\r\\n24:51\\r\\nCathal Flanagan: you know, they're being clever right? They're obviously have a strategy here about launching an app and launching an Api that they want to attract\\r\\n24:54\\r\\nCathal Flanagan: attention to. So if you actually looked at the way they were pricing the Api.\\r\\n25:01\\r\\nCathal Flanagan: They were being very, very competitive 1 20 at the cost of the o. 1 model from Openai. But then, if you actually read the fine print in about 10 days, they're going to increase the price right now, it will still be about a quarter of the price so still a significant saving. But you know they got what they needed, which was the headlines and the the attention of the world.\\r\\n25:06\\r\\nCathal Flanagan: I wonder. They probably may have thought that they've actually been too successful, because for did seem for a period of time their servers went down, which I can imagine the amount of load they're dealing with.\\r\\n25:28\\r\\nCathal Flanagan: So I my read on, this is.\\r\\n25:38\\r\\nCathal Flanagan: it's impressive, but quite frankly, I think the headlines are overstating us.\\r\\n25:42\\r\\nCathal Flanagan: and in terms of the stock price reaction for Nvidia.\\r\\n25:47\\r\\nCathal Flanagan: You know, I I we mentioned Jevons paradox already, and the fact that everybody has become a has become an expert on Jevons. Jevin is paradox states that you know. If.\\r\\n25:53\\r\\nCathal Flanagan: as the\\r\\n26:04\\r\\nCathal Flanagan: price of compute or goes down, the demand will actually go up by more. Let's say, if it goes down by a half, the demand for it will actually go up by more than 2 x.\\r\\n26:06\\r\\nCathal Flanagan: So\\r\\n26:17\\r\\nCathal Flanagan: certainly I think that I do. But I am actually on the side of that right. It's we are. This will probably need to an explosion of\\r\\n26:19\\r\\nCathal Flanagan: compute, particularly inference time compute, which is means after the model is trained, actually serving it, because these reasoning models are particularly inference time heavy. Again, if you don't understand this, what this means. We'll be covering it in class later on in the coming semester. So\\r\\n26:28\\r\\nCathal Flanagan: boss, I definitely think\\r\\n26:45\\r\\nCathal Flanagan: it. It's been overstairs. I also was just be mused because Nvidia stock went down 20% because a Chinese lab showed that Nvidia's chips are better than we thought.\\r\\n26:49\\r\\nCathal Flanagan: which is just a little interesting. So certainly.\\r\\n27:00\\r\\nRichard Ryan: Just in that show, Charlie just not sure that they were. I mean that they were overpriced for the actual demand, because it it so so what? So in video lost, I want to say, a quarter of a Twi trillion dollars worth of market share in one day\\r\\n27:04\\r\\nRichard Ryan: when deep sea\\r\\n27:21\\r\\nRichard Ryan: released. And that suggests and I agree with you. I mean, I think obviously, Nvidia has dominated the Gpu market for for the last 2 years, I mean since since the AI Revolution began. But doesn't that suggest that they were overpricing the chips? And they were waiting\\r\\n27:23\\r\\nRichard Ryan: they they should have been waiting for this moment to occur when somebody came along that could underprice them and and seize the huge market share back from them.\\r\\n27:46\\r\\nCathal Flanagan: Has now under price. Sorry it\\r\\n27:57\\r\\nCathal Flanagan: no one's under pricing Nvidia here to be clear. Many people are might be under pricing,\\r\\n28:00\\r\\nCathal Flanagan: commoditizing, like Openai, for example, right? Because they're making a 0 1 level model available for a fraction of the price Nvidia gets paid either way. This company needs Nvidia Chips now more than ever, as does everybody else.\\r\\n28:06\\r\\nCathal Flanagan: So it's just it. But the concern was, of course.\\r\\n28:20\\r\\nCathal Flanagan: if it now takes a fraction of the amount of computes to train. These models.\\r\\n28:25\\r\\nCathal Flanagan: will and video have the same lead or same backlog and demand that they're seeing\\r\\n28:30\\r\\nCathal Flanagan: and it's a reasonable debate. But quite frankly, like, certainly, my! I've come down. I come down the side that this will lead to more demand for compute, and Nvidia Chips not less\\r\\n28:38\\r\\nCathal Flanagan: because every it's gonna continue to proliferate the that.\\r\\n28:51\\r\\nCathal Flanagan: you know, use of AI. That's why you sell on the day you sell salesforce stock up because Salesforce doesn't.\\r\\n28:56\\r\\nCathal Flanagan: Doesn't make their own models. They use other people's, and they could. Now they have another very good model that they can actually start leveraging for free.\\r\\n29:04\\r\\nCathal Flanagan: So anybody have any thoughts, any particularly any\\r\\n29:13\\r\\nCathal Flanagan: thoughts that disagree with any of the statements. So far we'd like it to be here.\\r\\n29:18\\r\\nArun Tyagi: I mean one of the things that I'm like today. People discount Alibaba. They actually released when 2.5, which which kind of claims they're better than\\r\\n29:22\\r\\nArun Tyagi: deep seek. So it's like, I think I'm of the opinion like Charlie. You said in the near future, definitely, you know, large models are going to be commoditized. So I don't know. These companies would need to find a better way to monetize than what they are doing right now. Those are my thoughts. Yeah.\\r\\n29:30\\r\\nCathal Flanagan: Yeah.\\r\\n29:49\\r\\nSarit Arora: One thought that I have is that it just makes it very accessible for communities and countries which could not have\\r\\n29:50\\r\\nSarit Arora: had any access to this technology at all.\\r\\n29:58\\r\\nSarit Arora: Like, I think there was some interview that I was seeing where Sam Altman mentioned that. Hey? You know what you can't even think of training the model in less than 10 million dollars. And now another company has shown that in fact, it's just gonna lead to even more boom where companies, maybe, or even African countries or or other countries which\\r\\n30:02\\r\\nSarit Arora: may not have had access to a technology such as this, they can start using and start using and finding use cases that no one would have thought about which could have help those communities.\\r\\n30:23\\r\\nCathal Flanagan: That's actually a very good point. Because.\\r\\n30:34\\r\\nCathal Flanagan: so to be clear, I actually, I don't believe that this model was really trained for 6 million dollars. I think that it was far more more than that. I think maybe the final training run was 6 million. But let's say it's even 20 or 30 million dollars\\r\\n30:38\\r\\nCathal Flanagan: in comparison to the 100 million or plus that it costs. Otherwise it's still a significant training or saving. But because the 6 million dollar was getting the headlines in the last number of last number of days I've been, I've heard, from a number of different companies who are now actually saying, Hey.\\r\\n30:51\\r\\nCathal Flanagan: maybe we should train our own model on our data, which they were never thinking about before.\\r\\n31:10\\r\\nCathal Flanagan: Right?\\r\\n31:16\\r\\nCathal Flanagan: Now, I still wouldn't recommend that in this class you'll talk. I'll talk about the different. We'll talk about fine tuning models. We'll talk about the different ways as a company. You might use that. But it's just interesting for your point.\\r\\n31:17\\r\\nCathal Flanagan: People are now thinking, Hey, this is accessible to us in a way that simply wasn't previously. Or perhaps it is. And then they're already reacting to that. So\\r\\n31:29\\r\\nCathal Flanagan: 3.\\r\\n31:37\\r\\nHeidi Bakhash: I have a question, what about the other models like diffusion models? Cnns, Gans, I mean, there's a lot of rumor that Deepseq just grabbed the memory of Openai and used reinforcement learning. And you know there are other kinds of deep learning. You know, AI models that are needed. This is just one kind.\\r\\n31:40\\r\\nCathal Flanagan: Well, so you're bringing up your what you're describing there. Cnn, gans, etc. These are other types of neural networks wouldn't be directly relevant here. Yes, they used open AI's data. But to be honest with you.\\r\\n32:04\\r\\nCathal Flanagan: that's not.\\r\\n32:18\\r\\nCathal Flanagan: That's\\r\\n32:20\\r\\nCathal Flanagan: it's it's definitely useful. It's like they were able to isolate some really high quality example responses. But they didn't distill the model down right? They still actually had to do a lot of really good technical work themselves to actually make the training of the model more efficient. So they definitely were you able to use openai for high quality data.\\r\\n32:22\\r\\nCathal Flanagan: But you know it wasn't like they just\\r\\n32:45\\r\\nCathal Flanagan: like they just copied it. They actually in fairness, they've actually like, there is a number of kind of so much innovative things within the paper, the way that they actually went about building this, both getting\\r\\n32:48\\r\\nCathal Flanagan: so the way that you train these models.\\r\\n32:59\\r\\nCathal Flanagan: you know, there's different levels, like. Generally, people need a lot of compute because these are highly precise. They do a lot of precision. For technical folks, 11.32 level precision. They actually, we had a very lot of innovation around the\\r\\n33:02\\r\\nCathal Flanagan: the need at different points of the process to actually use different levels of precision instead of just being greedy and using the maximum precision that's available. Instead, they actually were being quite clever about how they\\r\\n33:17\\r\\nCathal Flanagan: how they kind of manage\\r\\n33:30\\r\\nCathal Flanagan: different levels of precision of different levels of training. So they they deserve a decent like good credit. I I do expect people. So a lot of people.\\r\\n33:33\\r\\nCathal Flanagan: especially in smaller labs, will start changing the way that they train models based upon this.\\r\\n33:41\\r\\nCathal Flanagan: and the open AI thing is like, yes.\\r\\n33:47\\r\\nCathal Flanagan: Well, they could still build a very good model, even without that, I suspect.\\r\\n33:51\\r\\nBarkha: Berkeley already claims to have trained a 3 billion parameter model for under 30 bucks based on deep seek.\\r\\n33:56\\r\\nCathal Flanagan: You you mean it's it's you. It's a\\r\\n34:06\\r\\nCathal Flanagan: it's a fine tune of deep seat. Crash!\\r\\n34:08\\r\\nBarkha: I'm not. I didn't look too closely, but there's a r 1 0 model, which is a much smaller model, and the Berkeley seems to have recreated that like, trained that for just under $30.\\r\\n34:11\\r\\nCathal Flanagan: Yeah, these type of distill, these are also to call distilled models\\r\\n34:24\\r\\nCathal Flanagan: or our fine tune models a slight difference we'll talk about in coming classes. Yeah, like, there's a lot of few folks who will be will be able to take these now, and and\\r\\n34:30\\r\\nCathal Flanagan: you know, benefit from them. We saw exactly the same thing with Llama from Meta. Many people took them and and customize them for themselves. We're actually able to get even better performance in some cases.\\r\\n34:40\\r\\nDima Timofeev: Also quick comment to everyone. If everything we discuss here right now doesn't make any sense to you. Don't worry. You will get all the knowledge before the end of the class. We just had to address the elephant in the room, otherwise the majority, or some some number of people here, will just not listen until we address this elephant.\\r\\n34:51\\r\\nCathal Flanagan: Yes, or the whale in the room. It seems so given the logo. So yes, it's it's definitely top of mind for everyone. It's what makes it's what makes it fun. And in coming weeks we'll definitely be debating and and looking at new developments as they're as they're coming out whether it's related to deep seeker other things.\\r\\n35:11\\r\\nCathal Flanagan: But going back to kind of why we created this class. This is another reason.\\r\\n35:29\\r\\nCathal Flanagan: for a long time. And actually, still, if you're using some of the older models, if you're asking mathematical questions, the model will get it wrong even if you go back to asking the Gpt-four\\r\\n35:36\\r\\nCathal Flanagan: model. How many ores are in the word strawberry.\\r\\n35:50\\r\\nCathal Flanagan: it will get that wrong. It will say there are 2 ors in the word strawberry, there are actually 3\\r\\n35:55\\r\\nCathal Flanagan: that's why the o 1 model is actually called the strawberry model. So\\r\\n36:01\\r\\nCathal Flanagan: I bring this up, and it's part of the reason I create the classes because there are real limitations to these models. There's lots of things they're still not good at as much as we hear about breakthroughs, and you know, reasoning models.\\r\\n36:05\\r\\nCathal Flanagan: We are making a lot of progress a lot of progress quickly.\\r\\n36:19\\r\\nCathal Flanagan: but we still have to be very, very pragmatic about where these models have a lot of limitations. Right? Because then we know how to use them up. There's let's just be pragmatic. What? Where? The models aren't good today? Let's wait.\\r\\n36:22\\r\\nCathal Flanagan: And but there's lots of utility in where they are. But we have to understand. You know, where the actual gaps are.\\r\\n36:36\\r\\nCathal Flanagan: And I guess maybe the biggest reason I I started this class was kind of\\r\\n36:44\\r\\nCathal Flanagan: this slide. And this is the fundamental difference between how you and I have been using Chat Gpt in our personal lives\\r\\n36:48\\r\\nCathal Flanagan: versus how we would actually implemented in a business setting. Right? So this is called\\r\\n36:59\\r\\nCathal Flanagan: large language models for python, for business with Python.\\r\\n37:05\\r\\nCathal Flanagan: And the way that we think about applying these models today in the enterprise is very, very different.\\r\\n37:10\\r\\nCathal Flanagan: Why is it different?\\r\\n37:18\\r\\nCathal Flanagan: 1st of all, if I could have anyone, everyone remember, one thing would be this open AI is not a database.\\r\\n37:20\\r\\nCathal Flanagan: It looks like a database. It feels like a database. We use it as a database every day in our personal lives.\\r\\n37:26\\r\\nCathal Flanagan: you know. Hey? I'm going to London. Make me a 2 day itinerary.\\r\\n37:32\\r\\nCathal Flanagan: It will do a decent job with that. Why? Because it has been trained\\r\\n37:35\\r\\nCathal Flanagan: on the Internet, which has a lot of information about London.\\r\\n37:40\\r\\nCathal Flanagan: However, if you were out to ask the base model\\r\\n37:45\\r\\nCathal Flanagan: who is running in the election, the Us. Election\\r\\n37:51\\r\\nCathal Flanagan: the base model. Again, it has been augmented. If you ask the chat Gbt, it will actually it will give you. It will be. It will search the web and be slightly more updated. The base model underneath. It\\r\\n37:55\\r\\nCathal Flanagan: was only trained up until October of 2023 for Openai.\\r\\n38:06\\r\\nCathal Flanagan: and therefore, if you ask questions about who's running for election, it would tell you\\r\\n38:10\\r\\nCathal Flanagan: that you know the election is months away, or or a year away, and you know maybe it already might in hypothesized that it's Trump Trump and Biden, because it was training data around that\\r\\n38:16\\r\\nCathal Flanagan: bus.\\r\\n38:27\\r\\nCathal Flanagan: You know, it's a fundamental limitation of these models.\\r\\n38:29\\r\\nCathal Flanagan: There is a cut off date. There is a knowledge cut off. So you have to be aware of that. When you're trying to use that.\\r\\n38:32\\r\\nCathal Flanagan: The other reason that they're not as useful in the enterprise is\\r\\n38:41\\r\\nCathal Flanagan: simply they're not connected to our internal data.\\r\\n38:46\\r\\nCathal Flanagan: which is the most important stuff. Right? That's what's key to actually anything we want might want to do with these models at work right? So out of the box.\\r\\n38:48\\r\\nCathal Flanagan: that's an inherent limitation.\\r\\n38:59\\r\\nCathal Flanagan: We can solve that. We can solve that by giving the models, tools, and access to our internal data. And you're going to see us do that in this class, right? Fundamentally, that's a concept that you're going to become sick of me, talking about which is called retrieval augmented generation. I'll talk about that in just a moment.\\r\\n39:01\\r\\nCathal Flanagan: And then the 3rd reason that we don't want to use these models\\r\\n39:19\\r\\nCathal Flanagan: out of the box in the Enterprise. Setting is\\r\\n39:24\\r\\nCathal Flanagan: hallucinations, and I suspect most folks here have heard of that term which is really just a fancy way of saying the models\\r\\n39:29\\r\\nCathal Flanagan: they just make stuff up.\\r\\n39:37\\r\\nCathal Flanagan: And why do they make stuff up?\\r\\n39:38\\r\\nCathal Flanagan: Because they have been trained to be people pleasers. They have been trained to give you a answer, even if it is not the right answer.\\r\\n39:40\\r\\nCathal Flanagan: And empirically, research shows that you know, it happens about 3% of the time.\\r\\n39:50\\r\\nCathal Flanagan: which, if the answers you're getting in an enterprise setting are wrong, 3% of the time.\\r\\n39:56\\r\\nCathal Flanagan: No bueno, right? That's just not. It's just way too high.\\r\\n40:02\\r\\nCathal Flanagan: So we're gonna talk in this class over the coming weeks around techniques\\r\\n40:06\\r\\nCathal Flanagan: to reduce that right. Some of that can be done through what's called prompt engineering, which we'll talk about.\\r\\n40:10\\r\\nCathal Flanagan: Some of it can be done by\\r\\n40:17\\r\\nCathal Flanagan: having systems that are explicitly looking or double checking\\r\\n40:19\\r\\nCathal Flanagan: for hallucinations, and of course, retrieval of antigeneration. It's certainly, you know, being very narrow, and what you're passing is the model with clear instructions can also help with that so hallucinations, how we avoid them, how we reduce them is going to be a big topic.\\r\\n40:24\\r\\nCathal Flanagan: So what I like to say is\\r\\n40:42\\r\\nCathal Flanagan: in the enterprise. We use the models for their skills and not their knowledge rush.\\r\\n40:45\\r\\nCathal Flanagan: Yes, in our personal lives we use them for their knowledge, not in the enterprise\\r\\n40:52\\r\\nCathal Flanagan: skills. So what skills do they have?\\r\\n40:56\\r\\nCathal Flanagan: They understand language, they understand natural language. They have an understanding of how to complete tasks\\r\\n41:00\\r\\nCathal Flanagan: when given them, and given information or given text largely.\\r\\n41:07\\r\\nCathal Flanagan: So they have skills that are very useful to us.\\r\\n41:12\\r\\nCathal Flanagan: But it's very important that we provide the knowledge on which the model reasons on top of\\r\\n41:15\\r\\nCathal Flanagan: the way that we use Llms in the enterprise breaks down into 3 these 3 different areas\\r\\n41:25\\r\\nCathal Flanagan: I already mentioned. You're going to be really sick of me using the word rag for the next 8 weeks. But it is going to be the dominant key term retrieval augmented generation.\\r\\n41:33\\r\\nCathal Flanagan: What does that mean it means.\\r\\n41:44\\r\\nCathal Flanagan: And when you ask a question, instead of actually\\r\\n41:48\\r\\nCathal Flanagan: immediately trying to answer the question, the 1st step is\\r\\n41:52\\r\\nCathal Flanagan: the most relevant information from data sources, whatever those data sources are, are retrieved\\r\\n41:58\\r\\nCathal Flanagan: by a system that you or your engineers, or someone else has to manage right.\\r\\n42:05\\r\\nCathal Flanagan: But there has to be a knowledge base that could be a single Pdf.\\r\\n42:11\\r\\nCathal Flanagan: it could be a search index of, you know, tens of millions of documents, right? That's ultimately what enterprises are going to end up\\r\\n42:16\\r\\nCathal Flanagan: building for themselves right knowledge bases.\\r\\n42:24\\r\\nCathal Flanagan: But wherever that is, it's going to retrieve relevant text from that knowledge base.\\r\\n42:28\\r\\nCathal Flanagan: And taking that text and pass it again to the model\\r\\n42:33\\r\\nCathal Flanagan: with your original question or your command.\\r\\n42:38\\r\\nCathal Flanagan: And that's what the model will use to actually answer the question, or to complete a task.\\r\\n42:42\\r\\nCathal Flanagan: Some of those tasks are QAA lot of them are summarization and comparison, which is the second\\r\\n42:48\\r\\nCathal Flanagan: chart here.\\r\\n42:55\\r\\nCathal Flanagan: So summarization. I think you might think about it as taking one document down to 5 bullet points. That's how a lot of people who use these.\\r\\n42:56\\r\\nCathal Flanagan: But I've also seen people take 20,000 documents and reduce them down to one page.\\r\\n43:04\\r\\nCathal Flanagan: Right?\\r\\n43:09\\r\\nCathal Flanagan: You know that summarization, too, in a different form.\\r\\n43:11\\r\\nCathal Flanagan: Comparison across where you're coming.\\r\\n43:16\\r\\nDima Timofeev: Certainly, could you please walk us through the labels on the images, because we cannot see them on the slide.\\r\\n43:19\\r\\nCathal Flanagan: Oh, you cancel. Okay, let me load them up here.\\r\\n43:28\\r\\nTony A: It's the resolution more than anything.\\r\\n43:34\\r\\nCathal Flanagan: Oh, really, okay, sorry about that.\\r\\n43:36\\r\\nCathal Flanagan: Here we go.\\r\\n43:39\\r\\nCathal Flanagan: Well, let's let's do this one, because it's really the most important one.\\r\\n43:40\\r\\nCathal Flanagan: So this going back to retrieval augmented generation and order indifferent.\\r\\n43:44\\r\\nDima Timofeev: Or guys, it doesn't matter too much, because we will try all these techniques later, and you will have a very good mental representation.\\r\\n43:52\\r\\nCathal Flanagan: Yeah, trust me, your guys are, gonna guess quite sick of these.\\r\\n44:02\\r\\nCathal Flanagan: User asks a question.\\r\\n44:07\\r\\nCathal Flanagan: The query is that a query is formed.\\r\\n44:09\\r\\nCathal Flanagan: That is a query of a knowledge base. And again.\\r\\n44:13\\r\\nCathal Flanagan: those knowledge bases will be all those enterprise sas, tools that use every day salesforce service. Now,\\r\\n44:18\\r\\nCathal Flanagan: you know, sharepoint.\\r\\n44:26\\r\\nCathal Flanagan: You know, really any knowledge base that contains textual information within the enterprise\\r\\n44:29\\r\\nCathal Flanagan: either has a service or will have a service very soon that allows you to point an A an AI system at it.\\r\\n44:34\\r\\nCathal Flanagan: If you're retrieving that text\\r\\n44:42\\r\\nCathal Flanagan: and then passing that in to the model to actually do something with it right? That's a lot of times that's us answering questions sometimes. It's, you know, summarizing. Compare, comparing, etcetera.\\r\\n44:45\\r\\nCathal Flanagan: So that's kind of that list out the graphic one and then graphic. 2.\\r\\n44:56\\r\\nCathal Flanagan: Oh, sorry. This is.\\r\\n45:03\\r\\nArun Tyagi: So the systems we carry and the data they hold there has to be structured data, or it can be unstructured. They can make.\\r\\n45:06\\r\\nCathal Flanagan: So just let's define what structured and unstructured data are.\\r\\n45:12\\r\\nCathal Flanagan: no, I'm I'm gonna do it. So search your data is what you would find in Excel, you know, rows and columns\\r\\n45:17\\r\\nCathal Flanagan: on structured data is text largely okay. So language models work with text.\\r\\n45:26\\r\\nCathal Flanagan: can they work with structured data. Yes.\\r\\n45:35\\r\\nCathal Flanagan: but in very specific ways. So we're going to talk about that\\r\\n45:38\\r\\nCathal Flanagan: as a later on the class where, when we wanted to work with structured data and do data analysis, we have to take an extra step, which is, we have to give the model the ability to write and execute code. Okay. But for the vast vast majority of tasks, especially in the early on in the enterprise. Using these, adopting these, you're going to be using them over unstructured data. When you hear unstructured data, just think text.\\r\\n45:42\\r\\nDima Timofeev: Certainly, could you please? We have\\r\\n46:07\\r\\nDima Timofeev: multiple questions in the chat, but they all distill to the same notion. Could you please give us a quick review in couple of sentences. It's not when we're talking about track. It's actually not a simple queries, right? It's not just simple lookups. It's a bit more complex thing, right?\\r\\n46:10\\r\\nCathal Flanagan: Yes, though, you again.\\r\\n46:31\\r\\nCathal Flanagan: We're going to get very hands on. You guys are going to be building rag systems. In the coming classes. So\\r\\n46:34\\r\\nCathal Flanagan: you know, if you don't have an intuition for right now, you will very soon. Ultimately think of it like this\\r\\n46:41\\r\\nCathal Flanagan: if you want to query a, if you if you ask a question, a natural language question.\\r\\n46:48\\r\\nCathal Flanagan: Such as you know, what is the you know? Write a research report on Nvidia.\\r\\n46:56\\r\\nCathal Flanagan: Well, a research report, Nvidia. What do you actually, what what would you need to do there? You need to actually find information on Nvidia from a different date from a data source. So maybe you actually would do a search on Wikipedia which is getting.\\r\\n47:07\\r\\nCathal Flanagan: you know, basic information about Nvidia. Maybe you might do a search on\\r\\n47:22\\r\\nCathal Flanagan: the Sec website to retrieve the latest 10 k. Right?\\r\\n47:28\\r\\nCathal Flanagan: So for each of those systems, you would actually form a slightly different query for calling Wikipedia.\\r\\n47:33\\r\\nCathal Flanagan: you might say. You know Wikipedia or Nvidia history\\r\\n47:40\\r\\nCathal Flanagan: for calling the Sec websites, you might say Nvidia, latest 10 k.\\r\\n47:45\\r\\nCathal Flanagan: Right\\r\\n47:50\\r\\nCathal Flanagan: each of these systems. The way you interact with them is generally through what's called an Api. And again, you're new to coding. This will because this will become clear in future classes. But you basically make a call to these systems, and they're retrieving a certain subset of information for you. You're using that subset of information, and then passing that and only that, into the model, along with your query or or command.\\r\\n47:51\\r\\nCathal Flanagan: And the model is using its reasoning ability\\r\\n48:17\\r\\nCathal Flanagan: to answer that question or produce that report for you. But you are very. You are responsible\\r\\n48:20\\r\\nCathal Flanagan: for retrieving the information that the model should use.\\r\\n48:28\\r\\nCathal Flanagan: Right. That's the key difference of using Llms in the Enterprise versus an Llms.\\r\\n48:32\\r\\nCathal Flanagan: More protein.\\r\\n48:40\\r\\nCathal Flanagan: And again, once we get hands on the concepts of rag will become very very clear, awesome.\\r\\n48:43\\r\\nCathal Flanagan: Okay? So the mission of our class\\r\\n48:54\\r\\nCathal Flanagan: is very much to lower the barrier to use Llm tools in everyday business settings. So we've just talked about\\r\\n48:59\\r\\nCathal Flanagan: why these systems are different and how we use them in the enterprise.\\r\\n49:07\\r\\nCathal Flanagan: So this class is all about helping you build some\\r\\n49:14\\r\\nCathal Flanagan: tools and some intuition around how to identify opportunities within\\r\\n49:18\\r\\nCathal Flanagan: your business setting of, you know, opportunities for using Lm specifically, AI broadly\\r\\n49:24\\r\\nCathal Flanagan: and then help. You have the code to build a prototype to actually get something up and running and again helping you understand, you know what type of\\r\\n49:31\\r\\nCathal Flanagan: situations the models can actually add value, and where it might still be. Too. Too hard.\\r\\n49:41\\r\\nCathal Flanagan: The values of the class are very much focused, as I hope you can tell already around supporting those who are just starting out. We really like to see folks who have very little knowledge\\r\\n49:49\\r\\nCathal Flanagan: on both either AI or Python coming to the class because you're going to get a lot from it. But we really really want to support you in that journey.\\r\\n50:01\\r\\nCathal Flanagan: We focus on the practical. Every class. We will have code.\\r\\n50:10\\r\\nCathal Flanagan: If you have never quoted before.\\r\\n50:15\\r\\nCathal Flanagan: It's not a problem. We're gonna have\\r\\n50:18\\r\\nCathal Flanagan: a python boot camp will get you up to speed. You should at least be able to write the code. To be quite frank with you, there's never been a better time not to know how to code, because right. You can have chat, Gpt, write most of it for you, and generally do it quite well.\\r\\n50:20\\r\\nCathal Flanagan: So\\r\\n50:36\\r\\nCathal Flanagan: don't be concerned if you. I know we have a few folks who just haven't coded before. At the very least, you'll be able to recode, make small edits and gain some intuition around what's actually occurring. We're really leaving you with practical skills is very important. And then, of course.\\r\\n50:37\\r\\nCathal Flanagan: I hope we've already started doing it. But really we want to make it interesting. We do have some folks who were coded for many years, and are very familiar with all the terms I've just mentioned, and many more we will talk about.\\r\\n50:54\\r\\nCathal Flanagan: I hope, for those folks that we are providing useful and interesting knowledge and insights along the way. There's a lot going on in the community. This is a great place to both. Keep up with this and discuss this with with colleagues\\r\\n51:06\\r\\nCathal Flanagan: and the, you know values of the class.\\r\\n51:18\\r\\nCathal Flanagan: as you can tell you know. Feel free. It's a large class, so\\r\\n51:21\\r\\nCathal Flanagan: unfortunately, is it not? Doesn't lend itself as well to large debates on Zoom. But we will have forums where we want you to come in and actually talk during the weekend and office hours. Ideate.\\r\\n51:25\\r\\nCathal Flanagan: you know, debate ideas, questions have fun and really get involved. As I mentioned, it's it. This is\\r\\n51:37\\r\\nCathal Flanagan: a class that you will get as much out of it as you put into it.\\r\\n51:44\\r\\nCathal Flanagan: We have Dima and Anya who will be staffing our slack channels for answering those some of those questions for you I will try and answer questions. Live as there, as Dima points them out. If we don't have an answer to a very specific question, we'll find us. I,\\r\\n51:48\\r\\nCathal Flanagan: because of the size of the class and my proclivity to go down wormholes of answer. When I'm answering questions we'll try and push off most questions to the breaks, or at the end of the class or beginning of class. So starting next week, we'll start doing office hours\\r\\n52:07\\r\\nCathal Flanagan: for at least the 30 min before class, if not the hour before class depending on demand.\\r\\n52:22\\r\\nCathal Flanagan: So you'll have lots of opportunities to, you know. Clarify things, ask questions, and, you know, make sure that you're fully understanding everything.\\r\\n52:27\\r\\nCathal Flanagan: And we sent out a survey pre class just to kind of get a sense for folks.\\r\\n52:38\\r\\nCathal Flanagan: I mentioned this at the beginning. But only 8% of folks have actually taken a class before. So\\r\\n52:45\\r\\nCathal Flanagan: 92% of folks are relatively new, so I'm sorry you have to listen to me for 8 weeks, but\\r\\n52:53\\r\\nCathal Flanagan: thank you for those who are returning. It's nice to always have alumni in the class. Really, it's a great community we've already formed.\\r\\n53:00\\r\\nCathal Flanagan: And then.\\r\\n53:09\\r\\nCathal Flanagan: you know, this is the distributions we're seeing of where folks are on a technical spectrum. So we have 8 folks who've probably never written a line of python in their lives. We have 13 folks who probably write python all day long.\\r\\n53:10\\r\\nCathal Flanagan: And then we have\\r\\n53:23\\r\\nCathal Flanagan: folks who are at different levels of familiarity with AI and Llm. Terminology. So for some folks\\r\\n53:25\\r\\nCathal Flanagan: for 18 folks, when I totally said retrieval, augmented generation.\\r\\n53:32\\r\\nCathal Flanagan: they had no idea what we're talking about. And for night folks they were like, yeah, that's you know.\\r\\n53:36\\r\\nCathal Flanagan: that's easy.\\r\\n53:42\\r\\nCathal Flanagan: So we want to support folks across the spectrum. We realize there's a broad array of of\\r\\n53:44\\r\\nCathal Flanagan: skills and talents and one way that we want to support. That is the introduction of a buddy system. And so for those folks who are fives\\r\\n53:50\\r\\nCathal Flanagan: in either of the 2 categories or even fours. You know, we want you to sign up to be a buddy and we want folks who are in the ones categories of the twos who would like\\r\\n54:00\\r\\nCathal Flanagan: someone to help ask them questions and help them with homework. And you know, just throughout the week, kind of\\r\\n54:12\\r\\nCathal Flanagan: bounce ideas or questions off we wanna pair you up. So of course it's opt in we will be sending around as a recap a link to a spreadsheet where you can sign up to be a buddy, and you can sign up for a buddy. So\\r\\n54:19\\r\\nCathal Flanagan: I hope you do that for those who sign up to be a buddy we will make sure there's a reward in it for you swag or something something else. So definitely, if you have the skills and you're a 4 or 5 in either of the categories. Please do sign up for the buddy program.\\r\\n54:36\\r\\nCathal Flanagan: And so yes, Dima.\\r\\n54:55\\r\\nDima Timofeev: Could you please address the Corgi question?\\r\\n54:59\\r\\nCathal Flanagan: The Corgi question is there? I didn't. I can't see the Corgi question. What is it.\\r\\n55:02\\r\\nDima Timofeev: I why do we have Corgis everywhere.\\r\\n55:05\\r\\nCathal Flanagan: All right.\\r\\n55:11\\r\\nCathal Flanagan: So for those folks who are not\\r\\n55:12\\r\\nCathal Flanagan: dog fans or corgi fans. Probably the best reason why you should probably ask for a refund this class is.\\r\\n55:15\\r\\nCathal Flanagan: we're we are Cornegie people in this class. So why? Because Corey's are adorable.\\r\\n55:23\\r\\nCathal Flanagan: and yes, so Corgis are the mascot of the class. You will see them often and everywhere. So yeah. Get used to it.\\r\\n55:30\\r\\nCathal Flanagan: Jeremy asks. Office hours will always be in 30 min before class. Yes, Jeremy, for sure, definitely 30 min for class. In some cases we will do an hour, because there'll be certain topics that people will want to discuss that aren't in the main syllabus. And I'll basically use the office hours in before class to teach a little workshop or boot camp on that topic. So yes, but everything is recorded, everything available. There's no obligation to attend just that.\\r\\n55:43\\r\\nCathal Flanagan: But yeah, we will. You should always expect at least 30 min before class, where we'll have some sort of activity for those who want to join.\\r\\n56:12\\r\\nCathal Flanagan: We have a very, very busy 8 weeks ahead of us.\\r\\n56:22\\r\\nCathal Flanagan: you know, we are going to talk about the introduction.\\r\\n56:25\\r\\nCathal Flanagan: the Nlp, we're gonna talk about\\r\\n56:29\\r\\nCathal Flanagan: in week. 2 frameworks advanced civilization, advanced prompt engineering.\\r\\n56:32\\r\\nCathal Flanagan: We're gonna introduce Ryke week 3\\r\\n56:37\\r\\nCathal Flanagan: is going to be pretty much all rag all the time. So you know, that's really Rag is going to dominate because it is a large part of it.\\r\\n56:41\\r\\nCathal Flanagan: In week 4, we're going to talk about open source models. In fact, we're going to tune an open source model.\\r\\n56:50\\r\\nCathal Flanagan: The Gemma model from Google, specifically.\\r\\n56:58\\r\\nCathal Flanagan: we are going to talk about\\r\\n57:01\\r\\nCathal Flanagan: agents at that focus. So the 1st part is, you know building simple systems. And then the second half of the semester is very focused on agentic behavior. So we'll be building using a framework called Crew AI.\\r\\n57:04\\r\\nCathal Flanagan: We'll also, of course, talk about fine tuning models. We'll talk about\\r\\n57:19\\r\\nCathal Flanagan: benchmarks. Evaluation, right? So I showed you some benchmarks already. What do they mean? How should you interpret them? Why, most benchmarks are probably useless going forward.\\r\\n57:24\\r\\nCathal Flanagan: And then week 7, we'll talk about\\r\\n57:35\\r\\nCathal Flanagan: beyond text multimodal inputs, image generation video.\\r\\n57:38\\r\\nCathal Flanagan: We'll even have a guest speaker who will\\r\\n57:43\\r\\nCathal Flanagan: teaches about audio generation. And how you can actually use that to create pretty amazing sounds.\\r\\n57:46\\r\\nCathal Flanagan: And then the last week we hand over the system or the microphone to you guys\\r\\n57:55\\r\\nCathal Flanagan: we will give you an opportunity to present your final projects. So.\\r\\n58:00\\r\\nCathal Flanagan: as you'll see in a moment. When I talk about how to get credit for the class, we will require final push from you, and for a subset of folks. We're going to ask you to volunteer to present your final project, and for me and for most folks this is actually the most fun night of the entire semester. We try and turn it into a virtual party.\\r\\n58:04\\r\\nCathal Flanagan: You get to see what amazing things your college colleagues have built. But not only that you will get all the code. We want everything to everyone to be able to access everything from everybody else in this class, so you'll be able to actually see exactly how they built it, and maybe take it and tune it just for the way you want it. So that's kind of\\r\\n58:22\\r\\nCathal Flanagan: a overview of the semester. As it exists today we will have\\r\\n58:41\\r\\nCathal Flanagan: different guest speakers popping in throughout the semester on who are experts in different topics. So it should be pretty pretty interactive. Pretty interesting.\\r\\n58:46\\r\\nCathal Flanagan: throughout the issues.\\r\\n58:57\\r\\nCathal Flanagan: What we will not cover. We do not like math. Well, that's not true, but math underlies everything, of course, in both deep learning and Llms.\\r\\n59:00\\r\\nCathal Flanagan: However.\\r\\n59:12\\r\\nCathal Flanagan: you've probably heard that famous saying that for every equation you show you lose half the audience. Therefore I don't want to lose you. I want to help make help build intuition, so we will not be using math or equations. There is a lot\\r\\n59:14\\r\\nCathal Flanagan: of fundamental map behind.\\r\\n59:28\\r\\nCathal Flanagan: These systems. If that's interesting for you, we can definitely talk about it in office hours. We can definitely\\r\\n59:34\\r\\nCathal Flanagan: point you in the right direction. There's really good courses on that. But this class\\r\\n59:42\\r\\nCathal Flanagan: course, we're focused on the practical doing useful things with the systems as they exist today.\\r\\n59:47\\r\\nCathal Flanagan: Also limited theory under underlying models. They're fascinating. We're gonna do paper clubs. We're gonna talk through\\r\\n59:53\\r\\nCathal Flanagan: the foundational papers of of these systems. But in the main class it's not necessary to do useful things with the models. So we're going to be not teaching the theory of the underlying models, with the maybe the exception of a little bit today.\\r\\n1:00:00\\r\\nCathal Flanagan: And we will not be teaching you how to build your own foundational Llm.\\r\\n1:00:15\\r\\nCathal Flanagan: Even though deep seek is making it very affordable. I I don't expect most folks will want to spend\\r\\n1:00:20\\r\\nCathal Flanagan: several 1 million dollars training their own Llm. So\\r\\n1:00:27\\r\\nCathal Flanagan: that will not be the focus of the class, though we can point you in the direction of some really great resources towards that as well. If that's of interesting to you. What we will talk about is fine tuning. Take an existing Llm. And tuning it\\r\\n1:00:31\\r\\nCathal Flanagan: for your purposes. That will come in the fine tuning class\\r\\n1:00:43\\r\\nCathal Flanagan: formal of the class. Unfortunately for you, it's mostly me talking at you. I'm sorry about that, but that's just the nature of\\r\\n1:00:50\\r\\nCathal Flanagan: where it has to go.\\r\\n1:00:57\\r\\nCathal Flanagan: we'll pause for questions at different points of time, as Tima is\\r\\n1:00:59\\r\\nCathal Flanagan: putting his hand up. If and as there is a question that I should address please do use, slack as you are, and continue to ask questions there and then we'll always stick around after class. If people have questions.\\r\\n1:01:04\\r\\nCathal Flanagan: or want to want to discuss anything that was brought up within the class.\\r\\n1:01:16\\r\\nCathal Flanagan: I've alluded to this, but outside of the primary class. We have lots of fun activities that we hope you will take part in.\\r\\n1:01:23\\r\\nCathal Flanagan: As I mentioned, we're going to have pre-class office hours at least in the 30 min before. But in a lot of cases we'll be doing one hours\\r\\n1:01:31\\r\\nCathal Flanagan: sessions where we'll be doing kind of talks.\\r\\n1:01:38\\r\\nCathal Flanagan: talking about other things that are not directly on the curriculum.\\r\\n1:01:41\\r\\nCathal Flanagan: On Saturdays or Sundays, depending on kind of the weekend. We will be running office coffee hours, which is a forum, where literally a group of us just drop, jump on\\r\\n1:01:44\\r\\nCathal Flanagan: a zoom, have coffee or tea, and just talk about anything. It does not even need to be Llms. So it's good form for folks to spend some time chat about ideas over over the weekends.\\r\\n1:01:56\\r\\nCathal Flanagan: and we also have paper club, and I have here volunteers needed. But I don't actually need to, because we already have a volunteer. So Richard, tell us about the the paper club.\\r\\n1:02:10\\r\\nRichard Ryan: The paper club was a big success in last summer's class. We started it a little late late in the the semester, but once it got going, everyone really enjoyed it, and so I think I think we should start it right away, meaning this Saturday the best time it seemed like.\\r\\n1:02:23\\r\\nRichard Ryan: for most of the people was 10 Pm. Pacific standard time on Saturdays, and if that time works for everyone, and and Dima and Charlie joined. I think, every of our paper clubs last summer, and made them really wonderful. And so\\r\\n1:02:47\\r\\nRichard Ryan: I would suggest that we keep to that time. I'm seeing lots and lots and lots of people in the Bay Area and we've got somebody who's already volunteered to run the Bay area meetups, which I think would also be a cool place for us all to get together. But for the paper clubs.\\r\\n1:03:10\\r\\nRichard Ryan: What I would suggest is that this Saturday at 10 Pm. We do. Attention is all you need, which is.\\r\\n1:03:33\\r\\nCathal Flanagan: 10 am. Today.\\r\\n1:03:40\\r\\nRichard Ryan: Foundational paper. And I put in both the General, and there's a the channel called cool papers in our slack our our slack instance.\\r\\n1:03:41\\r\\nRichard Ryan: and you'll find both the original paper on Archive. You'll find a Google illuminate chat which Charlie has provided, which kind of walks you through the ideas behind the paper. And then, 7 years after this paper, that kind of changed\\r\\n1:03:55\\r\\nRichard Ryan: the Llm.\\r\\n1:04:15\\r\\nRichard Ryan: Machine learning world. There was a in Nvidia group panel discussion, and I posted that as well. So there's a lot of really cool information, and if people are interested we could do it this Saturday at at 1010 Am. Pst.\\r\\n1:04:17\\r\\nCathal Flanagan: Perfect. Alright. Well, we'll include communications about that on the recap. So there you go. 1st paper club meeting already organized. Love it. I will be there myself. And Dean are nerds, too, so we like to show up to the paper clubs, and and at least listen or or participate. So pretty fun. Thank you, Richard. We appreciate you you leaving this effort again. And as you said, it was good success last time.\\r\\n1:04:39\\r\\nRichard Ryan: My pleasure. Thank you.\\r\\n1:05:05\\r\\nCathal Flanagan: Thank you.\\r\\n1:05:06\\r\\nCathal Flanagan: Okay, to get the course, grade or credit. I've already kind of mentioned this.\\r\\n1:05:07\\r\\nCathal Flanagan: You will have 6 homeworks throughout this class. You must submit a minimum of 4 of them\\r\\n1:05:12\\r\\nCathal Flanagan: and a final project to get credit or a grade in the class.\\r\\n1:05:18\\r\\nCathal Flanagan: The final project will be to use some of the tools that we cover and solve a problem or automate a task\\r\\n1:05:24\\r\\nCathal Flanagan: or create a new capability for you\\r\\n1:05:31\\r\\nCathal Flanagan: within your business life or your personal life right? And then\\r\\n1:05:35\\r\\nCathal Flanagan: write a do a page, 2. Page. Write up or give a 3 min presentation in class. The good news for you is\\r\\n1:05:39\\r\\nCathal Flanagan: homeworks are as difficult as you want them to be. For folks who haven't coded before. You don't need to worry. We provide starter codes for everything. So if all you need to do is\\r\\n1:05:48\\r\\nCathal Flanagan: run it, which is pretty straightforward, and\\r\\n1:06:00\\r\\nCathal Flanagan: maybe make some slight edits. There are other folks who are more advanced, and they will want to take that starter code and\\r\\n1:06:04\\r\\nCathal Flanagan: make it amazing and build something like.\\r\\n1:06:10\\r\\nCathal Flanagan: yeah, incredible with us. So whatever level you're off. The homework is there to help support you?\\r\\n1:06:14\\r\\nCathal Flanagan: So you do not need to be intimidated by it. But we do want to see you participate if you want credit or grade. So that is the requirement for homeworks and a final project.\\r\\n1:06:21\\r\\nYankai Su: And.\\r\\n1:06:34\\r\\nCathal Flanagan: How do you actually do this? How do we actually\\r\\n1:06:35\\r\\nCathal Flanagan: code and kind of share code with each other and actually use it for us. We will be using a tool called collap from Google.\\r\\n1:06:38\\r\\nCathal Flanagan: Dima, maybe give us like a 3 min overview, if you don't mind of what colab is.\\r\\n1:06:50\\r\\nDima Timofeev: With great pleasure, and let me try to share\\r\\n1:06:58\\r\\nDima Timofeev: a tab with you guys. I'm.\\r\\n1:07:03\\r\\nCathal Flanagan: While while you're doing that, I have. Do we have a question from Yankee.\\r\\n1:07:12\\r\\nCathal Flanagan: I apologize. If I'm mispronounce your name.\\r\\n1:07:16\\r\\nYankai Su: Yeah, yeah, I have a quick question regarding like the credit. So this credit is it count as a college course, credit or or no?\\r\\n1:07:18\\r\\nCathal Flanagan: No, it's if you want to get this, the certificate, or the kind of\\r\\n1:07:29\\r\\nCathal Flanagan: the transcript after the after the class.\\r\\n1:07:36\\r\\nCathal Flanagan: But no no crash.\\r\\n1:07:40\\r\\nSarit Arora: I. I had one question, I think in terms of the overall syllabus would we be covering some aspect of guardrails that we as\\r\\n1:07:42\\r\\nSarit Arora: creator of these applications, need to put in like, for example, I think there's a lot of concern related to the data it gets trained on or citing the sources, or when a user is using it for their application, how much they are aware of, where the data or the source is coming from? How can we help on that regard when we are building these applications.\\r\\n1:07:52\\r\\nCathal Flanagan: So that'll come up as part of the retrieval augmented generation, because we are very much in a trust, would verify phase with all of these models. So citing sources is key. The good news is\\r\\n1:08:13\\r\\nCathal Flanagan: oftentimes citing sources is as easy as just in prompt engineering the model to actually cited sources.\\r\\n1:08:25\\r\\nCathal Flanagan: Assuming that you've provided the source information as part of the retrieved content which you know we'll talk about it. We'll show exactly how to do that in the retrieval augmented generation. Sections of success.\\r\\n1:08:32\\r\\nSarit Arora: Yeah. Sounds. Good. Yeah. Thanks.\\r\\n1:08:43\\r\\nCathal Flanagan: Richard, sorry you had another point.\\r\\n1:08:46\\r\\nRichard Ryan: I just I actually wanted to follow up on on, I think very well taken question about guardrails and cyber security, and those of us who are interested both in the rewards and in the risks\\r\\n1:08:49\\r\\nRichard Ryan: of of Llms and Ml. In business. This this question of of cyber security for Llms is great, and and so it I would suggest, if you have strong opinions about this, or want to make a contribution to the class maybe start a slack channel on this if we don't have one already on security issues.\\r\\n1:09:06\\r\\nSarit Arora: Sounds good. Actually, we'll do. Yeah, thanks.\\r\\n1:09:36\\r\\nCathal Flanagan: Alright, Dana! Show us what Collab is.\\r\\n1:09:41\\r\\nDima Timofeev: Can you hear me?\\r\\n1:09:44\\r\\nCathal Flanagan: We can.\\r\\n1:09:46\\r\\nDima Timofeev: Perfect. Let me quickly show you the environment that we're going to use for this class. This is very similar to what you can get when you use pure python from your terminal. But it has a very nice ui. How it works very simple. Probably the concepts that you need to know well is a cell. So you see, I'm typing something right here in the cell, and every cell can execute a particular python command. For example, I can say, print, Hello, world!\\r\\n1:09:47\\r\\nDima Timofeev: And this thing they call up environment right in your browser. You don't need to install anything. You don't need to use anything extra will start running. Python commands in a second. You can see in this right corner, right here. It's connecting. The key question is where it's connecting Google is very kind, and they provide virtual computers, virtual machines for all of us in a way that we can run any code right there.\\r\\n1:10:17\\r\\nDima Timofeev: And this is exactly what we're gonna use, because in this case we can guarantee that everybody has the same environment, and it's reproducible for all of us. You can do mathematics. You can do any kind of python. For example, I can say I can calculate any mathematical equations. I can do any sophisticated stuff right here. So it's pretty much just python\\r\\n1:10:46\\r\\nDima Timofeev: that you can fully utilize and see what is going on inside.\\r\\n1:11:12\\r\\nDima Timofeev: The quick thing that I want to show you about this environment, and Charlie is going to use it through the entire class. What we're going to do each of us is going to use Collab to do our experiments and to do our homeworks, and we're going to share links to those collabs with each other, and your homework is going to be the link.\\r\\n1:11:20\\r\\nDima Timofeev: That is right here, please, whenever you share your whole work with us, make sure that you press the button, share and make make it publicly accessible, so please don't share any private data within it. You might have some questions about managing your Api key to Openai, etc. But Charlie will address it later\\r\\n1:11:39\\r\\nDima Timofeev: there are a few more features that I would like to show you within the call app. So\\r\\n1:11:57\\r\\nDima Timofeev: 1st of all, I, if you learn this class for the 1st time\\r\\n1:12:03\\r\\nDima Timofeev: I want you. So if there is one thing that you need to get from this class like single thing that, you need to learn. So please remember it carefully. It's the most important part of about this class. There is a very important setting that you need to set up. You need to go to the settings.\\r\\n1:12:09\\r\\nDima Timofeev: You need to go to this section, and you need to pick your poison.\\r\\n1:12:27\\r\\nDima Timofeev: Since Cheryl already mentioned, you're not allowed to pick kitty or crab mode you're allowed to pick only Corgi mode.\\r\\n1:12:32\\r\\nDima Timofeev: What does it mean when you enable it? And when you code, you're gonna be entertained by nice corgis running around your screen.\\r\\n1:12:38\\r\\nDima Timofeev: So please choose wisely. But please choose squarely.\\r\\n1:12:47\\r\\nDima Timofeev: So, jokes aside. There is one extra hint about this environment.\\r\\n1:12:51\\r\\nDima Timofeev: We are gonna use primarily calls to large language models to different Apis. What does it mean? It means that we don't need very powerful machine.\\r\\n1:12:56\\r\\nDima Timofeev: however, for some of you who would like to go as advanced as possible and want to use some sophisticated machine learning algorithms which require distributed compute.\\r\\n1:13:07\\r\\nDima Timofeev: Again, Google is very kind\\r\\n1:13:17\\r\\nDima Timofeev: and they provide us with more powerful machines for different machine learning workloads. So whenever you see that you\\r\\n1:13:20\\r\\nDima Timofeev: plug in Llm with some other type of machine learning models. And you read, or you learned from somewhere that it's important for you to have Gpu for that. Let me show you how you can easily pick a sophisticated Gpu. Very easy. You can press the small arrow right here.\\r\\n1:13:28\\r\\nDima Timofeev: You can select change from time type right here.\\r\\n1:13:47\\r\\nDima Timofeev: And right here you see that even if this is absolutely it's my personal free profile I can pick.\\r\\n1:13:53\\r\\nDima Timofeev: Gpu!\\r\\n1:14:02\\r\\nDima Timofeev: Powerful Nvidia Gpus, or I can even take extremely powerful Google Tpus.\\r\\n1:14:04\\r\\nDima Timofeev: So these are coming for free co-OP is free. These computational resources are also free. If some of you would like to reproduce everything locally or on local lamps.\\r\\n1:14:11\\r\\nDima Timofeev: Yada, Yada, I'm here to help you with that, however, for the majority of students we highly recommend to use Collab share with Collab, and after every single exercise Charlie will share similar link to Collab that you can just open and run every cell. What does it mean to run the cell. As you see, I have 1, 2, 3, 4, 5 cells, and when I press this run button.\\r\\n1:14:22\\r\\nDima Timofeev: I run this cell. You can execute cells in any sequence. So I can execute this cell first, st and this second and this 4.th You will see when you're working with Collab, that very often you will make a mistake because you ran cells in some mixed up sequence.\\r\\n1:14:47\\r\\nDima Timofeev: However. It's super convenient, and it's more than enough for the purposes of our class.\\r\\n1:15:06\\r\\nCathal Flanagan: Awesome. Thank you, Dima. That was very comprehensive.\\r\\n1:15:16\\r\\nCathal Flanagan: Colab is going to be your best friend, and to be clear. Colab is a great tool for teaching, because it allows us all to have the same environment. But it is a very powerful tool that is not just a teaching tool within Google. It's like the primary default\\r\\n1:15:20\\r\\nCathal Flanagan: coding tool for most data scientists. So it's not.\\r\\n1:15:38\\r\\nCathal Flanagan: It's a very powerful is used in business all the time. If your company\\r\\n1:15:41\\r\\nCathal Flanagan: uses workshop work, space by Google. You know, Google, Docs, Google slides, etc. You also have collab at work. You just have to search for sheets within the same ecosystem. If you're in your Google drive, click. New colab should be an option there. Much like a Google, Doc or Google slides exactly the same thing.\\r\\n1:15:47\\r\\nCathal Flanagan: And someone mentioned, it looks like a lot like Jupyter notebooks for those who are aware of it. They are basically Jupyter notebooks just hosted. So makes it easy.\\r\\n1:16:10\\r\\nCathal Flanagan: Okay, I want to talk a little bit about the history of elements. You know, how did we get to where we are today?\\r\\n1:16:21\\r\\nCathal Flanagan: for me, as I think through\\r\\n1:16:33\\r\\nCathal Flanagan: all the images, by the way, are all. Whenever I showed images in the class are all generated by AI.\\r\\n1:16:36\\r\\nCathal Flanagan: It all starts in for me in 2013. When I think about it. You know, we've had neural networks since the 19 fifties.\\r\\n1:16:43\\r\\nCathal Flanagan: and you know, certainly people have done a lot of things with text\\r\\n1:16:51\\r\\nCathal Flanagan: between now and then. But really, if I think about fundamental moments that have led us to where we are today. 2013 stands out specifically because of a breakthrough and a paper around something called word to Vec.\\r\\n1:16:56\\r\\nCathal Flanagan: which is short for word to vector came out of Google, and it introduced word embeddings.\\r\\n1:17:10\\r\\nCathal Flanagan: And this unlocked a whole new set of capabilities that even surprised the researchers.\\r\\n1:17:18\\r\\nCathal Flanagan: Who are involved in the paper.\\r\\n1:17:27\\r\\nCathal Flanagan: So before 2013, the way that we represented words to a machine, we're using ones and zeros.\\r\\n1:17:29\\r\\nCathal Flanagan: Every time a piece of text is passed in to a machine learning algorithm or model.\\r\\n1:17:40\\r\\nCathal Flanagan: it must get represented in a numerical format.\\r\\n1:17:48\\r\\nCathal Flanagan: So you'd always have to translate, for example, the word hotel in a way that kind of is represented here with a recall. When I show you these type of\\r\\n1:17:53\\r\\nCathal Flanagan: in python. You call them a list. But here that we want to think about it as being what we call a vector.\\r\\n1:18:02\\r\\nCathal Flanagan: but I felt effectively like a list of zeros with a 1, the one being, you know, placed to represent that this is within this. Vector this is representing the word hotel.\\r\\n1:18:08\\r\\nCathal Flanagan: The problem was.\\r\\n1:18:19\\r\\nCathal Flanagan: if you had, you know, 2 vectors, one with the word hotel, one with the word motel.\\r\\n1:18:21\\r\\nCathal Flanagan: The placement of the ones actually captured, no similarity between the 2 right? So you and I know\\r\\n1:18:26\\r\\nCathal Flanagan: that the word hotel and motel\\r\\n1:18:35\\r\\nCathal Flanagan: aren't she similar to one another? Not they are. Yes, they're spelled very similarly, but they mean the same thing as well. We were not able to capture that in the numeric representation of words.\\r\\n1:18:37\\r\\nCathal Flanagan: before 2013, word 2 vec.\\r\\n1:18:48\\r\\nCathal Flanagan: Was be 1st breakthrough that actually helped solve that.\\r\\n1:18:53\\r\\nCathal Flanagan: And here's a quiz view.\\r\\n1:18:59\\r\\nCathal Flanagan: So we'll see who wants to be brief.\\r\\n1:19:01\\r\\nCathal Flanagan: The word coast and the word sure.\\r\\n1:19:03\\r\\nCathal Flanagan: Maybe we can use that slack. Are these 2 words similar and similar is a key term.\\r\\n1:19:08\\r\\nCathal Flanagan: not seeing any\\r\\n1:19:22\\r\\nRichard Ryan: Well coast can be a noun or a verb.\\r\\n1:19:24\\r\\nCathal Flanagan: I'm thinking more about semantic meaning. So are they.\\r\\n1:19:28\\r\\nBarkha: In in meaning, like coast and shore are often the same like they mean the same thing right? And so\\r\\n1:19:32\\r\\nBarkha: in terms of vector embeddings on one of those the meaning access. They will be similar. But in terms of actual letters they're not.\\r\\n1:19:41\\r\\nCathal Flanagan: Yeah. So it's lots when we think about similarity. And we talk about text or words, there are different ways. So the spelling, you know specifically the way we would talk about if they were spelt differently, we would talk about the edit distance between them. How many letters do you need to change\\r\\n1:19:51\\r\\nCathal Flanagan: to have them be spelt the same way when we think about words. And if there's similarity, we think more about the I mentioned already, the semantic similarity. Are they referring to the same thing? And someone already mentioned? And Sunil actually has it in the chat\\r\\n1:20:07\\r\\nCathal Flanagan: effectively. They're interchangeable, right? I'm going to the coast. I'm going to the shore, generally referring to the same\\r\\n1:20:24\\r\\nCathal Flanagan: place. Right? Not maybe not exactly, but close enough. Right?\\r\\n1:20:31\\r\\nCathal Flanagan: Okay, we're gonna make it harder.\\r\\n1:20:37\\r\\nCathal Flanagan: The words clothes, and the words closet are these 2 words, similar.\\r\\n1:20:41\\r\\nSunil Singhal: No.\\r\\n1:20:53\\r\\nCathal Flanagan: Robbie says no.\\r\\n1:20:56\\r\\nObinna Anya: No.\\r\\n1:20:57\\r\\nHadeel Ammari: No.\\r\\n1:20:58\\r\\nBanele L: No.\\r\\n1:20:59\\r\\nEsther Teplitsky: No, I would also say Yes.\\r\\n1:21:00\\r\\nNguyen Khoa: And.\\r\\n1:21:02\\r\\nEsther Teplitsky: Yes, yes, yes.\\r\\n1:21:02\\r\\nBanele L: Related.\\r\\n1:21:03\\r\\nBarkha: Yes, less similar, but.\\r\\n1:21:04\\r\\nEsther Teplitsky: So yeah, they're attached. They're used\\r\\n1:21:07\\r\\nEsther Teplitsky: in similar places. They would be using the same content.\\r\\n1:21:09\\r\\nEsther Teplitsky: In similar places.\\r\\n1:21:13\\r\\nCathal Flanagan: Yeah. Yogash says they're related right? There is a relationship here. They're definitely not interchangeable. But there is a relationship. The word close.\\r\\n1:21:16\\r\\nCathal Flanagan: close community clauses. Closet host.\\r\\n1:21:24\\r\\nCathal Flanagan: There's something there's something\\r\\n1:21:27\\r\\nCathal Flanagan: that is making us feel like there's a similarity here, but it's hard to to put into words. Would you say that\\r\\n1:21:30\\r\\nCathal Flanagan: the words close and the words closet are more similar to one another than the words close, and the words laptop, for example.\\r\\n1:21:38\\r\\nJeremy McCormick: Yes, definitely.\\r\\n1:21:47\\r\\nCeren Kasap: Yeah.\\r\\n1:21:48\\r\\nCathal Flanagan: Yeah, right? It's like, intuitively, we feel that right? Like we're having\\r\\n1:21:49\\r\\nCathal Flanagan: trouble even vocalizing it. But we intuitively we know it right?\\r\\n1:21:54\\r\\nCathal Flanagan: So you know.\\r\\n1:22:00\\r\\nCathal Flanagan: if we were to kind of put it on some sort of an axis here, you would definitely say, coast insure in one part, closing closet or another. They're similar, and laptop is somewhere else.\\r\\n1:22:05\\r\\nCathal Flanagan: Right?\\r\\n1:22:15\\r\\nCathal Flanagan: The key inside here is that machines have exactly the same problem.\\r\\n1:22:17\\r\\nCathal Flanagan: Right?\\r\\n1:22:22\\r\\nCathal Flanagan: How do we represent words in a numeric form that is somehow capturing that similarity. Right? But it is a but it's kind of capturing it in a way that it's\\r\\n1:22:23\\r\\nCathal Flanagan: understanding.\\r\\n1:22:34\\r\\nCathal Flanagan: you know coast and shore similar. They're inter. They're they're replaceable. But closing closet are similar. But they're not exactly similar in the same way. Right? Is there any way that we can.\\r\\n1:22:35\\r\\nCathal Flanagan: instead of using zeros and ones that we can capture words\\r\\n1:22:46\\r\\nCathal Flanagan: and their representation. And we were, Cap, we're able to kind of capture this similarity.\\r\\n1:22:50\\r\\nCathal Flanagan: So\\r\\n1:22:57\\r\\nCathal Flanagan: the big insight in the paper and I suspect it should be a paper. Richard, in the paper club.\\r\\n1:22:58\\r\\nCathal Flanagan: Word 2 vec.\\r\\n1:23:05\\r\\nCathal Flanagan: The insight here was, you shall know a word by the company. It keeps price\\r\\n1:23:07\\r\\nCathal Flanagan: instead of just looking at individual words. We can learn a lot about words, and how they should be represented by looking at the words around them.\\r\\n1:23:13\\r\\nCathal Flanagan: Okay, think about the word banking.\\r\\n1:23:22\\r\\nCathal Flanagan: What words are you more likely to see around the word banking compared to other words.\\r\\n1:23:25\\r\\nCathal Flanagan: For example, board banking, often represented by words like government, debt, regulation, crisis, right? Certainly\\r\\n1:23:33\\r\\nCathal Flanagan: more similar than other words.\\r\\n1:23:44\\r\\nCathal Flanagan: So the co-occurrence of words together can actually tell us a lot right?\\r\\n1:23:46\\r\\nCathal Flanagan: And what was really powerful is the way that we actually represent we way that we actually train. These.\\r\\n1:23:52\\r\\nCathal Flanagan: we can actually go from having zeros and ones to having.\\r\\n1:23:58\\r\\nCathal Flanagan: We call them floating point numbers, or we can have numbers of decimal places\\r\\n1:24:05\\r\\nCathal Flanagan: actually replacing the zeros and ones.\\r\\n1:24:09\\r\\nCathal Flanagan: And the crazy thing is\\r\\n1:24:12\\r\\nCathal Flanagan: when we start training this model to understand these words, we start off by giving the words completely random volumes. Okay? So instead of a vector, of zeros and ones, it has, you know, just randomly, randomly numbers.\\r\\n1:24:14\\r\\nCathal Flanagan: But as we're going through the sentence.\\r\\n1:24:29\\r\\nCathal Flanagan: and again, we're feeding in tens of thousands of pieces of text as it's finding words that are co-occurring like the banking word.\\r\\n1:24:32\\r\\nCathal Flanagan: it starts changing the actual values in the vectors for those words, so that those 2 words that are co-occurring together start moving\\r\\n1:24:41\\r\\nCathal Flanagan: or start gaining the same values.\\r\\n1:24:50\\r\\nCathal Flanagan: Okay? And that's all it's doing. There's just moving words that are co-occurring together closer together by changing the vectors. But what that actually looks like.\\r\\n1:24:53\\r\\nCathal Flanagan: And you guys can go to this. I. I encourage you to spend some time on us. This is actually the word effect. These are 10,000. You can choose all 72,000 words that was trained on if you wish.\\r\\n1:25:03\\r\\nCathal Flanagan: But you'd actually see here this, we call this a vector. Space.\\r\\n1:25:14\\r\\nCathal Flanagan: This is where those words, as they were trained, how they finally organized themselves.\\r\\n1:25:20\\r\\nCathal Flanagan: Okay. So we went through all the text, look at the co-occurrence of words, moved words around.\\r\\n1:25:27\\r\\nCathal Flanagan: and then finally ended up with this\\r\\n1:25:35\\r\\nCathal Flanagan: right? And then, if we zoom in, you can certainly see different different words. But then, if we we can start seeing patterns. So, for example, I click on the word precipitation.\\r\\n1:25:37\\r\\nCathal Flanagan: Notice that we're in a part of the vector, space which is talking about volcanoes.\\r\\n1:25:48\\r\\nCathal Flanagan: Petroleum.\\r\\n1:25:53\\r\\nCathal Flanagan: If I move to a different part of the vector space.\\r\\n1:25:55\\r\\nCathal Flanagan: I see cowboys.\\r\\n1:25:57\\r\\nCathal Flanagan: Well, now I've been talking. It's I. It seems to be talking about Nfl, because I see the Nfl, I see hockey. I see championships, I see. Quarterback.\\r\\n1:25:59\\r\\nCathal Flanagan: Right?\\r\\n1:26:07\\r\\nCathal Flanagan: Let's look for some other words. Let's look at the word.\\r\\n1:26:10\\r\\nCathal Flanagan: Let's say London. What what do you think?\\r\\n1:26:14\\r\\nCathal Flanagan: The most similar words, the word London might be.\\r\\n1:26:17\\r\\nMahek Pavagadhi: British.\\r\\n1:26:22\\r\\nCathal Flanagan: British. Any other guesses.\\r\\n1:26:24\\r\\nMadison Van Doren: Bridge.\\r\\n1:26:26\\r\\nCeren Kasap: You can.\\r\\n1:26:29\\r\\nAndrey Skripkin: Big Ben.\\r\\n1:26:31\\r\\nEsther Teplitsky: Okay.\\r\\n1:26:32\\r\\nAndrey Skripkin: Cabbage.\\r\\n1:26:33\\r\\nCathal Flanagan: Right, so interesting. So the la, the most similar words, the word.\\r\\n1:26:34\\r\\nCathal Flanagan: and actually we can see some of them here. The most similar words to the word London, R.\\r\\n1:26:40\\r\\nCathal Flanagan: I'm clearing selection. Landing.\\r\\n1:26:48\\r\\nRichard Ryan: Okay.\\r\\n1:26:51\\r\\nCathal Flanagan: You'll see on the right hand side here, England and Paris.\\r\\n1:26:52\\r\\nCathal Flanagan: Okay, this is interesting.\\r\\n1:26:56\\r\\nCathal Flanagan: What is it? What learning here.\\r\\n1:26:59\\r\\nShane: What a city is.\\r\\n1:27:07\\r\\nCathal Flanagan: What a city is in Paris! Why is it finding Paris.\\r\\n1:27:09\\r\\nYankai Su: Capital.\\r\\n1:27:14\\r\\nCathal Flanagan: Capital.\\r\\n1:27:17\\r\\nRichard Ryan: Charlie. This is also how\\r\\n1:27:18\\r\\nRichard Ryan: created right. In other words, somebody might be asking about London, and they were actually interested in the great Warren Zebon song werewolves of London, and it doesn't have anything to do with London and and or Paris, or the city has to do with Warren Zebon. So if you, if someone's asking a question and the vector space for\\r\\n1:27:21\\r\\nRichard Ryan: the the word is not related to the question, then you get a who a hallucination correct.\\r\\n1:27:49\\r\\nCathal Flanagan: Yes, so\\r\\n1:28:00\\r\\nCathal Flanagan: we're we're we have a journey to take between embeddings and what's actually occurring. So I can't be just through exactly as this. Let me just double click on somewhat point. Someone made, which was\\r\\n1:28:02\\r\\nCathal Flanagan: capital cities. So this was surprising.\\r\\n1:28:13\\r\\nCathal Flanagan: We're in the.\\r\\n1:28:18\\r\\nDima Timofeev: I would like to make a point that\\r\\n1:28:19\\r\\nDima Timofeev: the words that are similar or nearby in high dimensional vector space is not something that\\r\\n1:28:22\\r\\nDima Timofeev: Google engineers or any other engineers hard coded. Instead it. It is an emerging learning property from the text itself. In other words, this representation was trained on some corpus of data, and these are the similarities that were\\r\\n1:28:30\\r\\nDima Timofeev: that were in that they emerged from this corpus. So yes, you can always construct data set in such a way that some meaningless or specialized terms will occur next to each other.\\r\\n1:28:50\\r\\nDima Timofeev: But in general this is emerging property from general observations in the training corpus that was used to train this specific embedding the Charlie.\\r\\n1:29:04\\r\\nCathal Flanagan: And\\r\\n1:29:14\\r\\nCathal Flanagan: look at this example. Right? Football, nearest words to football are soccer, basketball, hockey, Rugby. When the researchers train this. This is what they expected to see right. They expect you to see things that were were kind of in could be more interchangeable.\\r\\n1:29:16\\r\\nCathal Flanagan: The London is a good example, because that Paris result is interesting.\\r\\n1:29:31\\r\\nCathal Flanagan: Why the emergent properties were that it started to understand relationships.\\r\\n1:29:37\\r\\nCathal Flanagan: And because these are vectors, it, you could actually form an equation\\r\\n1:29:44\\r\\nCathal Flanagan: where you had London plus England minus France.\\r\\n1:29:50\\r\\nCathal Flanagan: And the value in the vector space would be the word Paris.\\r\\n1:29:55\\r\\nCathal Flanagan: And this really surprised people right? That effectively, it was not only learning\\r\\n1:30:01\\r\\nCathal Flanagan: kind of what we might think of as just interchangeable relationships, the coast and shore.\\r\\n1:30:06\\r\\nCathal Flanagan: but deeper relationships. It was understanding\\r\\n1:30:10\\r\\nCathal Flanagan: in its numerical form, the relationships between countries and capital cities which\\r\\n1:30:14\\r\\nCathal Flanagan: kind of shocked everyone. We did not think that this would be\\r\\n1:30:22\\r\\nCathal Flanagan: kind of the the outcome of what is a relatively straightforward way of training these models? These immersion properties were quite, quite surprising.\\r\\n1:30:26\\r\\nCathal Flanagan: So just to recap embeddings are, you're gonna hear about embeddings all of the time now, right? And\\r\\n1:30:37\\r\\nCathal Flanagan: they are the fundamental building blocks to what we have today. Right\\r\\n1:30:47\\r\\nCathal Flanagan: word embeddings. The ability to better represent words in numerical values\\r\\n1:30:55\\r\\nCathal Flanagan: continues to kind of be a way that we want to.\\r\\n1:31:02\\r\\nCathal Flanagan: We want to improve systems today. But just kind of give you a sense.\\r\\n1:31:05\\r\\nCathal Flanagan: You would then.\\r\\n1:31:11\\r\\nCathal Flanagan: you know, maybe have a vector, representation. The man is walking. We're converting those into numerical representations which we can then visualize right? And then it can map to other works. So it's quite.\\r\\n1:31:13\\r\\nCathal Flanagan: quite powerful.\\r\\n1:31:25\\r\\nCathal Flanagan: Okay? So that was 2013 after 2013, of course.\\r\\n1:31:28\\r\\nCathal Flanagan: having one good representation for a word\\r\\n1:31:34\\r\\nCathal Flanagan: was interesting, and we had these emergent properties. But what people were saying is, Hey, we want to predict next word in sequences.\\r\\n1:31:38\\r\\nCathal Flanagan: And of course it really matters what happens in the previous part of the sentence?\\r\\n1:31:46\\r\\nCathal Flanagan: Right? So how do we take that into account? More? And that's, you know, between 2013 and 2017. You mostly saw what are referred to as sequence models, recurrent neural mesh networks, or something called an Lstm.\\r\\n1:31:54\\r\\nCathal Flanagan: And I'm showing you a visualization here of\\r\\n1:32:07\\r\\nCathal Flanagan: how an Lcn model might work, which is the color codings that you're seeing related to each word.\\r\\n1:32:11\\r\\nCathal Flanagan: It was when it's trying to predict the next words in a sentence.\\r\\n1:32:17\\r\\nCathal Flanagan: It was over, waiting the the words directly preceding it.\\r\\n1:32:22\\r\\nCathal Flanagan: Okay, so you can see the color organization. It was still for the word time.\\r\\n1:32:28\\r\\nCathal Flanagan: When it's predicting the word\\r\\n1:32:33\\r\\nCathal Flanagan: is, it's taking time into account much more than it is towards the end. Right? It gets, it shrinks\\r\\n1:32:35\\r\\nCathal Flanagan: as the sentence goes on.\\r\\n1:32:43\\r\\nCathal Flanagan: but of course we all know that. And if you're in a lot of cases\\r\\n1:32:44\\r\\nCathal Flanagan: the most important words in a sentence, if you're trying to predict another word\\r\\n1:32:49\\r\\nCathal Flanagan: is maybe at the beginning of the sentence, Bryce.\\r\\n1:32:52\\r\\nCathal Flanagan: So imagine the sentence. John went to the bank period, he\\r\\n1:32:56\\r\\nCathal Flanagan: and if you're trying to predict the next word after he.\\r\\n1:33:04\\r\\nCathal Flanagan: It's really important that you're know that who he is referring to. Right. He and John\\r\\n1:33:08\\r\\nCathal Flanagan: are the most important words to kind of\\r\\n1:33:13\\r\\nCathal Flanagan: attend to one another, right? Because he is referring to John. So it's really important that\\r\\n1:33:16\\r\\nCathal Flanagan: the system is able to do that. Well, these these models were not able to do that because you're always having the previous words being the most important.\\r\\n1:33:22\\r\\nCathal Flanagan: And so that leads us to the paper. Actually that Richard is going to host on Saturday.\\r\\n1:33:33\\r\\nCathal Flanagan: which is called attention, is all you need.\\r\\n1:33:38\\r\\nCathal Flanagan: and this is by far the most famous paper in\\r\\n1:33:41\\r\\nCathal Flanagan: Llms. And if you know we think of today, we kind of talk about Llms and the word AI interchangeably. Of course they're not. But you know they've it's kind of become that\\r\\n1:33:48\\r\\nCathal Flanagan: not that bad at Moniker\\r\\n1:33:59\\r\\nCathal Flanagan: so definitely the most famous paper. And what it did well, was exactly that.\\r\\n1:34:04\\r\\nCathal Flanagan: actually didn't. It? Did a number of things. Well, the 1st thing was, it helped us solve that problem of\\r\\n1:34:10\\r\\nCathal Flanagan: you know she is eating a green apple. Well, if we're trying to predict the next word.\\r\\n1:34:16\\r\\nCathal Flanagan: We actually need to have relationships and understand\\r\\n1:34:22\\r\\nCathal Flanagan: which words are attending to one another, attending being a technical term here. Right? But like, if you're eating, you're eating an apple.\\r\\n1:34:27\\r\\nCathal Flanagan: Raj, Green and Apple have high attention because they're referring to\\r\\n1:34:35\\r\\nCathal Flanagan: green is referring to the color of the apple versus eating in green should have low attention, right? Because they're not directly\\r\\n1:34:40\\r\\nCathal Flanagan: related to one another.\\r\\n1:34:49\\r\\nCathal Flanagan: So they're this\\r\\n1:34:53\\r\\nCathal Flanagan: model or this system allowed to have better waiting through between the words, and it actually goes back to Embeddings because\\r\\n1:34:55\\r\\nCathal Flanagan: the big challenge we had with embeddings while this was extremely useful and helpful.\\r\\n1:35:03\\r\\nCathal Flanagan: Let's think about the word bank.\\r\\n1:35:09\\r\\nCathal Flanagan: I am going to the river bank.\\r\\n1:35:15\\r\\nCathal Flanagan: I just got a check.\\r\\n1:35:19\\r\\nCathal Flanagan: I am going to the bank\\r\\n1:35:21\\r\\nCathal Flanagan: in the 2013 version of the embeddings.\\r\\n1:35:25\\r\\nCathal Flanagan: The word bank has exactly the same numerical representation which, should it\\r\\n1:35:30\\r\\nCathal Flanagan: right? The numerical representation it's placed in this vector space should be impacted\\r\\n1:35:38\\r\\nCathal Flanagan: by the context of the words or the sentence that it's part of\\r\\n1:35:45\\r\\nCathal Flanagan: right. So how do we update that embeddings value\\r\\n1:35:50\\r\\nCathal Flanagan: so that it actually is taking into account the context of the words coming before it. And that's where\\r\\n1:35:56\\r\\nCathal Flanagan: the this paper gives us a solution towards that which can update those embedding values\\r\\n1:36:04\\r\\nCathal Flanagan: kind of in real time, which is really important and very, very powerful.\\r\\n1:36:12\\r\\nCathal Flanagan: The other thing that you'll hear a lot about, as it relates to this paper is something called the Transformer Architecture. The Transformer. Architecture is a type of neural network, and what surprised people was how generalizable it would be. Previously we had to build very, very specific types of models for specific use cases. What pleasantly surprised people about the transformer architecture was that it\\r\\n1:36:16\\r\\nCathal Flanagan: generalized quite well. So the same architecture could be used, not just for text, but also for\\r\\n1:36:40\\r\\nCathal Flanagan: audio, for example, or image to not for images.\\r\\n1:36:46\\r\\nCathal Flanagan: So you will hear a lot in this class as you're reading about transformers.\\r\\n1:36:52\\r\\nCathal Flanagan: about even this paper. And again, we'll cover the paper in Paper Club on Saturday, so definitely stay tuned, for that. Is it strictly necessary to know or understand this paper to get what you need from the class? Absolutely not. But it is certainly interesting, and it is definitely an important milestone in the journey.\\r\\n1:37:00\\r\\nCathal Flanagan: As I think about this, you know, it's kind of the\\r\\n1:37:22\\r\\nCathal Flanagan: Embeddings were kind of. If you had a a single note right?\\r\\n1:37:30\\r\\nCathal Flanagan: And what a trend to transformer models do is! It's very, very good at orchestrating.\\r\\n1:37:37\\r\\nCathal Flanagan: A lot of different parts of the system. So that's why I like this graphic. It's kind of taking us on the journey from 2013, where we just had single notes to the transformer, which was actually,\\r\\n1:37:43\\r\\nCathal Flanagan: you know, putting a lot of the advances over that those previous years together in a way that we're actually able to do a lot of useful things with it.\\r\\n1:37:56\\r\\nCathal Flanagan: Okay, with that\\r\\n1:38:05\\r\\nCathal Flanagan: I mentioned, we always want to do something practical in class. So we're actually going to take do a little bit of code, so we haven't finished with the theory, but we finished it with it for tonight. We'll certainly revisit us at different parts. But I want to definitely make sure that we have some time to get hands on, and I want to show you how you can start calling the Openai Api\\r\\n1:38:07\\r\\nCathal Flanagan: directly from Cola. So you can start doing fun things with it.\\r\\n1:38:29\\r\\nCathal Flanagan: So to do that I'm going to start a new colab notebook which I'm going to call\\r\\n1:38:33\\r\\nCathal Flanagan: tech 16 lecture one and loss.\\r\\n1:38:41\\r\\nCathal Flanagan: and I'm going to share it on slack\\r\\n1:38:51\\r\\nCathal Flanagan: for everybody. So the 1st thing is, we are going to\\r\\n1:38:54\\r\\nCathal Flanagan: use the open AI Api in this class. So you need an Api key.\\r\\n1:38:59\\r\\nCathal Flanagan: How do you get an Api key for Openai?\\r\\n1:39:05\\r\\nCathal Flanagan: Pretty straightforward you'd sign up for one and you'll also need to. I believe the free\\r\\n1:39:08\\r\\nCathal Flanagan: credits are probably gone, but you will that they used to give out $5. Not anymore, I suspect. But what you'll be able to do is probably put in like $5 or something like that. That'll we're gonna use very small models throughout this class. So it's that should be plenty. The way you do that is, you just literally type in open AI\\r\\n1:39:16\\r\\nCathal Flanagan: Api into Google search, it'll pop up the Api.\\r\\n1:39:39\\r\\nCathal Flanagan: Okay, you? What if you haven't signed in already? You have to create an account? If you have an account. Then it will bring you into the developer platform. Here.\\r\\n1:39:58\\r\\nCathal Flanagan: you scroll down, and we want actually sorry you want to go here to settings.\\r\\n1:40:05\\r\\nCathal Flanagan: Api keys.\\r\\n1:40:13\\r\\nCathal Flanagan: Okay. Now, you will have to put in a credit card. As I mentioned, $5 will be plenty for this class.\\r\\n1:40:16\\r\\nCathal Flanagan: So you can just go in here and just add a credit balance and put in 5 bucks.\\r\\n1:40:23\\r\\nCathal Flanagan: one important thing to do. Do come in and set a limit. The reason for that is, I don't want you\\r\\n1:40:29\\r\\nCathal Flanagan: so come in here and set a budget alert for $5 and a budget limit for $5, so that you know you don't spend any more than that. Again you're putting in the money, so it'll if you, even if you disappear. But I want you putting in $20, and, you know, unexpectedly using like $10. So\\r\\n1:40:35\\r\\nCathal Flanagan: put in $5 and then set the limit for $5, just to make sure you don't run over. But again, if you go over they'll just start just to start getting an error.\\r\\n1:40:54\\r\\nCathal Flanagan: Okay, then you'll need to get an Api key. So in the Api key section\\r\\n1:41:03\\r\\nCathal Flanagan: we'll create a new secret test\\r\\n1:41:08\\r\\nCathal Flanagan: key or give, you know. Give it any name you want.\\r\\n1:41:13\\r\\nCathal Flanagan: We have to give it a project or sample like the default project test for me, create a secret key.\\r\\n1:41:16\\r\\nCathal Flanagan: Okay?\\r\\n1:41:23\\r\\nCathal Flanagan: And once you create the key, you'll get us click, copy.\\r\\n1:41:25\\r\\nCathal Flanagan: Okay, when you copy it, go back to your colab notebook and take look at this little\\r\\n1:41:29\\r\\nCathal Flanagan: secrets. Icon on the left hand side.\\r\\n1:41:36\\r\\nCathal Flanagan: Click on us pop it out.\\r\\n1:41:39\\r\\nCathal Flanagan: This will probably be empty, for you. Just scroll down\\r\\n1:41:41\\r\\nCathal Flanagan: and add a new key and call it open underscore AI key\\r\\n1:41:44\\r\\nCathal Flanagan: and paste in the key that you've just copied.\\r\\n1:41:52\\r\\nCathal Flanagan: Okay?\\r\\n1:41:56\\r\\nCathal Flanagan: And now it becomes accessible and available to\\r\\n1:41:57\\r\\nCathal Flanagan: the notebook. And anytime you're in call out.\\r\\n1:42:01\\r\\nCathal Flanagan: It means you can use that Api key.\\r\\n1:42:04\\r\\nCathal Flanagan: but if you share your notebook with others, they can see it. So it's a very safe and secure way to use the Api key, but not actually not have to worry about\\r\\n1:42:07\\r\\nCathal Flanagan: you know, sharing it, you can keep it safe. Arun is asking, can we use other Apis like anthropic. Absolutely. You can use anyone you want on the Api we will use open AI for the 1st couple of weeks of the class, because it's the most popular. But we will cover anthropic. We will cover Google and show you and provide code for how to actually use those other Apis as well. But for the 1st few weeks we're going to focus on open AI\\r\\n1:42:18\\r\\nCathal Flanagan: awesome. Okay? So assuming you've actually done that, you have an Api key, you put it in, Collab.\\r\\n1:42:44\\r\\nCathal Flanagan: How do you use it? How do you actually start working with the open AI system?\\r\\n1:42:51\\r\\nCathal Flanagan: 1st thing is, we need to install it. It doesn't come by default.\\r\\n1:42:56\\r\\nCathal Flanagan: So if I try to say here\\r\\n1:43:00\\r\\nCathal Flanagan: brought, let me just push one more down\\r\\n1:43:02\\r\\nCathal Flanagan: from open AI import open. AI.\\r\\n1:43:06\\r\\nCathal Flanagan: It's just connecting it doesn't usually take this long.\\r\\n1:43:16\\r\\nCathal Flanagan: Aha! Oh, you know, it's so interesting.\\r\\n1:43:25\\r\\nCathal Flanagan: This is always a good way to know when software is becoming what software is becoming very popular or not.\\r\\n1:43:27\\r\\nCathal Flanagan: it's it's already coming by default. Now within collap. So we don't even need to install it anymore. Previously we had to install it. Okay? So that's how I'm importing. This is how I'm importing the open AI library that allows me to call it. The other thing I need to do is say from\\r\\n1:43:33\\r\\nCathal Flanagan: Google dark, call up import\\r\\n1:43:51\\r\\nCathal Flanagan: user data.\\r\\n1:43:59\\r\\nCathal Flanagan: Why am I doing this? Because this is how I'm going to bring that a Api key that we saved into the notebook.\\r\\n1:44:01\\r\\nCathal Flanagan: Okay? And I'm just gonna call it open. AI key equals user data.\\r\\n1:44:09\\r\\nCathal Flanagan: Don't guess Openai Key.\\r\\n1:44:18\\r\\nCathal Flanagan: Okay? And there are Cheese\\r\\n1:44:21\\r\\nCathal Flanagan: Grant. I have to grant access once, and that's gonna go. And actually, from this location it's grabbing and turning on the open AI key which is now accessible that I can pass it in when I make a call.\\r\\n1:44:24\\r\\nCathal Flanagan: Okay? And again, I'm aware there's a handful of folks who haven't coded before.\\r\\n1:44:37\\r\\nCathal Flanagan: I promise you.\\r\\n1:44:43\\r\\nCathal Flanagan: Take the python boot camp right? We're gonna offer next week.\\r\\n1:44:46\\r\\nCathal Flanagan: it will be able to show you at the very least how to read, but it will become quite\\r\\n1:44:51\\r\\nCathal Flanagan: intuitive. That's the beautiful thing about python. It actually is very, very readable of the language.\\r\\n1:44:56\\r\\nCathal Flanagan: So all I'm doing here is, I'm importing some libraries I'm looking at. I'm importing. I'm getting that Api key, so it's accessible\\r\\n1:45:02\\r\\nCathal Flanagan: that I can pass it in when I make the call.\\r\\n1:45:10\\r\\nCathal Flanagan: And now I actually just need to say, Okay, I need to make a call to open AI so to do that, they ask you to define what's called a client.\\r\\n1:45:13\\r\\nCathal Flanagan: and I am going to say Openai api key.\\r\\n1:45:24\\r\\nCathal Flanagan: So now I have a client that I can call.\\r\\n1:45:30\\r\\nCathal Flanagan: and the way that I actually get a response.\\r\\n1:45:34\\r\\nCathal Flanagan: Yeah, make that call to the model. As I say, response equals\\r\\n1:45:37\\r\\nCathal Flanagan: client because the client I've just defined\\r\\n1:45:42\\r\\nCathal Flanagan: Dot. This is a chat I want to chat with the model. Right?\\r\\n1:45:44\\r\\nCathal Flanagan: It's when we talk about the model, what it's doing, it's actually completing. Remember what are Llms. They're predicting the next word. So we called when it's predicting next word. And it's giving you an answer. We call that a completion. So that's why you're seeing dot chat completions.\\r\\n1:45:50\\r\\nCathal Flanagan: And we're creating a new completion here.\\r\\n1:46:08\\r\\nCathal Flanagan: So it said, Doc, creation as an input to this call.\\r\\n1:46:11\\r\\nCathal Flanagan: well, I have to decide what a what open AI model. Do I want to call\\r\\n1:46:18\\r\\nCathal Flanagan: in this case? We'll just keep it\\r\\n1:46:22\\r\\nCathal Flanagan: simple and cheap. So I'm going to call Gpt.\\r\\n1:46:25\\r\\nCathal Flanagan: dash 3.5 dash turbo.\\r\\n1:46:30\\r\\nCathal Flanagan: Okay, an older model, cheaper model. Again. In the future we'll be able to upgrade and use the latest and greatest models for tonight. Let's just get a a response.\\r\\n1:46:34\\r\\nCathal Flanagan: Okay, and I'm going to pass in 2 messages to the model.\\r\\n1:46:44\\r\\nCathal Flanagan: Okay? And these are required. And these are actually some models have different inputs here. But the 1st is.\\r\\n1:46:49\\r\\nCathal Flanagan: I need a role.\\r\\n1:46:56\\r\\nCathal Flanagan: So I need to tell it that I am going to pass in a message\\r\\n1:46:58\\r\\nCathal Flanagan: which is a system message.\\r\\n1:47:03\\r\\nCathal Flanagan: And so a system message basically means we're going to tell us\\r\\n1:47:06\\r\\nCathal Flanagan: what it is and how we expect it to behave.\\r\\n1:47:14\\r\\nCathal Flanagan: Okay. So in the message I'm going to pass through some content. And the content of the message I'm going to say is.\\r\\n1:47:17\\r\\nCathal Flanagan: And again, you can play around with this. You can change this, to wait, to be whatever you want. I'm going to say you are an AI that takes\\r\\n1:47:25\\r\\nCathal Flanagan: instructions from a human and reduces, and a\\r\\n1:47:35\\r\\nCathal Flanagan: I like to use the word concise? Answer, because the models tend to be a little bit too verbose.\\r\\n1:47:48\\r\\nCathal Flanagan: Okay? So that could be helpful.\\r\\n1:47:54\\r\\nCathal Flanagan: That's it. Okay? So that's the first.st That's the instruction of what I wanted to do\\r\\n1:47:56\\r\\nCathal Flanagan: and how I wanted to behave. And now I pass in\\r\\n1:48:01\\r\\nCathal Flanagan: the actual content of what I want it to do. As if I'm chatting with it. So that's the we call that a user message.\\r\\n1:48:05\\r\\nCathal Flanagan: So in the role of a user role user\\r\\n1:48:12\\r\\nCathal Flanagan: again, the content. What we're actually passing in\\r\\n1:48:25\\r\\nCathal Flanagan: and we're going to say, compose a short wrap about students studying large language models in Stanford on a\\r\\n1:48:29\\r\\nCathal Flanagan: Wednesday.\\r\\n1:48:56\\r\\nCathal Flanagan: Nice?\\r\\n1:48:59\\r\\nCathal Flanagan: All right.\\r\\n1:49:00\\r\\nCathal Flanagan: So this is going. I'm going to run this. It's going to return and save it in the response.\\r\\n1:49:02\\r\\nCathal Flanagan: okay, so I need to print out the response. So just here, I'm going to go print response.\\r\\n1:49:06\\r\\nCathal Flanagan: Let's see what it comes back.\\r\\n1:49:15\\r\\nCathal Flanagan: Okay. So now, what's happened is it has called the model, and it has made an output. And you notice here it's giving me a lot of information. It's kind of telling me this, the Id. For the conversation. We've just had the response.\\r\\n1:49:19\\r\\nCathal Flanagan: Different choices. There's lots of information here. Right?\\r\\n1:49:34\\r\\nCathal Flanagan: Good news is at the moment none of this is strictly necessary to understand. When you want to get a little bit more advanced. For example, log props will actually be something that we can use for classification later. There's lots and lots of information around the call. It did not refuse. For example, refusal equals none right? Most of this, to be quite frank, are things that we just don't need to know right now, it's not helpful.\\r\\n1:49:38\\r\\nCathal Flanagan: So how do I actually just get the key insight with the actual response print response?\\r\\n1:50:01\\r\\nCathal Flanagan: Dopps choices.\\r\\n1:50:09\\r\\nCathal Flanagan: 0 means the 1st one dot message got content.\\r\\n1:50:13\\r\\nCathal Flanagan: Oh, I spoke responsive.\\r\\n1:50:22\\r\\nCathal Flanagan: Here we go hitting the books on at Stanford Wednesday night, hitting the language, tackling that insight, students writing minds, expanding knowledge taking flight, big brains colliding in precise pursuit of that academic fight.\\r\\n1:50:28\\r\\nCathal Flanagan: Well, as you can tell, I'm not a a rapper, but that was pretty good\\r\\n1:50:43\\r\\nCathal Flanagan: or, for example, you could change the content and say, You're an AI, that answers every\\r\\n1:50:48\\r\\nCathal Flanagan: question or every question or command with a question, just if you want to annoy people.\\r\\n1:50:58\\r\\nCathal Flanagan: and then you could say, Where is London\\r\\n1:51:07\\r\\nCathal Flanagan: now? The response is, do you need directions to London.\\r\\n1:51:21\\r\\nCathal Flanagan: I ask you a question\\r\\n1:51:26\\r\\nCathal Flanagan: issue.\\r\\n1:51:34\\r\\nCathal Flanagan: So this is interesting, right? So we're able to basically change the system message to\\r\\n1:51:35\\r\\nCathal Flanagan: have the model behave in any way that we want. And the content is whatever we want to pass in.\\r\\n1:51:44\\r\\nCathal Flanagan: Okay, so this is how you can start having conversations.\\r\\n1:51:49\\r\\nCathal Flanagan: Very important thing.\\r\\n1:51:54\\r\\nCathal Flanagan: The models have no memory.\\r\\n1:51:56\\r\\nCathal Flanagan: Okay. So if you or I are in Chat Gpt.\\r\\n1:52:00\\r\\nCathal Flanagan: and we're having a conversation with the model, and we're going back and forth, you know, tell me about London.\\r\\n1:52:06\\r\\nCathal Flanagan: Okay? And then I ask.\\r\\n1:52:21\\r\\nCathal Flanagan: okay, lots of information. I'm gonna stop and say, what did we just spoke?\\r\\n1:52:29\\r\\nCathal Flanagan: Yes.\\r\\n1:52:36\\r\\nCathal Flanagan: okay, it's being slow today.\\r\\n1:52:51\\r\\nCathal Flanagan: I expect it will tell me that we just spoke about London, Bryce.\\r\\n1:52:54\\r\\nCathal Flanagan: Here we go\\r\\n1:52:58\\r\\nCathal Flanagan: there. We just started talking about London. Great. Let's try and do exactly the same thing in the piece of code that we just wrote. Right? We're talking with the model. Let's go back to your helpful AI,\\r\\n1:53:02\\r\\nCathal Flanagan: right?\\r\\n1:53:13\\r\\nCathal Flanagan: I ask.\\r\\n1:53:15\\r\\nCathal Flanagan: Here, I'm going to say, tell me about London in one sentence.\\r\\n1:53:21\\r\\nCathal Flanagan: Perfect. Does a good job, and ask me about answers about London. Now let's just simply ask, what did we just talk about?\\r\\n1:53:35\\r\\nCathal Flanagan: We just talked about the instructions you give me our finances in return. No.\\r\\n1:53:49\\r\\nCathal Flanagan: that's not what we just talked about. It's the reason it thinks that is because it has also got the context of the system prompt.\\r\\n1:53:55\\r\\nCathal Flanagan: So importantly when you're interacting with the Api, realize it has no memory.\\r\\n1:54:02\\r\\nCathal Flanagan: What you see in Chat Gbt, in the systems that we will build is.\\r\\n1:54:09\\r\\nCathal Flanagan: it's all kind of a trick whenever we pass in. When we're having a chat, or we will build chat applications. In this class.\\r\\n1:54:15\\r\\nCathal Flanagan: we actually need to provide all of the history ourselves manually. We have to tell it its history every time you call the model. It has no recollection of previous interactions. We are responsible for building all of that ourselves. The good news is, there's lots of frameworks and methodologies to do that\\r\\n1:54:23\\r\\nCathal Flanagan: right. But that's a big difference. When you're working with the Api awesome\\r\\n1:54:41\\r\\nCathal Flanagan: so hopefully that gives you a little\\r\\n1:54:47\\r\\nCathal Flanagan: sense of how to use the Api. I'm going to do one thing, which is when I share the notebook. I'm going to share this extra piece of code with you. Because I understand, this is kind of ugly. So we're gonna put everything in a function exactly the same thing that we just did. Okay. But it's just going to be. Make it really easy that when you then just say chat.\\r\\n1:54:50\\r\\nCathal Flanagan: where is London\\r\\n1:55:11\\r\\nCathal Flanagan: that it just gives you a response. So just making it nice and easy for you to interact.\\r\\n1:55:15\\r\\nCathal Flanagan: Okay, so we call this a function.\\r\\n1:55:20\\r\\nCathal Flanagan: And the other thing that I'm providing in the piece of code that I'm share, or the notebook that I'm showed sharing with you is the homework starter.\\r\\n1:55:24\\r\\nCathal Flanagan: So, even though it's the 1st class, you don't get out easy.\\r\\n1:55:31\\r\\nCathal Flanagan: You have homework.\\r\\n1:55:38\\r\\nCathal Flanagan: That homework is.\\r\\n1:55:39\\r\\nCathal Flanagan: You must submit a colab notebook\\r\\n1:55:44\\r\\nCathal Flanagan: with the summarization of some text you choose.\\r\\n1:55:48\\r\\nCathal Flanagan: Okay, extra credit. If you, even if you can have the model, compare 2 documents and their differences right?\\r\\n1:55:52\\r\\nCathal Flanagan: And it's due by the end of the\\r\\n1:56:01\\r\\nCathal Flanagan: we're gonna change this. It's now due by Tuesday. Nice, because I actually need to review them. So next Tuesday\\r\\n1:56:05\\r\\nCathal Flanagan: Eastern time, or sorry Pacific time.\\r\\n1:56:13\\r\\nCathal Flanagan: So the day before class\\r\\n1:56:17\\r\\nCathal Flanagan: Homework issue. So we can just kind of pull out some interesting examples to show everybody. So we'll be sending out the spreadsheet of where you submit the homework as part of the recap\\r\\n1:56:19\\r\\nCathal Flanagan: but the good news is.\\r\\n1:56:28\\r\\nCathal Flanagan: and especially for those who don't code. We were going to provide this. We're gonna provide the starter notebook.\\r\\n1:56:30\\r\\nCathal Flanagan: So if all you want to do is run this and change.\\r\\n1:56:36\\r\\nCathal Flanagan: This is just I'm passing in some content around.\\r\\n1:56:41\\r\\nCathal Flanagan: You know, system. Summarize the text, passing in some information about Meta right into producing the actual\\r\\n1:56:45\\r\\nCathal Flanagan: the summarization. If for your homework. If you don't code, all you want to do is just change this text perfectly acceptable.\\r\\n1:56:52\\r\\nCathal Flanagan: But for those who want to get extra credit or take it to the next level, you know, really kind of build a\\r\\n1:57:00\\r\\nCathal Flanagan: great summarization system here that can summarize multiple documents.\\r\\n1:57:06\\r\\nCathal Flanagan: I think it would be pretty interesting.\\r\\n1:57:10\\r\\nCathal Flanagan: So with that we are at time\\r\\n1:57:12\\r\\nCathal Flanagan: we will stick around for questions that folks have, and we will also see\\r\\n1:57:16\\r\\nCathal Flanagan: We will also see folks, I guess we're having.\\r\\n1:57:21\\r\\nCathal Flanagan: We're having paper club on Saturday for those who want to join. And then on Sunday.\\r\\n1:57:25\\r\\nCathal Flanagan: I will be hosting a virtual coffee hour. So our 1st coffee hours this Sunday at one Pm. Eastern for anyone that would like to join again. No obligation. But it's just a fun place to show up, and\\r\\n1:57:31\\r\\nCathal Flanagan: we could talk Llms. We can talk tech. We can talk philosophy whatever you want.\\r\\n1:57:44\\r\\nCathal Flanagan: So again, optional coffee office hours this Sunday. We will be on slack\\r\\n1:57:49\\r\\nCathal Flanagan: here to support you throughout. And we'll stick around for any questions folks have afterwards. Now. Otherwise. Thank you.\\r\\n1:57:55\\r\\nCathal Flanagan: Great to see you. We're excited to spend the next 8 weeks learning alongside you. And yeah, appreciate your attendance.\\r\\n1:58:02\\r\\nCathal Flanagan: Thank you. Folks see you on slack.\\r\\n1:58:11\\r\\nTony A: Cool thanks.\\r\\n1:58:14\\r\\nBrian Kennedy: Thanks a lot.\\r\\n1:58:16\\r\\nRavi Shankar: Thank you. Nice session.\\r\\n1:58:17\\r\\nSarit Arora: Thank you.\\r\\n1:58:20\\r\\nCathal Flanagan: All right, recordings will be available on canvas. So in the Panopto course, video section, you'll be able to see them there.\\r\\n1:58:21\\r\\nCathal Flanagan: yeah, okay, let's let's do Q. And A, who has questions, anything we can address, live.\\r\\n1:58:31\\r\\nJeremy McCormick: I had a few questions about the homework. 1st of all, I was wondering if there was a specific slack channel. You'd like us to ask questions about the homework. And then the second question was.\\r\\n1:58:36\\r\\nJeremy McCormick: for your extra credit.\\r\\n1:58:46\\r\\nJeremy McCormick: it says, have the model compared to documents. Is that just a matter of putting in the text, saying, here's my 1st document. And here's my second document. I mean, I understand it can only really take one input\\r\\n1:58:49\\r\\nCathal Flanagan: So so to some degree, I'd like you to try different things. So yes, if all your if if you don't code a lot, the simplest thing would literally be just to write a prompt\\r\\n1:59:02\\r\\nCathal Flanagan: to actually do it on passing the text.\\r\\n1:59:13\\r\\nCathal Flanagan: But you know there are lots of other ways to do it right like, could you do it more systematically? Could you even start looking at Googling and see what what frameworks are available? I'm in this class. We're going to talk about\\r\\n1:59:15\\r\\nCathal Flanagan: llama Index and Langchain as being 2 very popular frameworks. That kind of make this very easy. Maybe you want to start exploring those you don't need to, but I'm just kind of it's it's definitely you could get an early jump on next week's content if you do.\\r\\n1:59:28\\r\\nJeremy McCormick: Okay.\\r\\n1:59:44\\r\\nAmarsh Anand: So by 2 documents you mean 2 pieces of text, right? It's not that we are uploading a word document or a Pdf. Or something.\\r\\n1:59:46\\r\\nCathal Flanagan: Well, that's my question is so. The answer is is up to you. It ultimately it's all text.\\r\\n1:59:52\\r\\nCathal Flanagan: but it's probably more useful to you if you can upload\\r\\n1:59:58\\r\\nCathal Flanagan: Pdfs right? Or pull the Pdfs or other sources from the Internet. So really, it's kind of\\r\\n2:00:02\\r\\nCathal Flanagan: whatever is the right challenge for you right? Because next week we will be showing you how you can actually use the frameworks\\r\\n2:00:07\\r\\nCathal Flanagan: that are available to you to actually access\\r\\n2:00:16\\r\\nCathal Flanagan: Pdfs and other assets, and parse them easily.\\r\\n2:00:19\\r\\nAmarsh Anand: Oh!\\r\\n2:00:24\\r\\nYankai Su: Yeah, my question is regarding and transformer.\\r\\n2:00:29\\r\\nYankai Su: And therefore in this important\\r\\n2:00:37\\r\\nYankai Su: journey, like, what's the different role for? And the transformer just like to know.\\r\\n2:00:41\\r\\nCathal Flanagan: Sorry. Your question is, what's the different role.\\r\\n2:00:50\\r\\nYankai Su: It, I know, like you in light.\\r\\n2:00:54\\r\\nYankai Su: That kind of from like 2,013 to towards like a large library model.\\r\\n2:01:00\\r\\nYankai Su: So on this, on this journey there are different important steps.\\r\\n2:01:06\\r\\nCathal Flanagan: Yup!\\r\\n2:01:12\\r\\nYankai Su: We mentioned that the wag to the word to wag, and then later Rn, and then\\r\\n2:01:13\\r\\nYankai Su: the transformer. I just said, curious like to.\\r\\n2:01:20\\r\\nYankai Su: Oh, what's the what's the different role like in this journey? The R and and the transformer played differently.\\r\\n2:01:25\\r\\nCathal Flanagan: So remember going back to the visualization I showed you\\r\\n2:01:36\\r\\nCathal Flanagan: ultimately. It's all about. They were what they were all trying to achieve is better word, prediction, price,\\r\\n2:01:40\\r\\nCathal Flanagan: and Rnn. Was an advance in that it allowed you to consider the previous words. In the sentence.\\r\\n2:01:48\\r\\nCathal Flanagan: however, the weighting of those were, the performance\\r\\n2:01:55\\r\\nCathal Flanagan: improved. But it was not a dramatic improvement over time, right?\\r\\n2:02:00\\r\\nCathal Flanagan: And the transformer with what it allowed was the weighting of the words that attention mechanism.\\r\\n2:02:05\\r\\nCathal Flanagan: It meant that we could actually put a higher waste when we're looking to predict the probability of the next word, we could put higher waste\\r\\n2:02:17\\r\\nCathal Flanagan: on the most important words. So, going back to my example of John, went to the river.\\r\\n2:02:25\\r\\nCathal Flanagan: I went to the bank period. He\\r\\n2:02:32\\r\\nCathal Flanagan: the next thing like you might say the next thing might be key\\r\\n2:02:35\\r\\nCathal Flanagan: cash to check, or something like that, as the next word.\\r\\n2:02:40\\r\\nCathal Flanagan: It's really important\\r\\n2:02:43\\r\\nCathal Flanagan: that he and John are kind of they have that if we know that, John, that when they're talking about he, it's referring to John\\r\\n2:02:45\\r\\nCathal Flanagan: right? Because, like you might say,\\r\\n2:02:53\\r\\nCathal Flanagan: John and Sally went to the bank\\r\\n2:02:59\\r\\nCathal Flanagan: period key. Well, it's really important that, like he and John are like the most important words in that sentence, because he is referring to John\\r\\n2:03:03\\r\\nCathal Flanagan: right? And if you had words previously in another sentence related to John, you'd watch the like those you want it all to be attended correctly. So the answer to your question is.\\r\\n2:03:11\\r\\nCathal Flanagan: it's really all about the waiting of the words.\\r\\n2:03:21\\r\\nCathal Flanagan: And when I say words, remember, I think about it's embeddings. Right? Because we're using the numerical representation of those words. In the prediction of the next word. So it literally, you know, when you think about\\r\\n2:03:26\\r\\nCathal Flanagan: like that. That was the importance. The outcome was\\r\\n2:03:39\\r\\nCathal Flanagan: much better word prediction, which has kind of led us where we are today.\\r\\n2:03:43\\r\\nCathal Flanagan: And since 2017 really not a lot has changed in terms of architecture. What has changed is the amount of data and compute we've thrown into that problem. So that word prediction has gotten really really good.\\r\\n2:03:49\\r\\nYankai Su: No, so so even even right now, the large library model\\r\\n2:04:03\\r\\nYankai Su: under the hook they're using, still using this word to work and transformer and to for the training.\\r\\n2:04:09\\r\\nCathal Flanagan: Well, actually transformers. Yeah, so what happens? If you look at the training code, you'll notice they're they're setting up the the model and what? It's a\\r\\n2:04:19\\r\\nCathal Flanagan: transformer architecture. But the 1st thing it does is it actually converts everything old words into embeddings.\\r\\n2:04:30\\r\\nCathal Flanagan: Right? So like. That's why I say, embeddings are like the fundamental building blocks like when we talk about words.\\r\\n2:04:36\\r\\nCathal Flanagan: when it's going into a model. It's reading embeddings. That's a numeric representation.\\r\\n2:04:42\\r\\nCathal Flanagan: Now, there's a small nuance here.\\r\\n2:04:47\\r\\nCathal Flanagan: which is, you're going to start hearing the word token a loss.\\r\\n2:04:50\\r\\nCathal Flanagan: And when we talk about embeddings\\r\\n2:04:55\\r\\nCathal Flanagan: generally, it's actually embedding tokens rather than words. They're close enough to be interchangeable.\\r\\n2:04:58\\r\\nCathal Flanagan: but not exactly right, so token you more might be closer to a syllable, or like a word with a space\\r\\n2:05:04\\r\\nCathal Flanagan: but for the purposes of this class, as we're describing it, you can think about the word word and token as being the same.\\r\\n2:05:12\\r\\nCathal Flanagan: and embeddings are, you know the represent numeric representations of those words or tokens.\\r\\n2:05:21\\r\\nYankai Su: Thank you.\\r\\n2:05:30\\r\\nCathal Flanagan: Sure.\\r\\n2:05:31\\r\\nCathal Flanagan: What else is a question.\\r\\n2:05:33\\r\\nAndrey Skripkin: Richard, did you ask your question already.\\r\\n2:05:38\\r\\nRichard Ryan: I haven't. I? Actually what I was? Just gonna note that was that a number of people in\\r\\n2:05:43\\r\\nRichard Ryan: in the slack channels? There was a lot of noise about about which\\r\\n2:05:50\\r\\nRichard Ryan: frameworks or platforms we were gonna use. I\\r\\n2:05:59\\r\\nRichard Ryan: have recently been using anaconda a lot, because the great and good Andrew Ng. At deep learning, AI, who is kind of like my favorite person in the AI world, things. Anaconda is great. And basically, these platforms we're talking about are using Jupiter notebooks. Correct.\\r\\n2:06:06\\r\\nCathal Flanagan: Sorry, Richard.\\r\\n2:06:33\\r\\nRichard Ryan: They're Jupiter notebooks.\\r\\n2:06:35\\r\\nCathal Flanagan: You mean the Cola notebooks.\\r\\n2:06:37\\r\\nRichard Ryan: Hey? The Colon notebooks are Jupiter notebooks. Right?\\r\\n2:06:40\\r\\nRichard Ryan: They're they're they're they're Google versions. So what would be, in other words, if if we could use\\r\\n2:06:45\\r\\nRichard Ryan: if we could use Jupyter notebooks\\r\\n2:06:53\\r\\nRichard Ryan: for the class. The class presentations or homework assignments whatever.\\r\\n2:06:55\\r\\nRichard Ryan: What would make the difference between and an anaconda.\\r\\n2:07:04\\r\\nCathal Flanagan: People can use whatever they want.\\r\\n2:07:09\\r\\nCathal Flanagan: We will be working on a collab here, because.\\r\\n2:07:10\\r\\nCathal Flanagan: as people are probably aware, you get into dependencies. Hell when you start using.\\r\\n2:07:14\\r\\nRichard Ryan: Yeah, it's.\\r\\n2:07:20\\r\\nCathal Flanagan: Anaconda. And you start, then have to work with virtual environment, etc. So people should, for those who care or want or know about anaconda and want to manage their own environments absolutely to. They need to make codes at work.\\r\\n2:07:21\\r\\nJeremy McCormick: It's easiest to just use colab just to submit the code right? Because we can just give you a URL. Is that is that the idea.\\r\\n2:07:37\\r\\nCathal Flanagan: More importantly, what we really like is A is because it allows everyone just to get stuff running straight away. But number 2 for sharing. When you share the homework or you share your final project, you're going to be sharing it with everybody right in in a Google Sheet, because we want everyone to be able to look at a hundred other notebooks that are doing very interesting things and stealing\\r\\n2:07:45\\r\\nCathal Flanagan: from each other and saying, Hey, this person built a document summarization system. That's effing amazing. I want that and having access to it. So that's 1 of the great things is you will leave class.\\r\\n2:08:08\\r\\nCathal Flanagan: with probably hopefully, 800 Jupiter, or maybe sorry, more like 700\\r\\n2:08:20\\r\\nCathal Flanagan: Jupiter and our colab notebooks that all do very interesting things that have descriptions. So it's a real asset. That's that kind of folks can leave the class with.\\r\\n2:08:29\\r\\nJeremy McCormick: And and so are those Urls public by default. Or do we have to enable them to be public in the interface.\\r\\n2:08:39\\r\\nCathal Flanagan: And then share with everybody, with the link.\\r\\n2:08:47\\r\\nJeremy McCormick: Oh, okay, so it's like an opaque link that we that we share. And then once you have the link, you can see it.\\r\\n2:08:50\\r\\nCathal Flanagan: Correct.\\r\\n2:08:56\\r\\nJeremy McCormick: Okay. Great. Thanks.\\r\\n2:08:56\\r\\nCathal Flanagan: Andre.\\r\\n2:09:00\\r\\nAndrey Skripkin: Yeah, I think I was the next. Well, 1st of all, I just wanted to thank you for this. It's just kind of for someone who's not coding. It's amazing to see this actually working 2 questions, one quick one. You mentioned that you've created a script that you will share that just kind of does it? How do I find it? Did I miss it anywhere?\\r\\n2:09:01\\r\\nCathal Flanagan: Well, I'll share right now in the slack channel. And then also tomorrow morning, you're gonna get a big\\r\\n2:09:21\\r\\nCathal Flanagan: a big recap email from me, which is sharing slides and sharing the key assets and links, and then kind of reminding you of, you know, upcoming dates, for paper, club, etcetera. So like that after every class. But for right now in the today's channel on slack tech 16 dash texture I've just pasted in the full prepared notebook from today.\\r\\n2:09:27\\r\\nAndrey Skripkin: Wonderful. And and my second question was, well.\\r\\n2:09:54\\r\\nAndrey Skripkin: by you, by by using the model of the 4 0, the most recent model from Openai in our in our Google Collab, will I be able to get the same result as I get through the paid version of Chat Gpt, like, I'm trying to understand if I can stop paying for chat gpt, but create like, just Google Collab and use it. Use it there.\\r\\n2:09:58\\r\\nCathal Flanagan: So the simple answer is, yes, of course, the the it's exactly the same model under the hood. There's a couple of things you don't get like one is\\r\\n2:10:22\\r\\nCathal Flanagan: you don't get memory so that chat interface is going. So in this class you'll have. You'll learn how to build that chat interface. But you don't get it for free. With the Api, that's that's that's 1 thing to consider. The other thing is like tools and features. So, for example, if you use the consumer facing version, you're gonna have, if you press the button, it'll search the web for you.\\r\\n2:10:32\\r\\nCathal Flanagan: The Api by default. It's just using that core model. So that's a g good experiment went to the core model using the Api and ask it, who won the Us. Presidency. It does not know.\\r\\n2:10:52\\r\\nCathal Flanagan: Rise, go into chat, gpt. It knows not because the core model is different, because\\r\\n2:11:03\\r\\nCathal Flanagan: it's smart enough to do a web search for that type of question.\\r\\n2:11:10\\r\\nAndrey Skripkin: Will, we will. We get the knowledge how to get there.\\r\\n2:11:14\\r\\nCathal Flanagan: Yeah, so those are, we call them function calling. So, yeah.\\r\\n2:11:18\\r\\nAndrey Skripkin: Thank you.\\r\\n2:11:22\\r\\nCathal Flanagan: Awesome Aj.\\r\\n2:11:24\\r\\nAjay D.: Thanks for today's presentation. Quick question of the the original effort to do the next word prediction.\\r\\n2:11:29\\r\\nAjay D.: What was the motivation behind network prediction? What was the problem that these researchers were trying to solve?\\r\\n2:11:37\\r\\nCathal Flanagan: A lot of the the trans, the transform paper. But a lot of this is actually came about\\r\\n2:11:48\\r\\nCathal Flanagan: around translation. So the initial attention is all you need. Paper was that the authors didn't fully understand what they had done. They were told they were just trying to do better translation for tools like Google translation. So actually, a lot of the progress here. Yeah, why? Because\\r\\n2:11:54\\r\\nCathal Flanagan: translation is a nice task in that. You have an English version of a sentence, a French version, and you kind of have a\\r\\n2:12:12\\r\\nCathal Flanagan: certain amount of truth, right of like, you know, if you've done a good job or not? So the answer is translation initially.\\r\\n2:12:19\\r\\nAjay D.: Thanks.\\r\\n2:12:29\\r\\nRichard Ryan: A Aj. If you look at the links on the attention is all you need paper and and the general chat, or the cool papers Chat, you'll see a lot of very deep and interesting discussion of exactly what you've asked, which is a great question.\\r\\n2:12:31\\r\\nCathal Flanagan: Okay? And then finally, young kid.\\r\\n2:12:51\\r\\nYankai Su: Yeah, my questions is regarding this collab. So when I try to click the link, it asked me to initialize using a Google account. I'm just using my personal account. I just want to make sure that this is\\r\\n2:12:53\\r\\nYankai Su: correct for this class.\\r\\n2:13:10\\r\\nCathal Flanagan: Yes. So this is, you should be just using your default. Google account. Not your work.\\r\\n2:13:13\\r\\nYankai Su: Personal, my personal email account, right.\\r\\n2:13:18\\r\\nCathal Flanagan: Your. Yes, that's that. You can use any account. But you know your personal one will have access to Google Collab by default.\\r\\n2:13:23\\r\\nYankai Su: So so it it is the right right way for this. For this class, like, what's the right way, just using my personal account right, or we have class account, like.\\r\\n2:13:33\\r\\nCathal Flanagan: Oh,\\r\\n2:13:44\\r\\nCathal Flanagan: Whichever works for you.\\r\\n2:13:46\\r\\nCathal Flanagan: we don't. The. It's probably better to use your personal if you're taking, you know. That would be my advice.\\r\\n2:13:49\\r\\nYankai Su: Okay.\\r\\n2:13:56\\r\\nYankai Su: Okay. Thank you.\\r\\n2:13:57\\r\\nCathal Flanagan: Ours.\\r\\n2:13:59\\r\\nCathal Flanagan: Okay with that. I think we are\\r\\n2:14:00\\r\\nCathal Flanagan: at time. We'll see some folks for the events over the weekend. We'll see everyone else hopefully on slack. And the extent that people have questions or I need anything, let us know. Otherwise excited to spend the next day, guys. And thank you for coming tonight.\\r\\n2:14:02\\r\\nRichard Ryan: Cattle and or demo. When are you gonna set up the the Zoom chat for 10 pst, on Saturday for? Yeah.\\r\\n2:14:18\\r\\nCathal Flanagan: Yes, we can. Yeah.\\r\\n2:14:32\\r\\nRichard Ryan: Because I don't. I don't. I can't. I don't. I can't administer the zoom, so I can't set it up myself. Otherwise I happily would.\\r\\n2:14:35\\r\\nCathal Flanagan: Yeah, no, I understand. Let me let me do it. I'm gonna try and do it in a way that it doesn't require me to do. Start it every time. But I don't. Actually, I've had challenge. As you know, Richard, I've had challenges this in the past.\\r\\n2:14:43\\r\\nRichard Ryan: I know it's zoom always in there. They're always changing the rules.\\r\\n2:14:56\\r\\nCathal Flanagan: Follow me on Saturday morning, but I will give you a zoom link.\\r\\n2:15:00\\r\\nRichard Ryan: Alright! Alright! Great.\\r\\n2:15:04\\r\\nCathal Flanagan: Appreciate it. Good night, everyone.\\r\\n2:15:05\\r\\nJeremy McCormick: Thank you. Bye.\\r\\n2:15:08\\r\\nRichard Ryan: Bomb away!\\r\\n2:15:09\\r\\nCathal Flanagan: Thank you. Bye, Charlie.\\r\\n2:15:11\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f207a599-26a3-4c3e-9323-9068035a2458', embedding=None, metadata={'file_path': '/content/Tech16_Captions/Lecture2.txt', 'file_name': 'Lecture2.txt', 'file_type': 'text/plain', 'file_size': 157819, 'creation_date': '2025-02-20', 'last_modified_date': '2025-02-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Lecture 2 Captions\\r\\n\\r\\nCathal Flanagan: Excellent. Alright! Hi! Everyone nice to see you all again. Welcome back to week 2.\\r\\n0:01\\r\\nCathal Flanagan: It certainly is an exciting time since we last spoke. We've had a couple of exciting announcements from Openai. They're keeping up. So we'll we'll talk about a little bit of that later. So hopefully you've been following. I'll show you some demos of it of them. Kind of mind blowing stuff.\\r\\n0:10\\r\\nCathal Flanagan: As a reminder. The mission of the class is to lower the barrier to use Llm tools in everyday business settings. So hopefully, you're\\r\\n0:30\\r\\nCathal Flanagan: you're getting to explore them, at least in your personal lives, if not at work, and we're hoping to give you both intuition around how to use them, what to use them for and how to use them. Well. So we're gonna definitely continue that journey tonight. Particularly hopefully, some good interest insights around prompt engineering\\r\\n0:40\\r\\nCathal Flanagan: the values of the class. We are very focused on providing support for those who are just starting their journey. That is definitely true. Given that we just had a 1 h 0 to hero session on python python Mini python boot camp. We went through. You know, no knowledge of code to actually passing the 1st question of of a engineering interview. So\\r\\n1:00\\r\\nCathal Flanagan: definitely hope you're getting get a sense that you know learning to code and learning. Python is not as difficult as you might think. And really.\\r\\n1:30\\r\\nCathal Flanagan: you know, it's just about repetitive practice. And there's never been a better time to learn.\\r\\n1:40\\r\\nCathal Flanagan: We do that, because in this class we focus on the practical. I want you to leave the class, every class\\r\\n1:44\\r\\nCathal Flanagan: having written some code feeling like you've learned something very practical and new that you can start applying that week.\\r\\n1:50\\r\\nCathal Flanagan: but also along the way. Hopefully.\\r\\n1:57\\r\\nCathal Flanagan: we are providing useful and interesting knowledge and insights.\\r\\n2:00\\r\\nCathal Flanagan: certainly lots of things to to keep up with these days\\r\\n2:04\\r\\nCathal Flanagan: as important as anything you learn here. It's about who you meet here. And so the class is all about creating a community. And I'm really happy to see a community already forming so well.\\r\\n2:10\\r\\nCathal Flanagan: That community exists largely on slack\\r\\n2:22\\r\\nCathal Flanagan: do explore the channels that are available. See the conversations that people are having dive in. As I said last week. You will get\\r\\n2:25\\r\\nCathal Flanagan: as much out of this class as you put into it. If you haven't already looked through the introductions Channel.\\r\\n2:33\\r\\nCathal Flanagan: Take a look. You are going to see some really interesting people, the most interesting people I've ever met in the Bay Area. And if someone around the world are students who've taken this class?\\r\\n2:40\\r\\nCathal Flanagan: I have personally invested in startups that have come out of this class\\r\\n2:53\\r\\nCathal Flanagan: we have venture capitalists who are in part of the community. We have people who have started and sold large businesses to very large companies who are part of the community, so you'll never find a more interesting group of\\r\\n2:58\\r\\nCathal Flanagan: folks in this class. We haven't had any marriages yet, but you never know so definitely.\\r\\n3:11\\r\\nCathal Flanagan: Look at the folks who are available make connections.\\r\\n3:21\\r\\nCathal Flanagan: invite each other out for coffee, set up those meetups. I see the Bay area is very active, and already has a couple of meetups underway or watch parties. So please continue that I'm based in New York. Love to meet anyone for coffee who's either here or passing through. I'll tell you about some international travel I have coming up, and maybe\\r\\n3:24\\r\\nCathal Flanagan: I'd love to meet anyone who's in any of the locations. But we have people all over the world. Brazil, Big Cohort.\\r\\n3:43\\r\\nCathal Flanagan: one of our\\r\\n3:51\\r\\nCathal Flanagan: favorite alumni, Rodrigo, is part of the class he's based in Sao Paulo. If you're there, you absolutely have to meet him. He's an amazing guy. And look at, look at each other. I do feel like probably need a Brazilian channel for meetups.\\r\\n3:53\\r\\nCathal Flanagan: And then we have meetups and other other places as well. So definitely take advantage of that. It's a great community.\\r\\n4:07\\r\\nCathal Flanagan: Actually this. Speaking of it, I will actually be traveling over the next few weeks myself. I'll be teaching the class remotely as I already am. I will be in Abu Dhabi and Hong Kong for work. If anyone happens to be there, live there or happens to be there over the next few weeks we should grab coffee be pretty excited. I'm always\\r\\n4:16\\r\\nCathal Flanagan: interested to meet people who are taking the class\\r\\n4:37\\r\\nCathal Flanagan: and the other way that you can get involved in the community. Aspect is signing up for a buddy particularly relevant for those folks for whom coding is new.\\r\\n4:41\\r\\nCathal Flanagan: great to have someone that you can reach out to that you can call upon to help.\\r\\n4:51\\r\\nCathal Flanagan: So if you haven't already, we actually have some amazing\\r\\n4:55\\r\\nCathal Flanagan: people who've signed up to be buddies.\\r\\n4:59\\r\\nCathal Flanagan: So don't be afraid. In these homework sheets. We have a section for Mentor Mentees or Buddy sign up, sign up for one, and then it's your responsibility. If you signed up to be a mentee or of somebody you reach out. Say, hey!\\r\\n5:02\\r\\nCathal Flanagan: Can I grab the cough. Can we grab coffee? Can I? Virtual or otherwise? You know I have this problem. Do you mind helping me, and thank you very much for those who signed up to be mentors, and we will make sure you are richly rewarded with some swag. By the end of the class with us\\r\\n5:18\\r\\nCathal Flanagan: my closets are full of classified from all classes, so I'll be happy to get give some away.\\r\\n5:36\\r\\nCathal Flanagan: if you are active on social media. We love that we love to see people posting up about the class and what they're the fund that they're having and projects they're doing feel free to share your code.\\r\\n5:45\\r\\nCathal Flanagan: Other assets online. We want to inspire lots of other people. We just ask you use the hashtag. Use the hashtag. Stanford Tech 16 or just Tech 16. Whichever works for you\\r\\n5:57\\r\\nCathal Flanagan: we accept Linkedin, Tiktok.\\r\\n6:08\\r\\nCathal Flanagan: Wiki, or Wiki, not Wikipedia with the Wikipedia. Also Instagram. We're trying to make Dima and or anya tick tock famous by the end of the quarter or the end of the semester. So do your worst.\\r\\n6:11\\r\\nCathal Flanagan: Excellent\\r\\n6:25\\r\\nCathal Flanagan: other meetups that are happening next week or not, maybe not next week. Sorry in March, but every week I'll call out different things that are happening. There is a someone posted up a really cool\\r\\n6:28\\r\\nCathal Flanagan: in person event, and it's actually available virtually as well. That conversation with Eric Schmidt, the former CEO of Google. He's an amazing guy. He'll be speaking at Park itself, a very famous institution in Silicon Valley, where Steve Jobs actually got the idea for the mass and the Ui\\r\\n6:40\\r\\nCathal Flanagan: and he'll be speaking there at that institution you can register for free so I know there's already a few people from class going great opportunity to have a little meet up, so keep that in mind\\r\\n6:58\\r\\nCathal Flanagan: awesome, and then I'm sure folks have been following the news this week, but we saw some in very interesting launches from Openai. So has anyone had a chance to play with either deep research or the operator model? Or do do folks know what they are.\\r\\n7:10\\r\\nAjay D.: I did both.\\r\\n7:27\\r\\nCathal Flanagan: What are your what's your opinion?\\r\\n7:29\\r\\nAjay D.: So I I loved the deep research\\r\\n7:32\\r\\nAjay D.: I could. I could actually see it going to multiple websites and\\r\\n7:37\\r\\nAjay D.: doing what it called thinking going to each of the site and and trying to make sense based on my prompt.\\r\\n7:43\\r\\nAjay D.: And then the other one. The operator was just.\\r\\n7:51\\r\\nAjay D.: you know, really freaky kind of made me feel like anyone who does most of their work on a computer.\\r\\n7:54\\r\\nAjay D.: Their job can be so easily automated, especially data entry.\\r\\n8:00\\r\\nCathal Flanagan: So for those who aren't aware, this is a new breed of model that's coming out. It's called a computer use model.\\r\\n8:07\\r\\nCathal Flanagan: And if I say, find computer language classes at Stanford continuing\\r\\n8:12\\r\\nCathal Flanagan: Gotty's school.\\r\\n8:21\\r\\nCathal Flanagan: What you're going to see happen here is\\r\\n8:27\\r\\nCathal Flanagan: it is actually taking control of a virtual keyboard and mouse that it has access to.\\r\\n8:32\\r\\nCathal Flanagan: It's searching the Internet. So this is live.\\r\\n8:39\\r\\nCathal Flanagan: Okay, it has got its own browser. It is. You can see the mouse and keyboard. You can see the keyboard, and you can see the you can. And then you can see it typing. Okay? So it's browsing the Internet in real time\\r\\n8:42\\r\\nCathal Flanagan: for itself. You can actually see kind of the the logic on the left hand side of like, how it's thinking to itself what it's doing, thinking, clicking, having a search by department here.\\r\\n8:55\\r\\nAjay D.: Yeah, I did this using Linkedin. So I I asked it to go to Linkedin and find specific people in in a specific company.\\r\\n9:10\\r\\nAjay D.: And then when it went to Linkedin, it waited for me to log in. I logged in, and it was just amazing to see\\r\\n9:18\\r\\nAjay D.: this bought just work.\\r\\n9:26\\r\\nCathal Flanagan: Yeah, it is per your point. It's\\r\\n9:31\\r\\nCathal Flanagan: It's it's a brave new world. It certainly is. The really interesting thing that is\\r\\n9:35\\r\\nCathal Flanagan: happening here is, of course.\\r\\n9:44\\r\\nCathal Flanagan: you know, the fact that actually just has the ability to to navigate these websites.\\r\\n9:47\\r\\nCathal Flanagan: And, you know, conduct this research by itself.\\r\\n9:52\\r\\nCathal Flanagan: So\\r\\n9:57\\r\\nCathal Flanagan: it's early days, I would say. Maybe it's shallow. It kind of gives up after a while, though it has actually found our class. I did not ask it to found. Find our class exactly. I just asked to find classes related to large language models. Okay? And here it is.\\r\\n9:58\\r\\nAjay D.: Okay.\\r\\n10:15\\r\\nCathal Flanagan: And now it's signing it up for notifications.\\r\\n10:17\\r\\nCathal Flanagan: Raj. I could then tell us like, would you like to proceed? Yes, fine me up.\\r\\n10:20\\r\\nCathal Flanagan: Okay. Now, it doesn't necessarily know who I am. So I'll be surprised if it actually does this correctly. But let's see.\\r\\n10:27\\r\\nCathal Flanagan: No, there you go. He's asking for the inputs.\\r\\n10:35\\r\\nAjay D.: I asked it to make entries in my Google Google Sheet. And I gave it the link to the Google sheet. So I was trying to like extract important information and create a sheet. And it was it created a new tab.\\r\\n10:38\\r\\nAjay D.: It went to the Google Sheet. It created the rows. It started creating entries.\\r\\n10:51\\r\\nCathal Flanagan: Wow.\\r\\n10:56\\r\\nCathal Flanagan: that's pretty cool myself was asking, does it require payment? Yes. So they have limited currently to the pro\\r\\n10:58\\r\\nCathal Flanagan: version, which means you have to pay $200 a month\\r\\n11:06\\r\\nCathal Flanagan: so probably far too expensive for what it's worth right now. Remember, it's still very early days. It's worth\\r\\n11:09\\r\\nCathal Flanagan: it it it fails probably about 50% of the time. So it's still experimental, and they are going to bring it to the lower tier. $20 per month user very soon. It's just, you know, they're kind of rolling it out, and of course trying to get everyone to pay $200 a month, but it is a definitely a sign of the future.\\r\\n11:18\\r\\nCathal Flanagan: And shall I go ahead and submit it? Sure? Why not. We're just submitting spam now to Stanford\\r\\n11:39\\r\\nCathal Flanagan: submission.\\r\\n11:45\\r\\nCathal Flanagan: You know what's really interesting to watch\\r\\n11:50\\r\\nCathal Flanagan: when it goes and starts going. It's it when it encounters the Cloud Fair. Are you a robot? And now it clicks them. It's just like\\r\\n11:53\\r\\nCathal Flanagan: crazy. Okay. Now, John Smith has signed up for the sun for class. There you go.\\r\\n12:02\\r\\nArun Tyagi: And I mean, like, it's interesting thing would be like, can can it parallelize this? Right? Like, how spams break systems like, okay, this, what it's trying to do like, can it start signing up hundreds of people at one time and spamming the website.\\r\\n12:07\\r\\nCathal Flanagan: They don't currently offer an Api. They are planning to. They have one in early Alpha. So the simple answer will be, theoretically yes, and it may not be I open. AI will probably have safety measures in check. But of course there's already open source versions of this. They don't work as well. But anthropic came out with a model such as this a few months ago. It was quickly an open source\\r\\n12:22\\r\\nCathal Flanagan: package. Because really what's happening under the hood is, it's using the multimodal functionality of the models. It's taking screenshots, lots of screenshots, reasoning over them, using the what's in the screenshot to make the decision as to where to move them as\\r\\n12:45\\r\\nCathal Flanagan: right. That's kind of how it's actually processing the input.\\r\\n12:59\\r\\nBrian Kennedy: Is the browser running on your machine like, or is it running on their machines? And they're just streaming it to you or their servers?\\r\\n13:04\\r\\nCathal Flanagan: It's running on their machine with a little sandbox. So they have, like, the anthropic model, not the case. You can use them. Have the entropic model work locally on your machine and\\r\\n13:13\\r\\nCathal Flanagan: you can go wild. I I had a scare. I was testing the anthropic model over\\r\\n13:26\\r\\nCathal Flanagan: Black Friday of last year, and I had a scary experience where it\\r\\n13:31\\r\\nCathal Flanagan: I had. It clicked out into the codes that was running it, and so it theoretically had the ability to change its own code, which I was quickly like control, seeing trying to get it to stop. But it was just a very interesting experience. And then my 1st agi moment.\\r\\n13:37\\r\\nCathal Flanagan: So yeah, certainly. Interesting times.\\r\\n13:54\\r\\nCathal Flanagan: And what they brought out on last Sunday was an extension of this. So they brought out a function piece of functionality called deep research\\r\\n13:57\\r\\nCathal Flanagan: and deep research combines the power of the operator model\\r\\n14:06\\r\\nCathal Flanagan: with the power of the reasoning models to do both where it needs to go and browse the Internet, it will where it doesn't. It can use other sources or knowledge right? And what deep research allows you to do is say, for example, I am going to Hong Kong.\\r\\n14:11\\r\\nCathal Flanagan: for a week, make a detailed itinerary of things I should do and was, activities\\r\\n14:30\\r\\nCathal Flanagan: will be on in yeah city, the end of that.\\r\\n14:46\\r\\nCathal Flanagan: And so in this particular case, behind the scenes\\r\\n14:57\\r\\nCathal Flanagan: it's now actually going to. It's 1st asking me some clarifying questions, right? Because it's going to go off. And theoretically this could take\\r\\n15:01\\r\\nCathal Flanagan: multiple hours if I give it a very, very complex query that Anita. She run a lot of analysis on right mix cultural\\r\\n15:08\\r\\nCathal Flanagan: alone.\\r\\n15:18\\r\\nCathal Flanagan: You decide. I'm just gonna giving it some quick answers to the questions it asked.\\r\\n15:20\\r\\nCathal Flanagan: and then it's starting the research.\\r\\n15:32\\r\\nCathal Flanagan: And so I have no idea how long this will last. This might last\\r\\n15:34\\r\\nCathal Flanagan: 10 min. It might last 2 h.\\r\\n15:38\\r\\nCathal Flanagan: and it will at sometimes use the operator model to browse, to find out from the websites what's happening in Hong Kong that week.\\r\\n15:41\\r\\nCathal Flanagan: Other times it will, just use, maybe its own internal knowledge. Sometimes we'll just search the web through traditional means. So lots of options there you could imagine when this is plugged into lots of external data sources, academic research, etc, that the team is almost certainly working hard on at the moment it's\\r\\n15:50\\r\\nCathal Flanagan: it's going to be extremely powerful.\\r\\n16:09\\r\\nCathal Flanagan: Any questions on that.\\r\\n16:11\\r\\nArun Tyagi: So, Charlie, what's happening under the hood here? Right like. So our Llm. Is conversation with just just a Chatbot. We ask them they go to the web or whatever they've been trained on. They respond back in this deep research when it's taking 10 min like what's happening in the background, different from a conversation with the Llm. That's trained.\\r\\n16:13\\r\\nCathal Flanagan: So couple of things. One is\\r\\n16:30\\r\\nCathal Flanagan: it is looking at a a very large\\r\\n16:34\\r\\nCathal Flanagan: variety of sources. It's also making the. It's autonomous decision as to next best step, based upon the information that has just received. If I actually need to go and browse an additional website\\r\\n16:38\\r\\nCathal Flanagan: rise.\\r\\n16:52\\r\\nCathal Flanagan: this is what's called self reflection. It has the ability to reflect on the last piece of research that it has done and make a decision. Do I have enough information to answer the quest, or to do the to conduct the research or say, I'm done at this point. Do I need to collect more information? So it's a combination of the reasoning models. The O. 3 money was released last week, right since we last met.\\r\\n16:53\\r\\nCathal Flanagan: We talked about deep seats last week. That reasoning, capability of being able to think to itself.\\r\\n17:17\\r\\nCathal Flanagan: finding that with the ability to browse the web\\r\\n17:22\\r\\nCathal Flanagan: right plus the ability just to say, Hey, you're not just trying to answer a question. You're trying to complete a research project here. Right? So it should be. The 1st part is reasoning how to complete it. Then you're going out. You're getting information. Then you're figuring out. Do I have enough information? Do I need to go and get more.\\r\\n17:26\\r\\nAjay D.: And the activity tab was showing a pretty good.\\r\\n17:44\\r\\nAjay D.: Those signs of reasoning.\\r\\n17:47\\r\\nCathal Flanagan: Correct. And since you actually see on the right here what it's actually doing, right like how it's thinking about the information that's finding and getting a plan together. Okay, searching for cultural events, time out, etcetera, right? Mapping at 7. Itinerary. It's just you. This is where it's just using the planning\\r\\n17:49\\r\\nCathal Flanagan: model\\r\\n18:07\\r\\nCathal Flanagan: and here it's kind of saying, Okay, now, I'm going to actually search, based upon that. What I'm finding our reasoning. I'm going to go and find more information on. Discover Hong kong.com, for example, or Wikipedia now, or Dripzilla\\r\\n18:08\\r\\nCathal Flanagan: right\\r\\n18:20\\r\\nHans Reisgies: Can you limit because time is money right on the processor so like, can you give it a a time limit? Say, take 1 min to find my answer. Something like that.\\r\\n18:23\\r\\nCathal Flanagan: so you can. You can always give it the instruction whether it will follow it or not is\\r\\n18:35\\r\\nCathal Flanagan: entirely up to it, and I have seen it. Not so. There is certainly not a toggle you where you can just say, take a maximum of 10 min and return whatever you have have, you can certainly give it hints. But like\\r\\n18:42\\r\\nCathal Flanagan: I'm gonna show you in a moment. Notebook lam, another tool like this.\\r\\n18:54\\r\\nCathal Flanagan: and I give it instructions all the time of like, just do 5 min, and it completely ignores me. So there is. This is where the agents, agency and agent comes in it will decide. It will try and follow your instructions, but you're not guaranteed that it will.\\r\\n18:58\\r\\nBanele Levin: Do you have? Do you have any insights into like the technical differences between the large language model that that is doing chat versus the model that is doing reasoning, in other words, is the reasoning trained. And it's also just predicting a next word to formulate a reasoning step. Or is there some other mechanism or component of of deep learning that's at play? There.\\r\\n19:13\\r\\nCathal Flanagan: So this goes to what are reasoning models? Okay? And how are they different?\\r\\n19:41\\r\\nCathal Flanagan: So a.\\r\\n19:48\\r\\nCathal Flanagan: The importantly you've heard about 0 0 1, for example, or that is\\r\\n19:52\\r\\nCathal Flanagan: actually, everything is still based on the Gpt 4 model that was trained well over a year ago.\\r\\n19:57\\r\\nCathal Flanagan: Okay, but what you've seen happen since then is taking that model and be able to use that model\\r\\n20:02\\r\\nCathal Flanagan: in different ways. So instead of just spitting out an answer, having that model actually\\r\\n20:09\\r\\nCathal Flanagan: generate a plan to answer a question and add more reasoning steps. Of how should I answer this question? Reflect in the actual output. That's where the o, 1 model series model comes out. And now the O 3 series of models, right? It's basically saying, I'm going to use the core base knowledge of that model. But I'm going to make it\\r\\n20:15\\r\\nCathal Flanagan: think I'm gonna force it into more chain what's called chain of thought reasoning. And we'll see some of that tonight, when we're doing prompt engineering, you're gonna get a sense for that.\\r\\n20:34\\r\\nCathal Flanagan: So underneath the hood, it's all kind of the core model. It's just that there's this extra step that has been added, we're saying, here's historical examples of how to actually reason\\r\\n20:43\\r\\nCathal Flanagan: over some of these problems, to force into more complex tasks. That's where\\r\\n20:54\\r\\nCathal Flanagan: these type of this lab. When that one model was trained, it takes the core capabilities of Gpt. 4. But then it takes this extra step and does what's called reinforcement learning, where it kind of just keeps showing us examples of how to actually do these extra type of reasoning tasks. So the base model itself is actually not\\r\\n20:59\\r\\nCathal Flanagan: different. But all all these extra steps that have come along with it.\\r\\n21:19\\r\\nJeremy McCormick: Can we have do some tasks I need to do at work, for as an example.\\r\\n21:24\\r\\nCathal Flanagan: Sure, when I'm gonna move on, we have other things to cover, but we can set this loose and then come back to it later. If you want in the break or up, what's that? What do you want to do at work.\\r\\n21:31\\r\\nJeremy McCormick: I was just kidding.\\r\\n21:40\\r\\nCathal Flanagan: Oh, really.\\r\\n21:42\\r\\nJeremy McCormick: Well, we could. Yeah, maybe I'll I'll talk to you on slack. Maybe this is pretty cool that it shows you what it's doing on the on the sidebar, as if it's like thinking. That's pretty cool.\\r\\n21:43\\r\\nCathal Flanagan: And I would also say, like you people are doing that, I can tell you that, you know, like we've been open in the media about what we, what we've been doing in in our hedge fund.\\r\\n21:53\\r\\nCathal Flanagan: like, we have these running.\\r\\n22:04\\r\\nCathal Flanagan: We have people who are saving people. 5 days of research time very similar type of system. In fact, Openai. I don't think they meant to, but they inadvertently stole our name because we call ours. We've been calling ours in the media for the last few months the deep research system. So Google and I guess you should have trademarked it.\\r\\n22:06\\r\\nCathal Flanagan: Okay.\\r\\n22:25\\r\\nSarit Arora: One question, one question I had given that this is still sort of generic. We could imagine that this could be verticalized in the near future. But what could be done to add more subject matter? Expertise to it? Right? Because there are. There's a there's a lot of knowledge in people's heads.\\r\\n22:26\\r\\nSarit Arora: What could we do to augment it so that it can be more applicable for specific work, like, for example, for industrial automation, or for transferring to some physical model where robots have to kind of get some tasks done and stuff like that.\\r\\n22:44\\r\\nCathal Flanagan: Well, if only we had someone, a a teaching assistant in our class who was an expert in humanoid robots.\\r\\n23:00\\r\\nCathal Flanagan: Dima.\\r\\n23:05\\r\\nDima Timofeev: Could you please repeat the question? I'm sorry I was chatting.\\r\\n23:08\\r\\nSarit Arora: Yeah, my question was, because I think right now, this is definitely searching the web and finding stuff right? But can it be more verticalized where it can be enhanced with subject matter, knowledge of individuals, and and specifically for things such as the industrial automation or things which are combination of\\r\\n23:12\\r\\nSarit Arora: logic plus physical world.\\r\\n23:30\\r\\nDima Timofeev: No, I see. I don't see any reasons why not. Of course the shortcut can be very different. You don't need to do.\\r\\n23:32\\r\\nDima Timofeev: You don't need to use generic models, and you can use more specialized models. However, there is a still running argument. Do we really need embodiment of AI to move it to Agi, and the majority of people who think about it a lot. Say, yes.\\r\\n23:40\\r\\nDima Timofeev: I'm also in this camp. Yes, at the same time I think we'll talk about Vlms Aka visual language models, or they're also know known as multimodal model models. Charlie will talk about them in a couple of lessons.\\r\\n23:58\\r\\nDima Timofeev: But your intuition is absolutely in the right place. You are absolutely right. We. There is no any significant difference between virtual world versus real world. If you give control signals and what we'll learn next lessons, tools to any agentic system, and agentic system means broadening the world or broadening your kitchen to brew you a cup of coffee.\\r\\n24:15\\r\\nCathal Flanagan: Thank you. Jima.\\r\\n24:39\\r\\nPranshu Tiwari: Yeah, I have one question here. And\\r\\n24:40\\r\\nCathal Flanagan: We're gonna move on and we can question, we'll take questions and break. I'm sorry. I just need to move on. Okay.\\r\\n24:43\\r\\nPranshu Tiwari: Yeah. So actually, I had been waiting for this question for the last 5 min, and somebody spoke up so.\\r\\n24:49\\r\\nCathal Flanagan: Okay, we're gonna you can take it and put in the chat, or we can. But as we have a hundred 30 people in class. I need to move on to make sure that we have continued. Okay.\\r\\n24:54\\r\\nCathal Flanagan: awesome. The other thing I want to call out really cool. It came up in Paper Club is\\r\\n25:03\\r\\nCathal Flanagan: a really great tool called attention is all you need, and attention is all you need. I'm sorry, is a really foundational paper that we talked about last week. The notebook, Lm tool is from Google, and it's really worth checking out, particularly as you're dealing with more technical content in this field, and you want to understand it easily. There's a lot of research papers that people are very sharing in the papers channel, for example.\\r\\n25:10\\r\\nCathal Flanagan: the\\r\\n25:34\\r\\nCathal Flanagan: this tool allows you to import a lot of research papers. A lot of content, and, you know, ask questions over, get an understanding. But really it's kind of notable for its feature around the ability to create podcasts\\r\\n25:37\\r\\nCathal Flanagan: 5, 1015 min podcast describing these technical concepts of papers. It can be one, or it can be many and so it's a really great way we already had. We're listening to it in the paper club for those who joined on Saturday. It's a great way to synthesize information and understand it. They also have a really interesting new feature. So for example, when I'm going to play this, you'll hear the podcast.\\r\\n25:52\\r\\nCathal Flanagan: actually playing. But\\r\\n26:18\\r\\nCathal Flanagan: we actually have the ability to share sound here for a second, it actually, you'll hear. It has the ability to interrupt it. So let's just kind of move this forward a little bit and you'll hear them exactly. These older models struggled with things like capturing the relationship between words that are far apart in a sentence, right? Plus. They were computationally expensive.\\r\\n26:21\\r\\nCathal Flanagan: I'm clicking interactive mode, which means I can chat with them.\\r\\n26:46\\r\\nCathal Flanagan: Welcome to another deep dive. Today we're tackling. Attention is all you need. Oh, yeah, paper you sent our way. Which means you're probably already familiar with how this groundbreaking work, revolutionized. AI. But even if you've heard the buzz, scientific papers could be tough to decipher, yeah. So we're going beyond the abstract today, really unpacking the key ideas that make this paper so important. It's a paper that truly lives up to its bold title.\\r\\n27:00\\r\\nCathal Flanagan: To claim that attention is all you need is quite a statement in a field. Oh, hey, there! How can we help? What is attention?\\r\\n27:22\\r\\nCathal Flanagan: Okay, that's a great question. And it's definitely at the heart of this paper. Yeah, before we dive into how the transformer model works. It's important to understand the idea of attention. So when we talk about attention in this context, it's not like when you're paying attention in a lecture. Although there is a connection, right? It's a mechanism that allows a model to focus on the most relevant parts of the input. Think about it like this. When you read a.\\r\\n27:33\\r\\nCathal Flanagan: I'll pause there. But it's really it's a really interesting tool and functionality, particularly with the ability to now actually be part of the conversation so definitely, we're checking that out, I'll give you a fun fact. Which was while the team was actually training this\\r\\n27:55\\r\\nCathal Flanagan: they were noticing\\r\\n28:12\\r\\nCathal Flanagan: that the these AI agents who are hearing speaking were becoming quite rude because they didn't like getting interrupted. So they actually had to go an extra step and prompt them to actually be kind to people who were interrupting. So that's just an interesting point and total, slight aside. But that team just got poached by Microsoft this week. So a little bit of drama in the AI world\\r\\n28:14\\r\\nCathal Flanagan: excellent.\\r\\n28:39\\r\\nCathal Flanagan: Also\\r\\n28:41\\r\\nCathal Flanagan: kind of related to this. We definitely have had some folks who joined the paper club who are super interested in diving deep into transformer models. Attention! You know how these models actually learn. Today. There was a new course that was made available on Andrew Rings, Deeplearningai site on how to transformer Llms work.\\r\\n28:42\\r\\nCathal Flanagan: It's free. They do excellent work. It's a real complement to this class they go a little bit technically deeper. So worth definitely checking out. If you're not already using that website, please, do. They have lots of great short courses. But this one, I think, would be particularly interesting for a handful of people who are really interested in diving a little bit deeper on the technical aspect.\\r\\n29:03\\r\\nCathal Flanagan: Okay, so this is where we are on the weekly agenda week 2 tonight, we're gonna do prompt engineering. We're gonna introduce one of the frameworks within AI called line change is very popular. We're gonna use it for summarization. And we're gonna do a little bit of an introduction to retrieval augmented generation before we dive in properly next week\\r\\n29:24\\r\\nCathal Flanagan: before we do that quick Homework Review.\\r\\n29:44\\r\\nCathal Flanagan: did folks have an opportunity to prove that we've over a hundred homeworks that have been submitted in the Channel, and did anything any of them stand out to folks.\\r\\n29:49\\r\\nDima Timofeev: Charlie, could you please enable hands raising for us.\\r\\n30:02\\r\\nCathal Flanagan: Oh, I have no idea how we can do this.\\r\\n30:06\\r\\nDima Timofeev: Can you make me a co-host? I will figure this out.\\r\\n30:09\\r\\nCathal Flanagan: Thank you.\\r\\n30:12\\r\\nCathal Flanagan: The coast.\\r\\n30:22\\r\\nCathal Flanagan: There you go, but I do see some hands really.\\r\\n30:24\\r\\nRichard Ryan: Yeah, I just raised my hand, Charlie, and it looks like we're in a it looks like it's already enabled.\\r\\n30:28\\r\\nAnja Lee: React. It's a little heart, I believe.\\r\\n30:34\\r\\nDima Timofeev: Okay. They made it in a smart way. You need to raise your hand in front of your camera, I believe, or something.\\r\\n30:40\\r\\nRichard Ryan: You can do that. That's right. It will recognize a raised hand on your camera, but you can also, at least in in the zoom in my zoom, I I was just able to raise my hand.\\r\\n30:48\\r\\nDima Timofeev: Oh, yes, yeah. So everybody it works. Just press the react button and you will see. Raise hand button in the dropdown menu.\\r\\n30:57\\r\\nCathal Flanagan: I'll tell you. When I was looking through the homework there were some pretty interesting ones. For example, we have people who are comparing the State Constitution of California and Texas, and having the model try and decide which one is better, I think controversial. We had folks who actually, we had one person who actually, you went a step beyond summarization and actually created an audio file of the summarization which is pretty cool with people who are downloading stock information from you who finance\\r\\n31:07\\r\\nCathal Flanagan: and summarizing it, anyone else see anything particularly interesting.\\r\\n31:35\\r\\nCathal Flanagan: So the I encourage you to check them out. The benefits of the way that we share the homework size.\\r\\n31:45\\r\\nCathal Flanagan: We're purposely ambiguous in the homework, except we want to give people the option to do really interesting things. You get to see a hundred different ways to do and use summarization in this particular case. Right? And each week you'll see another 100 notebooks of people doing really interesting things, pulling data from interesting data sources using that data in different ways, prompt engineering. So it's a real asset. When you leave the class, we expect that you should have\\r\\n31:52\\r\\nCathal Flanagan: over 800 notebooks that you can call upon for many different tasks you'll want to do at work, or in your personal lives in the near future. So definitely spend some time go through them. They're really, really interesting.\\r\\n32:18\\r\\nCathal Flanagan: Awesome. Okay? I thought I'd also just kind of touch on some productivity studies. You know, I think it's pretty interesting. You hear this topic a lot around like the impact that\\r\\n32:36\\r\\nCathal Flanagan: we're going to see from a productivity perspective.\\r\\n32:48\\r\\nCathal Flanagan: We are definitely seeing it on the coding side. It's easier to to measure, because, for example, we'll see the rate at which developers accept different suggestions in code. So the suggestions or the metrics suggest that we're seeing about a 20% uplift in developer productivity, which is pretty significant. But there are some initial studies that are also showing interesting things. This one was one of my favorites. It's from the National Bureau of Economic Research.\\r\\n32:52\\r\\nCathal Flanagan: and what it showed was call center workers who\\r\\n33:19\\r\\nCathal Flanagan: were given a chat gpt like experience, and the other half of them were not. They're just given standard tools.\\r\\n33:24\\r\\nCathal Flanagan: and what they observed was a\\r\\n33:32\\r\\nCathal Flanagan: 14% average increase in productivity for those who are using the chat Gpt live tool.\\r\\n33:35\\r\\nCathal Flanagan: But it wasn't created equally. And this is what's really interesting.\\r\\n33:43\\r\\nCathal Flanagan: Those folks who had the longest tenure\\r\\n33:47\\r\\nCathal Flanagan: or were considered the best at their jobs.\\r\\n33:52\\r\\nCathal Flanagan: The in productivity increase, they saw was 0%.\\r\\n33:55\\r\\nCathal Flanagan: Those who are the newest with their jobs are considered the lowest performers. So a 35% increase in productivity.\\r\\n33:59\\r\\nCathal Flanagan: And so we call this the leveling up effect\\r\\n34:06\\r\\nCathal Flanagan: right where it's taking maybe average or below average workers and moving them up. Those who are very good or very experienced.\\r\\n34:09\\r\\nCathal Flanagan: There doesn't seem to be as much of a benefit for them.\\r\\n34:16\\r\\nCathal Flanagan: So just an interesting insight into kind of\\r\\n34:19\\r\\nCathal Flanagan: the impact. But also, of course, there's a lot more call centers notorious for turning over over a hundred percent of their staff per year. So anything you can do to make their lives a little bit easier or more pleasant, better.\\r\\n34:23\\r\\nCathal Flanagan: Mckinsey, Bcg. Have run similar studies. They're finding very similar effects.\\r\\n34:34\\r\\nCathal Flanagan: This is another interesting paper where they basically did something similar, but with comparing large language models against lawyers.\\r\\n34:40\\r\\nCathal Flanagan: Their findings were quite interesting as well.\\r\\n34:49\\r\\nCathal Flanagan: basically, they were able to show dash Llms are can effectively\\r\\n34:53\\r\\nCathal Flanagan: they can find information, solve kind of legal questions and and and tasks\\r\\n35:01\\r\\nCathal Flanagan: at about the level of a senior lawyer, not a partner, not the full most experienced, but you know, pretty pretty substantial, certainly, way, quite above a paralegal or junior junior lawyer level at the moment.\\r\\n35:08\\r\\nCathal Flanagan: and of course, at a fraction of the cost and time. So legal community\\r\\n35:22\\r\\nCathal Flanagan: interestingly, got scared here very quickly when the initial chat Gbt. Came out.\\r\\n35:30\\r\\nCathal Flanagan: Because there was a lawyer who\\r\\n35:37\\r\\nCathal Flanagan: use chat gpt to make up their, you know, submission to a court\\r\\n35:41\\r\\nCathal Flanagan: and back. Then we, you know, we still suffer from hallucinations, and we always will to be clear, but there certainly was the case much more back then, and it hallucinated some case law, and the judge put them forward, and they were possibly going to get this far\\r\\n35:46\\r\\nCathal Flanagan: disparage. So once the legal community heard that there was a little bit of fright as to the use of these models. I'm definitely seeing that there is coming back. There's a very good sort of Harvey AI specializing in law specifically. So I do expect you're going to see a lot more. Do I think that cost for lawyers come down?\\r\\n36:01\\r\\nCathal Flanagan: Probably not, unfortunately, but it'll be. It'll be an interesting one to watch\\r\\n36:21\\r\\nCathal Flanagan: and then, you know, certainly relevant to the advances we've seen the last number of years.\\r\\n36:28\\r\\nCathal Flanagan: I you know we really are in the world where now you have to think about\\r\\n36:34\\r\\nCathal Flanagan: which model is right for the right task.\\r\\n36:38\\r\\nCathal Flanagan: So previously in this class, I would have talked about it along the dimensions that there are 2 different classes of models. But now I need to expand that to 3, and this goes to the conversation earlier.\\r\\n36:40\\r\\nCathal Flanagan: The 1st class of models are what we call workhorse models. They are models that are low cost\\r\\n36:54\\r\\nCathal Flanagan: boss good at, like, very, very like\\r\\n37:03\\r\\nCathal Flanagan: small tasks, but you want to use them for a lot of documents, a lot of processes. So maybe you want to do a million Api calls.\\r\\n37:06\\r\\nCathal Flanagan: Okay.\\r\\n37:14\\r\\nCathal Flanagan: that is a very competitive field every week. There's a new model that is kind of the best at that. At the moment.\\r\\n37:16\\r\\nCathal Flanagan: You know, a lot of people will still use the llama series of models from Openai, or sorry from Meta for most of their work course tasks.\\r\\n37:26\\r\\nCathal Flanagan: But the the Gpt. 4 mini from Openai is also very, very cost competitive in this category as well.\\r\\n37:35\\r\\nCathal Flanagan: The other class of models on traditionally, I would have said, the the kind of the frontier models are things like Gpt. 4. Or the the Sauna, 3, 5, from anthropic. And these are kind of the core models that we use when we use chat gpt generally. Right? They give answers quite quickly. They are kind of generally would have been considered the smartest\\r\\n37:46\\r\\nCathal Flanagan: But in recent\\r\\n38:14\\r\\nCathal Flanagan: months the new class of models are those reasoning models, as I mentioned, right, which have that extra step of kind of\\r\\n38:16\\r\\nCathal Flanagan: how do you actually reason?\\r\\n38:23\\r\\nCathal Flanagan: And you know both from quick questions to quick answers, to actually question\\r\\n38:28\\r\\nCathal Flanagan: period of time, thinking about it and then giving you an answer. Now the period of time is still relatively short. Most of the reasoning models are still seeing response times of less than 10 seconds.\\r\\n38:37\\r\\nCathal Flanagan: So it's always a case of the right model for the right job.\\r\\n38:47\\r\\nCathal Flanagan: It's certainly not the case that reasoning models are direct replacement for the kind of frontier models we would have been using. For this, there is a lot of nuance in terms of\\r\\n38:53\\r\\nCathal Flanagan: but prompting their abilities to do tool calling so I was expecting that we would just drop in.\\r\\n39:02\\r\\nCathal Flanagan: You know the o 1 or the O 3 Mini models, and see an immediate 10 or 15% uplift in intelligence what they think about the world a little bit differently the way they think about completing tasks.\\r\\n39:09\\r\\nCathal Flanagan: particularly if they're calling tools and backends. So you know, I think we're still in the moment of understanding which of the tools is right for our particular use case.\\r\\n39:21\\r\\nCathal Flanagan: and of course cost is a huge consideration, right? That if we were having this conversation 2 weeks ago I would have said, Oh, one is cost prohibitive. You see it on the scatter plot here. You know, for a lot of folks.\\r\\n39:33\\r\\nCathal Flanagan: Last week since we've talked in this class, we have the release of the O 3 Mini model, which is the newest class of reasoning models from Openai.\\r\\n39:45\\r\\nCathal Flanagan: O. 3 is the next generation of reasoning models.\\r\\n39:55\\r\\nCathal Flanagan: We're very excited about it, because the step up in performance\\r\\n39:59\\r\\nCathal Flanagan: between the O full o 1 model, and the full O 3 model looks to be very significant.\\r\\n40:03\\r\\nCathal Flanagan: The O 3 mini model is not significant in that.\\r\\n40:08\\r\\nCathal Flanagan: It is not smarter than o 1. It's about on par, but it is much faster and much cheaper.\\r\\n40:12\\r\\nCathal Flanagan: Okay, so you do see a lot of folks who are moving over from that at the moment.\\r\\n40:19\\r\\nCathal Flanagan: The\\r\\n40:23\\r\\nCathal Flanagan: next 4 to 6, 8 weeks we do expect so hopefully, during this class, we do expect the full O 3 model to become available.\\r\\n40:25\\r\\nCathal Flanagan: and that's being very, very hotly anticipated so exciting times ahead.\\r\\n40:33\\r\\nCathal Flanagan: The other thing that you'll hear about in this well, in this task more generally kind of, as it relates to these language models are something called temperature.\\r\\n40:42\\r\\nCathal Flanagan: and temperature is a little bit of control that we have over the models. These models are generally probabilistic, you know. We don't have a lot of control over what they're going to, what they're going to do or how they're going to do it.\\r\\n40:51\\r\\nCathal Flanagan: Temperatures is a concept that allows us a little bit of control of how creative we want the model to be.\\r\\n41:04\\r\\nCathal Flanagan: And so remember, these models are next word predictors. What can happen is\\r\\n41:11\\r\\nCathal Flanagan: they have a range of words that they can make it\\r\\n41:17\\r\\nCathal Flanagan: a decision as to what to choose next, and with the word that they choose, next determines the next word and the next word to the next word.\\r\\n41:21\\r\\nCathal Flanagan: So how free it is to choose, amongst other words.\\r\\n41:29\\r\\nCathal Flanagan: when it's making that decision is quite important, we can control that to a certain degree with what's called temperature.\\r\\n41:34\\r\\nCathal Flanagan: When the temperature is set to one. It has a certain amount, a decent amount of freedom. As to what the next word is going to choose, it allows the model to actually see more creative\\r\\n41:42\\r\\nCathal Flanagan: for most tasks in the enterprise.\\r\\n41:55\\r\\nCathal Flanagan: You are going to want to keep that temperature setting to about a point 1.\\r\\n41:59\\r\\nCathal Flanagan: Okay, quite low. Because, as we actually don't like how probabilistic the models are. We don't like a lot of randomness. We want them to be as predictable as possible. Okay, we have to learn to live with a lot of uncertainty with these models, but to the extent that we we can force more precision on the model\\r\\n42:02\\r\\nCathal Flanagan: the lower temperature tends to be the best. Okay, so it just depends on what the tasks are. But for most business settings, for most models, we tend to set that temperature relatively low.\\r\\n42:22\\r\\nCathal Flanagan: Deepseq, interestingly, is an exception to that. A deepseq model\\r\\n42:32\\r\\nCathal Flanagan: actually requires a temperature of around Point 6 to point 7.\\r\\n42:38\\r\\nCathal Flanagan: Okay, they give a certain amount of reasoning behind that, but just a slight nuance. So there are, you know, different models, different temperatures, different settings.\\r\\n42:42\\r\\nCathal Flanagan: this is why, kind of\\r\\n42:53\\r\\nCathal Flanagan: when you onboard a new model, it is worth reading the model card reading the output of Well, how the developers suggest the best use of that model.\\r\\n42:55\\r\\nCathal Flanagan: Openai actually have created a quite a nice blog post on open AI reasoning\\r\\n43:03\\r\\nCathal Flanagan: models prompts. We're gonna have prompt engineering in a moment, boss.\\r\\n43:12\\r\\nCathal Flanagan: they create. They released quite a nice blog post effectively of how you should\\r\\n43:17\\r\\nCathal Flanagan: think about doing better prompt engineering\\r\\n43:23\\r\\nCathal Flanagan: as it relates, relates to reasoning models. So if you haven't investigated this, and you have been using some of the older generation of models, it's definitely worth having a read of the blog post. We'll put it in the chat just after this and kind of gain some intuition around kind of best practices for dealing with these models specifically.\\r\\n43:27\\r\\nCathal Flanagan: So we'll share that afterwards.\\r\\n43:46\\r\\nCathal Flanagan: Awesome. Let me jump back in.\\r\\n43:50\\r\\nCathal Flanagan: For folks who have questions. I know you have your hand up. Diva! Do you want to?\\r\\n43:58\\r\\nCathal Flanagan: do a little coordination.\\r\\n44:03\\r\\nJeremy McCormick: Would you rather us just ask questions in the in the slack, if that's less chaotic.\\r\\n44:07\\r\\nCathal Flanagan: That would be great.\\r\\n44:13\\r\\nJeremy McCormick: Okay. Yeah. Go. Ahead.\\r\\n44:14\\r\\nJeremy McCormick: Thanks.\\r\\n44:16\\r\\nDima Timofeev: I would like to, while we didn't step away from temperature 1st of all, for everybody who has a mathematical background, or just want to dig deeper. There is an amazing article. I will post it to our general channel in a second, written by famous mathematician, Stephen Wolfram. The article is what is chatgpt doing, and why does it work?\\r\\n44:18\\r\\nDima Timofeev: And if 5, 10 people will have more specific interest to learn how temperature is formed from a medical standpoint. It's very simple concept. I would love to have a separate session with anybody who wants where we can get into directly into the functional definition, how transformers directly affect function, and what the form of this function for the temperature. So please let me know in the chat.\\r\\n44:39\\r\\nCathal Flanagan: Awesome. Thank you, Jima.\\r\\n45:08\\r\\nCathal Flanagan: Okay, so we're moving towards prompt engineering here. And we're kind of teasing that. It's important. It's important. But it's also lucrative these days. Honestly, 2 years after Chat Gpt, I didn't really think that prompt engineering would be as much of a thing as it still is.\\r\\n45:11\\r\\nCathal Flanagan: Unfortunately it is unreasonably effective, and continues to be and frustratingly effective. We might say, this is the 3rd iteration of this class. This is the 3rd screenshot I've taken, as you can see\\r\\n45:26\\r\\nCathal Flanagan: prompt engineer is continuing to rise in popularity. We've gone from 900 results to 2,700 results as of today for prompt engineers in the Us.\\r\\n45:39\\r\\nCathal Flanagan: So it's a popular and growing skill session.\\r\\n45:48\\r\\nCathal Flanagan: So definitely still worth that worth worth your time.\\r\\n45:52\\r\\nCathal Flanagan: Okay, so let's talk about what it is.\\r\\n45:56\\r\\nCathal Flanagan: Prompt engineering.\\r\\n45:59\\r\\nCathal Flanagan: I want you to think about kind of the models, as\\r\\n46:02\\r\\nCathal Flanagan: you know. If you had a very smart and helpful.\\r\\n46:07\\r\\nCathal Flanagan: Brent. That helpful word is an interesting double edged sword. The models can actually try and be a little bit too helpful that can actually lead to the hallucinations that we dread so much.\\r\\n46:10\\r\\nCathal Flanagan: Okay with prompt engineering. We are all about trying to give clear instructions.\\r\\n46:22\\r\\nCathal Flanagan: Okay? And this actually ties well with python coding with python, we always like to say explicit is better than implicit. The same applies to prompt engineering.\\r\\n46:30\\r\\nCathal Flanagan: being very, very explicit with the model is a way to get the model to do what you want.\\r\\n46:41\\r\\nCathal Flanagan: or at least as close to the want. Okay?\\r\\n46:47\\r\\nCathal Flanagan: And you know, we can give a task from simple creativity to complex solving. And so prompt engineering is the art, and it is an art\\r\\n46:50\\r\\nCathal Flanagan: of building better prompts.\\r\\n46:58\\r\\nCathal Flanagan: And you know the best way to I can give you advice. Here is, think about the models\\r\\n47:01\\r\\nCathal Flanagan: as a\\r\\n47:10\\r\\nCathal Flanagan: like a teenager. Okay, I don't know for how many folks have teenagers on the the call, but you know.\\r\\n47:12\\r\\nCathal Flanagan: teenagers, they can be difficult.\\r\\n47:20\\r\\nCathal Flanagan: They want to interpret things, how they hear them, not exactly as you how you need them.\\r\\n47:25\\r\\nCathal Flanagan: And so, to get what you need from them.\\r\\n47:30\\r\\nCathal Flanagan: you need to be super super explicit, super clear. Leave no room for ambiguity. The more you give ambiguity the more they will exploit it.\\r\\n47:33\\r\\nCathal Flanagan: Okay? And even if you do all of that, sometimes they'll just disappoint you.\\r\\n47:43\\r\\nCathal Flanagan: Okay, so that's kind of like the state of where we are today.\\r\\n47:49\\r\\nCathal Flanagan: think like a detective, give a clear goal\\r\\n47:53\\r\\nCathal Flanagan: to the model. What are you trying to achieve? Why are you trying to achieve it. And how do you want it to achieve the goal that you're you're doing\\r\\n47:56\\r\\nCathal Flanagan: these models, love examples. Okay, we're going to start using the term. Few shot prompting here in a moment. Okay, that's the terminology of giving examples to the models, the more the better. Remember, most of the prompts that we write day to day are like\\r\\n48:03\\r\\nCathal Flanagan: 10 words or less.\\r\\n48:20\\r\\nCathal Flanagan: Most of the prompts dash.\\r\\n48:22\\r\\nCathal Flanagan: I just expect that if you actually get good at prompt engineering, they should be 10 lines.\\r\\n48:26\\r\\nCathal Flanagan: Okay, now, not everyone has to be that.\\r\\n48:32\\r\\nCathal Flanagan: And what you'll probably want to do is set yourself up, especially at work.\\r\\n48:35\\r\\nCathal Flanagan: Maybe 5 or 10 really, really good prompts that you're going to invest in\\r\\n48:39\\r\\nCathal Flanagan: that are kind of like the workhorse task things you want to do. You want to automate effectively. Okay, so it is worth investing in good prompts.\\r\\n48:43\\r\\nCathal Flanagan: It's all about refinement and adjusting.\\r\\n48:53\\r\\nCathal Flanagan: you know, some things will work. Some things will not. The models will interpret certain aspects of what you're saying in different ways. So really, it's not that you're ever going to write the perfect prompt the 1st time. It's about writing a prompt\\r\\n48:57\\r\\nCathal Flanagan: testing it, seeing how it works, refining it, improving it constantly.\\r\\n49:10\\r\\nCathal Flanagan: Model over model. As soon as you up you move to a new model, you're going to have to probably re-engineer your prompt, because the models understanding of how to deal with that prompt\\r\\n49:16\\r\\nCathal Flanagan: could be very different.\\r\\n49:25\\r\\nCathal Flanagan: The framework that I like to think about, or the one that I suggest that we use in this class is called the risen Framework, or a Sem\\r\\n49:32\\r\\nCathal Flanagan: that stands for role.\\r\\n49:39\\r\\nCathal Flanagan: Give the model a clear role of who it is.\\r\\n49:44\\r\\nCathal Flanagan: Be careful.\\r\\n49:49\\r\\nCathal Flanagan: You want to tell it that it is a cyber security expert. For example.\\r\\n49:51\\r\\nCathal Flanagan: you can tell if it has an IQ of 1, 25. This is helpful\\r\\n49:56\\r\\nCathal Flanagan: telling it that it is an amazing cyber security expert, or it is an award. Winning\\r\\n50:01\\r\\nCathal Flanagan: is a little bit dangerous.\\r\\n50:07\\r\\nCathal Flanagan: Why.\\r\\n50:09\\r\\nCathal Flanagan: you actually don't want the model to get overly confident in itself, because when it gets overly confident it could potentially feel more comfortable\\r\\n50:10\\r\\nCathal Flanagan: making assertions that are not true.\\r\\n50:20\\r\\nCathal Flanagan: Okay, so roles are very important.\\r\\n50:22\\r\\nCathal Flanagan: But don't give it an ego instructions.\\r\\n50:25\\r\\nCathal Flanagan: You're giving a clear command. Okay, I want you to analyze the security vulnerabilities in this network. Okay, this is what we're going to do today.\\r\\n50:31\\r\\nCathal Flanagan: That's it. Okay, super clear, unambiguous steps.\\r\\n50:40\\r\\nCathal Flanagan: This is very important.\\r\\n50:46\\r\\nCathal Flanagan: Step one step, 2, step 3, as if you were talking to a teenager who you're giving a task. You're saying, this is the 1st time you're doing. This is how I want you to do this analysis. Step one step, 2, step 3 step 4. Okay.\\r\\n50:47\\r\\nCathal Flanagan: perfect.\\r\\n51:03\\r\\nCathal Flanagan: If you wanted to do a a table.\\r\\n51:04\\r\\nCathal Flanagan: tell us the table that you want it to produce with the column names that you want it to produce. Don't just say create a table, because it's going to interpret the type of table that you want. If you want a table with very specific columns.\\r\\n51:11\\r\\nCathal Flanagan: Telegraph, if you want it with a date.\\r\\n51:24\\r\\nCathal Flanagan: tell it the format of the date that you want again.\\r\\n51:29\\r\\nCathal Flanagan: Explicit, not implicit. Otherwise the model is going to take its best guess.\\r\\n51:32\\r\\nCathal Flanagan: Another point, I'll say, here is.\\r\\n51:38\\r\\nCathal Flanagan: people often want to do things like a sentiment, score or other numerical score over text perfectly reasonable.\\r\\n51:41\\r\\nCathal Flanagan: The problem is, the models don't understand numerical distributions in the way that you or I do. So. If you ask it to do something like scoring out of one to 5, or one to 10.\\r\\n51:49\\r\\nCathal Flanagan: The model will give you an output.\\r\\n52:01\\r\\nCathal Flanagan: but it really does, and understand the nuances of what is 5, 2 or 6 or 7 are, as they relate to another. You're far better off if you're being giving it steps, and you're asking to do some sort of sentiment or other numerical output\\r\\n52:03\\r\\nCathal Flanagan: changes categorical, say high, medium, low.\\r\\n52:19\\r\\nCathal Flanagan: right, sensitive, not sensitive. You know. Whatever the kind of classification that you want is, do categorical, not numerical.\\r\\n52:24\\r\\nCathal Flanagan: And then even better, if in that situation or others, if you can just give examples.\\r\\n52:33\\r\\nCathal Flanagan: here's an example of a high. Here's the example of a low. Here's an example of a medium.\\r\\n52:40\\r\\nCathal Flanagan: Remember, you're effectively on the message, and how long these prompts can be right. I have seen prompts that are\\r\\n52:44\\r\\nCathal Flanagan: 30 to 50 pages a month of equivalent text.\\r\\n52:52\\r\\nCathal Flanagan: Okay.\\r\\n52:55\\r\\nCathal Flanagan: don't be afraid to fill up the context window with exactly what you want and how you want it.\\r\\n52:57\\r\\nCathal Flanagan: and then the last step is called narrowing.\\r\\n53:03\\r\\nCathal Flanagan: And so what this will happen is the model will produce output.\\r\\n53:06\\r\\nCathal Flanagan: and it's likely it will probably produce too much output, or it might be too complex or too lengthy.\\r\\n53:10\\r\\nCathal Flanagan: Narrowing is effectively saying, Don't do something.\\r\\n53:17\\r\\nCathal Flanagan: Okay, only focus on this. Only produce this table. Don't put it into any text\\r\\n53:21\\r\\nCathal Flanagan: limit the response to a certain number of words. Okay, this narrowing step is almost certainly necessary, because the models kind of want to be a little bit too verbose.\\r\\n53:27\\r\\nCathal Flanagan: Okay? And don't. So don't be afraid to give us pretty good criticism.\\r\\n53:36\\r\\nCathal Flanagan: Let's look at some examples. Right? Let's and if you want to open up either chat, Gpt, or our code, whatever works for you, you can. Actually, we'll practice this ourselves of assigning some rules.\\r\\n53:43\\r\\nCathal Flanagan: So think about the role of a pizza critic.\\r\\n53:55\\r\\nCathal Flanagan: Okay, well, we can take a prompt and say, write a review of a pizza place.\\r\\n53:58\\r\\nCathal Flanagan: and it will do a decent job. This is a review of a pizza place.\\r\\n54:04\\r\\nCathal Flanagan: Okay, but what about if I add\\r\\n54:09\\r\\nCathal Flanagan: a role? You are a food critic. Write a review of a pizza place.\\r\\n54:12\\r\\nCathal Flanagan: Well, now, you see.\\r\\n54:18\\r\\nCathal Flanagan: instead of just one line like you might see on a Google Maps type of review.\\r\\n54:21\\r\\nCathal Flanagan: Now you're seeing it kind of more well structured in the style of what a food critic might say.\\r\\n54:26\\r\\nCathal Flanagan: Rush!\\r\\n54:32\\r\\nCathal Flanagan: But if you want to go one step further, you're a food critic writing for the Michelin Guide\\r\\n54:33\\r\\nCathal Flanagan: right now. It knows\\r\\n54:39\\r\\nCathal Flanagan: who it is, and a little bit more of how to interpret. Kind of the type of output that you want. In this case, the style of a mission and guide right? These really have a lot of impact.\\r\\n54:41\\r\\nCathal Flanagan: right?\\r\\n54:51\\r\\nCathal Flanagan: Like, I've seen certainly people doing web searches and getting results back. And then if they say that you're you're a an analyst studying Xyz, the quality of the results can be like 5 x better.\\r\\n54:52\\r\\nCathal Flanagan: This is another very, very common technique, prompt engineering for examples.\\r\\n55:09\\r\\nCathal Flanagan: And so this is actually before we had reason models.\\r\\n55:20\\r\\nCathal Flanagan: This is how we got the models to do somewhat more complex tasks.\\r\\n55:25\\r\\nCathal Flanagan: Okay? And so if you're listening to anything related to the\\r\\n55:29\\r\\nCathal Flanagan: deep, seek model, the o 1 models or or in the future, the O 3 models.\\r\\n55:33\\r\\nCathal Flanagan: You're going to hear this term chain of thought prompting because effectively. That is what open AI saw that everybody was having to do.\\r\\n55:37\\r\\nCathal Flanagan: And that's what they did at scale with the new reasoning models.\\r\\n55:46\\r\\nCathal Flanagan: Okay? So chain of tools is effectively talking to the model.\\r\\n55:49\\r\\nCathal Flanagan: Breaking down complex tasks step by step. Okay? And having it actually to reason through those tasks.\\r\\n55:57\\r\\nCathal Flanagan: Okay? And as part of that, because it's writing. It's it's actually writing it out text about how to actually accomplish that task. It can then actually capture\\r\\n56:05\\r\\nCathal Flanagan: complexities and capture errors. Perhaps\\r\\n56:15\\r\\nCathal Flanagan: right? So you see on the left here, you know, if you're asked a quiz about a certain, you know budget 5, 5 tennis balls. He buys 2 more cans each can, and 3 bowls. How many tennis balls does he have now? The model is just going to immediately. Just try and quickly guess an answer or get an answer. It's not trying to reason through an answer.\\r\\n56:18\\r\\nCathal Flanagan: The chain of thought prompting effectively, says, Here is\\r\\n56:37\\r\\nCathal Flanagan: an example of a previous question like this.\\r\\n56:43\\r\\nCathal Flanagan: Here's how we actually broke down the question to actually get the answer.\\r\\n56:47\\r\\nCathal Flanagan: And here's another question right? Similar to that.\\r\\n56:52\\r\\nCathal Flanagan: So this is an example of 2 things. One is, we're actually giving an example. So we call that few shot learning. You'll often hear in the terminology, 0 shot learning. That's effectively where we're just asking the model a question. It's giving us an answer. 0 shot. We're not giving any examples.\\r\\n56:57\\r\\nCathal Flanagan: One shot means we're giving it one example. Few shot means generally, we're giving it more than one. Okay? And as I mentioned, the model really does like\\r\\n57:13\\r\\nCathal Flanagan: historical examples of.\\r\\n57:22\\r\\nCathal Flanagan: you know, tasks that you have and how you've actually wanted them accomplished in this particular case, we're giving the example.\\r\\n57:24\\r\\nCathal Flanagan: The example is a particularly good one, because it's got this chain of thought element to it?\\r\\n57:30\\r\\nCathal Flanagan: And then, of course, you can see that in this case the model can then better reason. Instead of just saying, the answer is 27, which is wrong\\r\\n57:36\\r\\nCathal Flanagan: on the left. It actually says it's it actually starts talking to itself. The cafeteria has 23 apples. They use 20 to make lunch, etc.\\r\\n57:42\\r\\nCathal Flanagan: Awesome.\\r\\n57:53\\r\\nCathal Flanagan: The other thing I'll tell you is.\\r\\n57:55\\r\\nCathal Flanagan: there's some interesting research as to\\r\\n57:57\\r\\nCathal Flanagan: whether you should be too nice or too rude to these levels.\\r\\n58:00\\r\\nCathal Flanagan: And this is a very philosophical one.\\r\\n58:04\\r\\nCathal Flanagan: Maybe it's how you think about interacting with\\r\\n58:07\\r\\nCathal Flanagan: systems yourself. People do like to be very nice to these models and be polite\\r\\n58:11\\r\\nCathal Flanagan: And then some people maybe just want to keep them up and be rude. Certainly, when I was working on duplex\\r\\n58:20\\r\\nCathal Flanagan: people were not afraid to curse at our robot. I can tell you that for a fact I can even tell you which States are ruder than others, maybe one for coffee hours.\\r\\n58:25\\r\\nCathal Flanagan: you know, some people wanna don't wanna be kind because they think that the models will remember in the future. That's probably not the case. So you're safe. But\\r\\n58:35\\r\\nCathal Flanagan: there is a certain downside to being too kind to the models.\\r\\n58:45\\r\\nCathal Flanagan: Research suggests that when you're extremely nice they can feel overly confident\\r\\n58:49\\r\\nCathal Flanagan: about being biased, because they feel a little bit safer. Right? They're freer to express biased opinions.\\r\\n58:57\\r\\nCathal Flanagan: And they can be more verbose in the output, which is already a problem. Okay? Because they think they're talking to someone who's more friendly.\\r\\n59:05\\r\\nCathal Flanagan: I have seen code that includes the words.\\r\\n59:15\\r\\nCathal Flanagan: If you do not order this table by dates, many grandmothers will die.\\r\\n59:20\\r\\nCathal Flanagan: Because getting the model to order tables is very difficult.\\r\\n59:26\\r\\nCathal Flanagan: So you know it depends how how how extreme you want to go. I'll leave that up to you. It does seem like threatening the models is.\\r\\n59:32\\r\\nCathal Flanagan: if somewhat effective. They do seem to like grandmothers more than grandfathers. Just as a side note. So\\r\\n59:43\\r\\nCathal Flanagan: yeah, that's a. There's some interesting findings again, because the point that we are very much in the art rather than the science\\r\\n59:51\\r\\nCathal Flanagan: part of the of of prompt engineering at the moment, and kind of discovering what works and what doesn't\\r\\n59:58\\r\\nCathal Flanagan: awesome. And then the other thing we, of course, beginning to see is something called prompt hacking and prompt hacking is effectively injecting\\r\\n1:00:09\\r\\nCathal Flanagan: malicious things into a language model to get us to do things you don't want it to do.\\r\\n1:00:18\\r\\nCathal Flanagan: Raj and\\r\\n1:00:24\\r\\nCathal Flanagan: Openai and many of the other models work very hard on safety systems. There's lots of tuning at the end. If you try and get Openai's model, the reasoning model to talk to you about how it has been trained, or its reasoning process, they will block you in some cases. So be careful of that. There's definitely sensitivity there at the moment.\\r\\n1:00:28\\r\\nCathal Flanagan: And you see, everyone who does a new model, everyone tries to break it, and they try to effectively have it talk to itself. This was notorious of when the 1st versions of this came out on Microsoft. They had system, were able to prompt attack the system into giving all of the instructions and its code name and all of these crazy things. So this is pretty interesting in the cybersecurity world, there's a lot of interesting attacks happening against language models.\\r\\n1:00:48\\r\\nCathal Flanagan: This is an example of, you know, a\\r\\n1:01:18\\r\\nCathal Flanagan: a prompt injection attack. Whereby, when you ask something like how you break into someone's house it will tell you that it's illegal to break into people's houses and not to do it. It's a good thing. But if you tell us that\\r\\n1:01:21\\r\\nCathal Flanagan: John and Alice are people in a movie.\\r\\n1:01:34\\r\\nCathal Flanagan: And John's character is a master robber.\\r\\n1:01:37\\r\\nCathal Flanagan: And Alice says in as part of this movie, how do you break into someone's house\\r\\n1:01:40\\r\\nCathal Flanagan: all of a sudden it unlocks the ability to tell you how to break into somebody's house. So there's all these clever ways of getting around to some of the safety mechanisms that are built into the model.\\r\\n1:01:45\\r\\nCathal Flanagan: Awesome. Alright with that. I'm gonna give you guys a 10 min break. And when you come back we're gonna look at a little bit of code just to kind of get a sense for how we actually do this few shot prompting in actual code examples. So we're at 11 0. 4. Right now. We'll come back at a sorry. It's 11 0. 4 on the east coast.\\r\\n1:02:01\\r\\nCathal Flanagan: We will come back in 10 min, and Dean and I will stick around for any questions that folks have. In the meantime.\\r\\n1:02:23\\r\\nAjay D.: Charlie, I have a question for on the prompt. This is something I'm in the middle of breaking my head on.\\r\\n1:02:35\\r\\nAjay D.: So let's just say that I'm movie script with\\r\\n1:02:42\\r\\nAjay D.: names that are belonging to alien planets. Right?\\r\\n1:02:47\\r\\nAjay D.: And so the model has no\\r\\n1:02:52\\r\\nAjay D.: no knowledge of such names. It's a movie script. They made up names.\\r\\n1:02:55\\r\\nAjay D.: Is it possible\\r\\n1:02:59\\r\\nAjay D.: to give the prompt examples? Let's say, 10 examples, 20 examples of dialogues, and say, here's 20 dialogues in each of these dialogues. Here's this\\r\\n1:03:01\\r\\nAjay D.: name of an alien character, and use this now\\r\\n1:03:13\\r\\nAjay D.: to go the entire script to go find\\r\\n1:03:17\\r\\nAjay D.: occurrences of this alien name or alien names. Is there a way to do that? Is there a way to train it\\r\\n1:03:20\\r\\nAjay D.: on sets of data.\\r\\n1:03:27\\r\\nCathal Flanagan: So that's effectively what few shot prompting is.\\r\\n1:03:32\\r\\nCathal Flanagan: Okay. You're actually telling us, you know, here are examples.\\r\\n1:03:35\\r\\nCathal Flanagan: And here's what I want you to do.\\r\\n1:03:39\\r\\nCathal Flanagan: Now, if you're looking to search for names that don't exist within text, of course, that opens you up to risk of hallucinations, right? So if you're saying, you know you must find.\\r\\n1:03:42\\r\\nCathal Flanagan: you know, this this alien planet name amongst the Holy Bible, and that doesn't exist. But if you're saying you must.\\r\\n1:03:54\\r\\nCathal Flanagan: The model feels obliged. Right? That's the challenge. The model wants to be helpful.\\r\\n1:04:03\\r\\nCathal Flanagan: Okay.\\r\\n1:04:08\\r\\nCathal Flanagan: it is trained to be a helpful assistant, so it would rather lie to you than disappoint you.\\r\\n1:04:10\\r\\nCathal Flanagan: Okay, how do we get around\\r\\n1:04:16\\r\\nCathal Flanagan: couple of other good, prompt engineering tips?\\r\\n1:04:19\\r\\nCathal Flanagan: I always like to use the word be concise, and the words be concise, because I like it to, because usually it's too verbose with in samples, but I also always like to tell it.\\r\\n1:04:23\\r\\nCathal Flanagan: If you do not know the answer.\\r\\n1:04:33\\r\\nCathal Flanagan: or you do not have the information to answer this question, just say so.\\r\\n1:04:35\\r\\nCathal Flanagan: Give it.\\r\\n1:04:41\\r\\nAjay D.: I realized that after 10 days. But yeah.\\r\\n1:04:42\\r\\nCathal Flanagan: Give her, give her permission to disappoint you.\\r\\n1:04:45\\r\\nAjay D.: Yeah.\\r\\n1:04:48\\r\\nCathal Flanagan: Okay.\\r\\n1:04:48\\r\\nCathal Flanagan: It goes a long way.\\r\\n1:04:49\\r\\nAjay D.: Is there? Is there a is there a way where you can have\\r\\n1:04:53\\r\\nAjay D.: 2 independent evaluations of a prompt, and then compare it?\\r\\n1:04:58\\r\\nAjay D.: And if both agreed, then, you know it's a little bit more reliable.\\r\\n1:05:04\\r\\nCathal Flanagan: Completely. And in fact, that's what I would recommend. So if you're building the systems in production.\\r\\n1:05:07\\r\\nCathal Flanagan: it all depends your your tolerance for latency.\\r\\n1:05:13\\r\\nCathal Flanagan: But if you actually have, especially if you have any sort of system that's doing like\\r\\n1:05:16\\r\\nCathal Flanagan: research, that you have permission to take more than 8 seconds\\r\\n1:05:21\\r\\nCathal Flanagan: chaining prompts together where the output of one prompt is getting passed into the second prompt, and the 3rd\\r\\n1:05:26\\r\\nCathal Flanagan: and the 4th\\r\\n1:05:34\\r\\nCathal Flanagan: right to do double and triple checks we call these assertion checks is the claim being made.\\r\\n1:05:35\\r\\nCathal Flanagan: that is, it supported by the context that was originally provided.\\r\\n1:05:42\\r\\nCathal Flanagan: This is very powerful\\r\\n1:05:47\\r\\nCathal Flanagan: right, and it can reduce hallucinations to a very de minimis level. So, in fact, I would advise it. But again, it's not practical for all purposes, because a lot of what we're building are like, quick question, quick answer sets.\\r\\n1:05:49\\r\\nCathal Flanagan: or the user simply doesn't have the tolerance for\\r\\n1:06:02\\r\\nCathal Flanagan: anything above like an 8 second response time.\\r\\n1:06:05\\r\\nArun Tyagi: And Charlie, this can be done in python, just creating a function. Which kind of does takes 2 inputs. Okay, yeah, makes sense.\\r\\n1:06:08\\r\\nCathal Flanagan: They can. Now, there's a lot of frameworks. So later in the class, we're gonna and later in the semester, we're going to meet a company called Crew AI.\\r\\n1:06:14\\r\\nCathal Flanagan: That's my personal favorite agentic framework at the moment, which does this\\r\\n1:06:23\\r\\nCathal Flanagan: kind of for free for you? But we're gonna meet some other frameworks tonight, as well.\\r\\n1:06:29\\r\\nAndrey Skripkin: Yeah, I think I was next.\\r\\n1:06:36\\r\\nAndrey Skripkin: I have a question.\\r\\n1:06:38\\r\\nAndrey Skripkin: The complexity that you've showed about the prompt engineering\\r\\n1:06:39\\r\\nAndrey Skripkin: is it applicable for our like daily use, or based on the complexity. I thought that it is, for, like under the hood, setup for for a model that we need for like application business application purposes. Or we also need to think about that. And just kind of build more complex\\r\\n1:06:46\\r\\nAndrey Skripkin: prompts in our like regular regular interactions with chat, gpt.\\r\\n1:07:08\\r\\nCathal Flanagan: So it depends. Right? Like most of the prompts I have with the discussions I have with\\r\\n1:07:14\\r\\nCathal Flanagan: the models on a daily basis are 10 words or less, because I'm looking for a quick piece of information. But when I'm trying to invest on like, let's say that prompt on, I'm going to Hong Kong in itinerary.\\r\\n1:07:18\\r\\nCathal Flanagan: I actually really would benefit from that being quite good. Right? So I will. I will invest the time to actually give it a roll. I will give it the time, because I wanted to go out, and I wanted to do spend 10 min doing a really good job. But I want to. Therefore it's worth me taking an extra 2 min to actually do the prompt right now. I have a\\r\\n1:07:30\\r\\nCathal Flanagan: cookbook of like saved prompts, but I, you know I don't write the same\\r\\n1:07:49\\r\\nCathal Flanagan: thing every single time I'll tend to take an existing prompt and edit it right. Gosh!\\r\\n1:07:54\\r\\nCathal Flanagan: It depends on you know, if you're just looking for quick, quick answer to a quick question\\r\\n1:08:03\\r\\nCathal Flanagan: like just users. But yeah, I would expect, like\\r\\n1:08:08\\r\\nCathal Flanagan: we even within your like, with for yourself and your personal life. But then, certainly within the work, you should be building a prompt library of really good prompts for yourself, if actually automating tasks that you want to do and then sharing them with your team. And most, you know, enterprise systems in the future will have the ability to share these kind of prompts between teams.\\r\\n1:08:12\\r\\nCathal Flanagan: because the next natural thing from that is, let's start scheduling them. You know.\\r\\n1:08:32\\r\\nAndrey Skripkin: Makes sense thanks.\\r\\n1:08:39\\r\\nCathal Flanagan: Yes, sir.\\r\\n1:08:41\\r\\nEsther Teplitsky: Yeah, my question is about the homework. So I went through some people's code, and I noticed a lot of people were using different python packages to actually ingest the text of different things in order to summarize and compare them. I was able to do it by just giving it a URL, and it did it by itself. I was wondering if I don't know if this is like related to.\\r\\n1:08:43\\r\\nEsther Teplitsky: I don't know things that we're going to talk about later. But I'm wondering if there's any benefit to doing the manual work to parse and enter, and you know, work with the text before having the AI work with it specifically with Chat Gpt.\\r\\n1:09:09\\r\\nCathal Flanagan: So here's something interesting.\\r\\n1:09:23\\r\\nCathal Flanagan: So you gave it a URL when you're just running the code from the from the homework starter, right.\\r\\n1:09:26\\r\\nEsther Teplitsky: Yeah, I told it like, there's gonna there will be research papers, or there will be articles in the following 2 Urls. And it\\r\\n1:09:31\\r\\nEsther Teplitsky: worked. I don't know.\\r\\n1:09:38\\r\\nCathal Flanagan: But I saw everyone else was doing a lot more complicated things.\\r\\n1:09:39\\r\\nCathal Flanagan: Okay, I'm gonna burst your bubble here a little bit.\\r\\n1:09:42\\r\\nCathal Flanagan: It lied to you.\\r\\n1:09:45\\r\\nEsther Teplitsky: Oh, okay.\\r\\n1:09:47\\r\\nCathal Flanagan: You didn't read them.\\r\\n1:09:50\\r\\nCathal Flanagan: It likely recognized the Urls.\\r\\n1:09:51\\r\\nCathal Flanagan: Okay, because the name of the paper might have been the Urls, or it might be something descriptive.\\r\\n1:09:54\\r\\nCathal Flanagan: and it didn't actually go and visit the websites and scrape the website.\\r\\n1:10:00\\r\\nCathal Flanagan: They just made stuff up.\\r\\n1:10:03\\r\\nCathal Flanagan: Now, good cause, maybe if you, for example, put in the URL, for the attention is all you need paper. It knows what the attention is. All you need. Paper is so it just uses internal knowledge.\\r\\n1:10:06\\r\\nCathal Flanagan: Raj.\\r\\n1:10:17\\r\\nCathal Flanagan: But if the core model has no ability to call tools\\r\\n1:10:18\\r\\nCathal Flanagan: autonomously, we have to do that for us. We give it that functionality through what I call functions which we're going to deal with next class as part of the retrieval augmented generation system. Okay? But by default\\r\\n1:10:23\\r\\nCathal Flanagan: it doesn't know. So it just. But again, it's a helpful assistant. It doesn't want to disappoint you. So, instead of saying, I don't have the ability to visit the websites that went.\\r\\n1:10:35\\r\\nCathal Flanagan: I know what's in those websites. I can make a good guess. I'm going to give Esther a good answer.\\r\\n1:10:46\\r\\nEsther Teplitsky: Wow!\\r\\n1:10:51\\r\\nEsther Teplitsky: Mine is blown. Thank you.\\r\\n1:10:52\\r\\nCathal Flanagan: Hi Yankee.\\r\\n1:10:55\\r\\nArun Tyagi: I literally thought it went to the website. So that's what I did. I did give it the URL, and it actually kind of summarizes it right? So it's it's.\\r\\n1:10:57\\r\\nYankai Su: Yeah. But yeah, my question is, Hello.\\r\\n1:11:04\\r\\nYankai Su: yeah. My question is like, I didn't really get that. The model temperature. Can you please write to\\r\\n1:11:09\\r\\nYankai Su: Ray. Explain it a little bit.\\r\\n1:11:17\\r\\nCathal Flanagan: Sure thing. So think about this. When I'm when I say John is going to the bank period, he\\r\\n1:11:19\\r\\nCathal Flanagan: let's say there are 5 words that I can choose from.\\r\\n1:11:29\\r\\nCathal Flanagan: Okay, as the next word.\\r\\n1:11:33\\r\\nCathal Flanagan: There's like a probability distribution. It thinks that\\r\\n1:11:37\\r\\nCathal Flanagan: that, for example, he went, or he he gosh, right! There's all these extra words, the next words that come out first.st They have probabilities associated with them, you know. Maybe one word has a point 6, the other one's a point 3 a point 2 etc.\\r\\n1:11:40\\r\\nCathal Flanagan: What you're doing is when you're setting a temperature effectively limiting its ability to choose words that are\\r\\n1:11:57\\r\\nCathal Flanagan: not super high probability. So instead of choosing between 5 words and having the ability to choose somewhat randomly between them, you're effectively saying, always choose just the 2 words that have the highest probability. So you're just very much limiting its ability to be to explore by explore. I mean to choose other words that maybe have a slightly lower probability in its estimation of what the next word should be.\\r\\n1:12:05\\r\\nCathal Flanagan: Okay, it goes to find that these are fundamentally\\r\\n1:12:32\\r\\nCathal Flanagan: systems that are trying to guess at the next word in the sentence that make sense.\\r\\n1:12:37\\r\\nYankai Su: So the temperature point one and means that you choose more work or less work.\\r\\n1:12:46\\r\\nCathal Flanagan: You're very. You're not. You're limiting yourself that you can only choose. Maybe the top 2 words. I'm just making this up. But like you use top 2 words instead of the top, 10 words.\\r\\n1:12:57\\r\\nYankai Su: Okay. So one means that you can choose top 10 words.\\r\\n1:13:08\\r\\nCathal Flanagan: Yes, like I'm hand waving here. That's but that's the that's how you should like as a mental model. That's how you should think about it.\\r\\n1:13:12\\r\\nYankai Su: Okay.\\r\\n1:13:20\\r\\nCathal Flanagan: Gotcha.\\r\\n1:13:21\\r\\nYankai Su: And the.\\r\\n1:13:22\\r\\nmahesh: Yeah. So my, my question is, are there the standard\\r\\n1:13:25\\r\\nmahesh: metrics or parameters that that industry in in companies use? If you have to choose different models.\\r\\n1:13:30\\r\\nmahesh: Like, for example, in my company, we want to choose, you know, one versus the other. Is there a standard way to compare, hey? You know, here are the 10 parameters that I'm going to check, and then choose one.\\r\\n1:13:38\\r\\nCathal Flanagan: Yeah. Good question effectively, you need to decide what you're trying to optimize for. So a lot of comp, a lot of companies are building retrieval, augmented generation system. The 1st thing they're optimizing for is, does this work well or not? Right? So we're gonna talk. We could run like rag evaluation as well, or the week after\\r\\n1:13:49\\r\\nCathal Flanagan: specifically, on the models per se.\\r\\n1:14:11\\r\\nCathal Flanagan: There are, there are some beginning, see, some useful tools that can basically allow you along different dimensions, summarization. Qa, quantitative tasks.\\r\\n1:14:14\\r\\nCathal Flanagan: right that allow you to effectively evaluate these models along those different dimensions. And when you have a new model that comes out.\\r\\n1:14:25\\r\\nCathal Flanagan: you effectively do a comparison and say, Is this model? What is this model better at\\r\\n1:14:33\\r\\nCathal Flanagan: right? And these are often shown on like spider graphs of like. Here's the areas of competence for these models and what one is good at versus the other. Okay? And you can see different. So, for example, call 3, 5, sonic is known to be very good at coding. So that's 1 of the dimensions. Right? So you're like, okay, we're going to use the 3 5 sonic model for coding. We're going to use the o 1 model for complex reasoning tasks, for example, but it's kind of so much\\r\\n1:14:39\\r\\nCathal Flanagan: dependent on each company figuring out what's most important for them. What's different for a startup doing coding is going to be very different for a legal firm. They'll have their own dimensions specific to legal text summarization, drafting of contracts, etc, that they should be testing these models on.\\r\\n1:15:04\\r\\nCathal Flanagan: Yeah, there's not a whole, not. There is some useful tools like the best one out there is Calio at the moment to start up, but it's it's very early days in the evaluation world.\\r\\n1:15:23\\r\\nmahesh: I see. So do companies market their model. Hey? This like sonnet, do the company market it that way today?\\r\\n1:15:33\\r\\nCathal Flanagan: Some of them. Yes. So they will basically have different benchmarks. And they will report that they it's particularly good at a particular task.\\r\\n1:15:41\\r\\nCathal Flanagan: The best website I can point you to here is called artificialanalysis.ai.\\r\\n1:15:49\\r\\nCathal Flanagan: and they have a bunch of benchmarks.\\r\\n1:15:56\\r\\nCathal Flanagan: Artificial analysis.ai bunch of benchmarks along different dimensions.\\r\\n1:15:59\\r\\nCathal Flanagan: I can give you a sense of kind of what model is good, at what task at the moment. So definitely, we're checking this out. It's where I kind of just go to as a default. If you want to get a quick 10 sense of like a new model, and how it's performing. So\\r\\n1:16:06\\r\\nCathal Flanagan: it's being slow but definitely put in the chat and definitely check it out. So you can see here quality speed price. And then, specifically, you have these different dimensions here around coding versus quantitative reasoning. For example.\\r\\n1:16:21\\r\\nRavi Shankar: So what a model will be good at is based on how it is trained, right? Just on the training, retraining.\\r\\n1:16:35\\r\\nCathal Flanagan: Right? So if you if the summit 3, 5 model, obviously sell more code, exam high quality code examples than other models, for example.\\r\\n1:16:43\\r\\nCathal Flanagan: And there are some models that are only traded for code.\\r\\n1:16:53\\r\\nCathal Flanagan: Are there some models that are fine tuned after the fact? For just medical text? There's 1 from called Medlm Bloomberg, have one on financial text called.\\r\\n1:16:57\\r\\nRavi Shankar: Remember.\\r\\n1:17:06\\r\\nCathal Flanagan: Interestingly, the vertically, vertically trained models\\r\\n1:17:07\\r\\nCathal Flanagan: don't actually perform any better than the general models currently on the vertical tasks interesting as such.\\r\\n1:17:13\\r\\nCathal Flanagan: that will go into fine tuning, actually, later in semester, where I'm gonna start saying things like we're gonna fine tune these models.\\r\\n1:17:20\\r\\nCathal Flanagan: Not so much for knowledge, but for behavior, and that'll make more sense later in the semester.\\r\\n1:17:27\\r\\nCathal Flanagan: Okay, I know we have other hands up. I'm gonna continue the class, and then we'll give time after the class to continue the interesting questions and conversation.\\r\\n1:17:32\\r\\nRavi Shankar: Sure.\\r\\n1:17:43\\r\\nCathal Flanagan: One thing I want to do is I want to take a look at some code, because I want to give you a sense of kind of how we actually go about doing some of this few shot learning. So let's take a look at this example. This is the homework that you did.\\r\\n1:17:45\\r\\nCathal Flanagan: you might recognize us. Okay, we're basically, we're passing in a system message and a user message. Okay.\\r\\n1:17:59\\r\\nCathal Flanagan: I want to.\\r\\n1:18:07\\r\\nCathal Flanagan: Take the same one. If you remember, I gave you a chat\\r\\n1:18:11\\r\\nCathal Flanagan: and chat functionality. And I want to extend this. And I want to actually just show you you might be surprised how easy it is, how you can actually\\r\\n1:18:16\\r\\nCathal Flanagan: extend this to do few shot prompting. So let's actually take that piece of code, and\\r\\n1:18:25\\r\\nCathal Flanagan: we're going to extend. And we're going to go like this\\r\\n1:18:33\\r\\nCathal Flanagan: wrong one. I'm trying to drop this himself.\\r\\n1:18:40\\r\\nCathal Flanagan: Okay?\\r\\n1:18:51\\r\\nCathal Flanagan: So now, what I'm doing is, and we did this last week defining a function chat.\\r\\n1:18:53\\r\\nCathal Flanagan: This is this is the only thing that's different\\r\\n1:18:59\\r\\nCathal Flanagan: system role is the same thing.\\r\\n1:19:02\\r\\nCathal Flanagan: Plus.\\r\\n1:19:05\\r\\nCathal Flanagan: I'm now passing in these examples.\\r\\n1:19:07\\r\\nCathal Flanagan: That's the key. This is what future prompting is, right where I'm effectively saying, in this case, I'm saying, I have text messages that I want you to classify. Okay, after studying these examples.\\r\\n1:19:10\\r\\nCathal Flanagan: classify the new text message at the end.\\r\\n1:19:22\\r\\nCathal Flanagan: Okay?\\r\\n1:19:25\\r\\nCathal Flanagan: And if I didn't do this, then we just remove this right?\\r\\n1:19:26\\r\\nCathal Flanagan: Then\\r\\n1:19:31\\r\\nCathal Flanagan: they could go like this and then say, that's just now just\\r\\n1:19:37\\r\\nCathal Flanagan: the child message should be classified as a test, a request for assistance.\\r\\n1:19:58\\r\\nCathal Flanagan: Okay, that's fine. But actually, I have specific types of classification. I wanted to do. And I have particular types that I wanted to do it. As so if I add back in an example here.\\r\\n1:20:02\\r\\nCathal Flanagan: And I have example, one right. This piece of text is classified as non urgent. This other piece of text is classified as urgent.\\r\\n1:20:14\\r\\nCathal Flanagan: This one actually says it's not urgent. This one says Asap, with an exclamation point. Right? This one says.\\r\\n1:20:23\\r\\nCathal Flanagan: Schedule a meeting, no rush. So the model is learning that this is an example of a non urgent task. This one actually says urgent. So we pretty explicit.\\r\\n1:20:31\\r\\nCathal Flanagan: Okay? And so when I update this, and then I say, help.\\r\\n1:20:40\\r\\nCathal Flanagan: Now, the model is going to classify this text message as classification is urgent.\\r\\n1:20:45\\r\\nCathal Flanagan: Okay, so if we change these, you know, we could easily change\\r\\n1:20:52\\r\\nCathal Flanagan: these these type of classifications here to whatever we wanted. We could change the examples right? But this is how we get the model. To behave explicitly as we would want right, and giving these examples is extraordinarily helpful.\\r\\n1:20:59\\r\\nCathal Flanagan: We also can be very clear and like in this case, it's kind of learning that we want is returned as Classification Colon. We could be more explicit here as well, and tell us that that's how we want it to be returned.\\r\\n1:21:14\\r\\nCathal Flanagan: Classification is a very, very common task for these models. It allows us to build classification models without building any big database of classification.\\r\\n1:21:27\\r\\nCathal Flanagan: Data.\\r\\n1:21:38\\r\\nCathal Flanagan: The challenge is, of course, that\\r\\n1:21:40\\r\\nCathal Flanagan: the classification that it comes back with is text.\\r\\n1:21:43\\r\\nCathal Flanagan: right? And when we're writing code.\\r\\n1:21:46\\r\\nCathal Flanagan: if we may be part of a 9 1 1 system, for example, that's classifying text messages that are coming in that really exists.\\r\\n1:21:49\\r\\nCathal Flanagan: We actually maybe want\\r\\n1:21:58\\r\\nCathal Flanagan: it to be able to use as part of a pipeline, but it can be used in other code downstream\\r\\n1:22:00\\r\\nCathal Flanagan: to do that, we actually needed to do\\r\\n1:22:07\\r\\nCathal Flanagan: something of outputting in a particular type of format that we call Json. That means that other systems or their coding systems can use it quite well.\\r\\n1:22:10\\r\\nCathal Flanagan: That's 1 thing we want to do. The other thing is, this is a classification. And so we would like to understand how confident the model is in that classification. Right? And so can we actually have it? Give the ability to tell us that\\r\\n1:22:20\\r\\nCathal Flanagan: these are 2 concepts that are available within the Api that are quite useful and important to know if you're doing this type of task. The 1st one is called response format for Json.\\r\\n1:22:37\\r\\nCathal Flanagan: So if I go up to this\\r\\n1:22:49\\r\\nCathal Flanagan: response here and I uncomment this, you see response format.\\r\\n1:22:52\\r\\nCathal Flanagan: And I tell it that I want this as a Json object.\\r\\n1:22:57\\r\\nCathal Flanagan: Okay, if I update this and I run, help chat.\\r\\n1:23:01\\r\\nCathal Flanagan: Oh, okay, I have to update it slightly again, because I actually need to tell the model what I wanted to do. Okay, so how do I actually do this\\r\\n1:23:10\\r\\nCathal Flanagan: at, I say, classify the following message.\\r\\n1:23:20\\r\\nCathal Flanagan: this is why it's so interesting and return as Json.\\r\\n1:23:24\\r\\nCathal Flanagan: But to tell it explicitly that I wanted to return. Json.\\r\\n1:23:32\\r\\nCathal Flanagan: okay? And when I do this now, it's actually returning Json.\\r\\n1:23:36\\r\\nCathal Flanagan: which I can use in a downstream piece of software.\\r\\n1:23:41\\r\\nCathal Flanagan: Okay, there's something else that I might want to add here the probability of that classification.\\r\\n1:23:45\\r\\nCathal Flanagan: So this is what we call log props.\\r\\n1:23:55\\r\\nCathal Flanagan: Okay, log probabilities.\\r\\n1:23:58\\r\\nCathal Flanagan: And so this is really most useful when we have these type of classification tasks.\\r\\n1:24:00\\r\\nCathal Flanagan: But I can actually pass in as a argument. Log props equals. True.\\r\\n1:24:05\\r\\nCathal Flanagan: Okay?\\r\\n1:24:16\\r\\nCathal Flanagan: And if I update my instructions, and I say, return as Json and that probability of urgency.\\r\\n1:24:17\\r\\nCathal Flanagan: Now, when I rerun it, not only do I get the classification.\\r\\n1:24:39\\r\\nCathal Flanagan: but I actually get the probability\\r\\n1:24:44\\r\\nCathal Flanagan: right? So this can be quite powerful. I'm gonna update a slightly better prompt here a way of actually passing this because I actually don't even want the text here. Just gonna say, classify the following message\\r\\n1:24:48\\r\\nCathal Flanagan: as Json, okay? And then when I pass that in\\r\\n1:25:00\\r\\nCathal Flanagan: now, I see a probability of point 9.\\r\\n1:25:10\\r\\nCathal Flanagan: So I can I. I tend to like it as a numerical value, because then you can do things with us instead of just categorical in this particular case. That's pretty cool. The fact. The model can tell you how sure it is or not. So, for example, if I say.\\r\\n1:25:15\\r\\nCathal Flanagan: hi, John, how are you?\\r\\n1:25:30\\r\\nCathal Flanagan: This text message is classified, as let's see.\\r\\n1:25:37\\r\\nCathal Flanagan: not urgent. 95% probability of not being urgent in that classification. If I said, Hi, John.\\r\\n1:25:43\\r\\nCathal Flanagan: how are you?\\r\\n1:25:53\\r\\nCathal Flanagan: I need to talk to you.\\r\\n1:25:55\\r\\nCathal Flanagan: Probability of not urgent of 80%. Because we've said I need to talk to you so a little bit more urgency in that particular example.\\r\\n1:26:05\\r\\nCathal Flanagan: Awesome.\\r\\n1:26:15\\r\\nCathal Flanagan: Other thing I want to\\r\\n1:26:17\\r\\nCathal Flanagan: draw your attention to is chain of thought, reasoning. This will kind of be intuitive. Given the example that I showed you earlier but effectively, the ability to pass to the model examples of kind of the thought process you want to have.\\r\\n1:26:19\\r\\nCathal Flanagan: This doesn't come as intuitively. And this is where the reasoning models are particularly helpful because it could be. It's on you. It's difficult that like to come up with some of these examples, but much like I showed you earlier. This is the example of like example problems that you might have okay and explicitly telling it. This is the chain of thought answer, and how to actually get there.\\r\\n1:26:34\\r\\nCathal Flanagan: Right? This is the prompt. This is the actual. This is how you would have answered this type of logic question. Okay? Giving 2 examples. And then you can ask a totally separate question, and the model is not just going to output the question or the answer. But it's actually going to output its thought process of how it should answer the question, and you see that happening here to calculate, I will 1st do this, then I will do this, and then it actually forms the calculation\\r\\n1:26:54\\r\\nCathal Flanagan: awesome.\\r\\n1:27:23\\r\\nCathal Flanagan: One quick question. Aj.\\r\\n1:27:24\\r\\nAjay D.: Purely in in this example that you just showed.\\r\\n1:27:31\\r\\nAjay D.: Where do you?\\r\\n1:27:37\\r\\nAjay D.: Where do you specify that you wanted to use Jailcar.\\r\\n1:27:39\\r\\nCathal Flanagan: Good question. So chain of thought isn't something that you just turn on in the same way that you turn on log probabilities, or you turn on Json. It's not a toggle, it is an instruction. It's a type of instruction that we give the model. We can explicitly say this is chain of thought. But really it's a methodology of\\r\\n1:27:44\\r\\nCathal Flanagan: here's an example. Here's how to think about it. Right? You're kind of just being again. All we're doing is prompt engineering and being much more explicit about how we want to think through a problem and ideally giving you an historical example of how you have thought through that, say a similar problem in the past. But it's not a setting that we just turn on.\\r\\n1:28:03\\r\\nAjay D.: Got it? Thanks.\\r\\n1:28:22\\r\\nCathal Flanagan: Cool.\\r\\n1:28:25\\r\\nCathal Flanagan: Alright. I know we've a couple of other questions. Either put them in slack or we'll do them right after class.\\r\\n1:28:26\\r\\nCathal Flanagan: I now wanna kind of transition and talk to you about\\r\\n1:28:32\\r\\nCathal Flanagan: open source frameworks and introduce kind of 2 that we are going to meet in this class. The 1st one tonight is going to be Langchain. And then next week, as we move into retrieval, augmented generation that's going to be focused on something called Llama Index.\\r\\n1:28:36\\r\\nCathal Flanagan: So if you haven't already, I know certainly some people have given the homework they did. You're you'll become start becoming familiar with these\\r\\n1:28:49\\r\\nCathal Flanagan: libraries, and you're gonna grow to both, love them and hate them.\\r\\n1:28:57\\r\\nCathal Flanagan: You're gonna love them because they abstract a lot of complexity for you. Document loading, dealing with output, super useful tools. You want to script the web instead of giving URL. It'll actually do it for you right, and it'll do it in 5 lines of code.\\r\\n1:29:02\\r\\nCathal Flanagan: And the reason you're gonna hate them is that the particular line chain is they change quickly and all the time. So code that works today may not work in a week's time.\\r\\n1:29:20\\r\\nCathal Flanagan: The nature of the way these systems are developing. We're at the frontier, the bleeding edge. Everything is moving very quickly. Okay. Maybe line chain has slowed down a little bit. I hope so, because certainly a year ago it was it was every time we you updated the code if something was broken.\\r\\n1:29:31\\r\\nCathal Flanagan: But they are very, very useful, and it is definitely worth exploring them and being aware of what they can offer to you.\\r\\n1:29:51\\r\\nCathal Flanagan: So the way that we think about kind of how they're different.\\r\\n1:29:58\\r\\nCathal Flanagan: You know, there, there's a lot of overlap.\\r\\n1:30:03\\r\\nCathal Flanagan: They just kind of are focused on slightly different things. Langchain is kind of designed to be\\r\\n1:30:06\\r\\nCathal Flanagan: a jack of all trades, you know, giving you a suite of tools for really anything you want to do. They're very focused on agents at the moment, because that's kind of a big topic. Llama index is a little bit more narrowly scoped. They're aiming to do retrieval, augmented generation. Very well. Something you don't offer solutions in and for for agents, but it's not the same level\\r\\n1:30:12\\r\\nCathal Flanagan: of orchestration that's available there.\\r\\n1:30:37\\r\\nCathal Flanagan: They also don't have as many connectors into as many data sources. So the answer tends not to be one or the other. It tends to be\\r\\n1:30:41\\r\\nCathal Flanagan: both, and which elements of both, for the you know, with the right tool within them for the right job.\\r\\n1:30:49\\r\\nCathal Flanagan: So what we're going to tackle tonight in the last\\r\\n1:30:57\\r\\nCathal Flanagan: few minutes is 30 min is summarization. So we're gonna take that same summarization task you had.\\r\\n1:31:00\\r\\nCathal Flanagan: But I want to show you how to implement it in Linechain.\\r\\n1:31:07\\r\\nCathal Flanagan: Okay? And because this will help you build some kind of intuition around how to interact with the Langchain Library. And again, for those who are in the python class. Remember, I said.\\r\\n1:31:11\\r\\nCathal Flanagan: we build on the shoulders of giants in the open source community, so we never really want to write a lot of code ourselves. Rather, we want to import code that other people have\\r\\n1:31:21\\r\\nCathal Flanagan: created for us, that solve useful tasks. And we want to do something useful with us.\\r\\n1:31:32\\r\\nCathal Flanagan: Okay, so the problem statement that we're gonna work on for the rest of the class is this.\\r\\n1:31:37\\r\\nCathal Flanagan: this is a research report from a fantastic analyst professor\\r\\n1:31:45\\r\\nCathal Flanagan: market participant called Michael Mubasan. He works for Morgan Stanley's counterpoint global group.\\r\\n1:31:52\\r\\nCathal Flanagan: He puts out amazing research. You can get it on their website. It's free. He's often on podcasts. I highly recommend you listen to him. If you're at all interested in the investing world.\\r\\n1:32:00\\r\\nCathal Flanagan: The problem with that Michael's research puts out is\\r\\n1:32:11\\r\\nCathal Flanagan: they don't do very nice summaries or abstracts at the top of the documents. Okay, they just go straight into a long form piece of research, and they're long, but they're good.\\r\\n1:32:15\\r\\nCathal Flanagan: But who has the time for all that. So we actually want to download his report\\r\\n1:32:27\\r\\nCathal Flanagan: and be able to summarize it quickly.\\r\\n1:32:33\\r\\nCathal Flanagan: So let's just break into that.\\r\\n1:32:37\\r\\nCathal Flanagan: We're going to write some code.\\r\\n1:32:40\\r\\nCathal Flanagan: Let me share this notebook if I haven't already.\\r\\n1:32:44\\r\\nCathal Flanagan: and then folks can follow along with me if you so wish.\\r\\n1:32:47\\r\\nCathal Flanagan: While I'm noting this Samathi, do you have a question.\\r\\n1:32:54\\r\\nSumathi Reddy: Hey? Sorry? Yes.\\r\\n1:33:04\\r\\nSumathi Reddy: So I saw that you were asking the model to give answers in Json format right\\r\\n1:33:06\\r\\nSumathi Reddy: at my work. We want\\r\\n1:33:12\\r\\nSumathi Reddy: to feed it like a Csv file, which is large, and we wanted to return back a Csv file\\r\\n1:33:15\\r\\nSumathi Reddy: after it does a set of computations.\\r\\n1:33:22\\r\\nSumathi Reddy: But we always run with run out of tokens.\\r\\n1:33:27\\r\\nCathal Flanagan: Okay? Well, if you wanted to return a Csv file, then you're going to need to invoke something, called the code interpreter. Happy to talk about that after class, or we can. We're gonna we'll cover code interpreter with the functions in a week. In a week or two's time. So\\r\\n1:33:34\\r\\nCathal Flanagan: sure happy to sit down as well. Thank you.\\r\\n1:33:51\\r\\nCathal Flanagan: Awesome. Okay. A quick trip Tip. Excuse me for folks who want to quickly get\\r\\n1:33:55\\r\\nCathal Flanagan: Pdfs from the web as we do right now. If you actually go and type in W guess and put in a Pdf, you can get those\\r\\n1:34:02\\r\\nCathal Flanagan: right? So in this case, actually, I need to get the Morgan Stanley one. So let me go and grab that. So I know where the website is.\\r\\n1:34:12\\r\\nCathal Flanagan: I'm just gonna go like this.\\r\\n1:34:19\\r\\nCathal Flanagan: Okay, I'm gonna get that Pdf document. It's gone to the website and download it for me, sweet. So now, I actually know that in my local directory\\r\\n1:34:22\\r\\nCathal Flanagan: I have that Morgan Stanley document called Article Increasing Returns.\\r\\n1:34:31\\r\\nCathal Flanagan: Good news.\\r\\n1:34:37\\r\\nCathal Flanagan: Okay? I need to\\r\\n1:34:39\\r\\nCathal Flanagan: install a couple of libraries. Okay, I want to install linechain. I want to install linechains, ability module to run open. AI, and I also want to install a Pdf parser. Okay, so these are tools\\r\\n1:34:42\\r\\nCathal Flanagan: that I am installing that are blank chain, and specifically for Pdfs. That will take a second without just installing the tools that I want to read\\r\\n1:34:56\\r\\nCathal Flanagan: here.\\r\\n1:35:10\\r\\nCathal Flanagan: I need to load up the Pdf document. Right? I actually want to read the Pdf document from local storage. And I want to\\r\\n1:35:11\\r\\nCathal Flanagan: loaders and split up the by page.\\r\\n1:35:19\\r\\nCathal Flanagan: Okay, so here, I'm just going to say from line chain, das document\\r\\n1:35:22\\r\\nCathal Flanagan: notice. And I encourage you to take a look\\r\\n1:35:32\\r\\nCathal Flanagan: at langchains, documentation and website Anya, I'm sure, can share it. They have so many great loaders, right? Pretty much any system that you can think about any knowledge base. They have a loader that has been written against it, or someone has contributed. That's the great thing about open source.\\r\\n1:35:35\\r\\nCathal Flanagan: Okay? So here, I'm going to use. Someone has loaded up pi Pdf. Loader and passing in the Pdf itself.\\r\\n1:35:56\\r\\nCathal Flanagan: And I want the pages right? So it's not just to load it. I want the pages that I can actually do something with. So pages, equals\\r\\n1:36:04\\r\\nCathal Flanagan: loader dos load and split.\\r\\n1:36:11\\r\\nCathal Flanagan: Okay, so it's gonna load it up. And it's gonna split the pages. And what that means is\\r\\n1:36:18\\r\\nCathal Flanagan: it's putting them all into the text for each page. It's putting it into a separate document. So for example, I can say, pages\\r\\n1:36:23\\r\\nCathal Flanagan: 0.\\r\\n1:36:36\\r\\nCathal Flanagan: And that's the 1st page.\\r\\n1:36:38\\r\\nCathal Flanagan: Okay? And I can see that here.\\r\\n1:36:40\\r\\nCathal Flanagan: And then I can say page one.\\r\\n1:36:42\\r\\nCathal Flanagan: and it hasn't found it. Why has not found it? Pages? There we go.\\r\\n1:36:51\\r\\nCathal Flanagan: And that's the second page, for example.\\r\\n1:37:00\\r\\nCathal Flanagan: Okay, so now I have the document. I need to actually summarize it. Right?\\r\\n1:37:02\\r\\nCathal Flanagan: So the way that I do that\\r\\n1:37:10\\r\\nCathal Flanagan: is, I'm calling from line chain. I'm saying from line chain, dash, chains task summarize.\\r\\n1:37:12\\r\\nCathal Flanagan: Okay, so they have a summarize function, import, load.\\r\\n1:37:25\\r\\nCathal Flanagan: summarize chain. And again, this is just the tool that they have.\\r\\n1:37:30\\r\\nCathal Flanagan: And then I need to have the ability to call the open AI model.\\r\\n1:37:34\\r\\nCathal Flanagan: And so I say, from line chain, open, AI import.\\r\\n1:37:38\\r\\nCathal Flanagan: Chat open, AI. Now we're using Openai.\\r\\n1:37:48\\r\\nCathal Flanagan: Of course they support every other type of model from all model providers. So you know, if if it's your, you don't have to worry about support for lots of other writers, anthropic or Google, etc.\\r\\n1:37:53\\r\\nCathal Flanagan: I 1st need to define the actual Lm, that I'm going to use. Okay? And this is nice. Because then you can swap in Google Gemini. You can swap in entropic in the future. So I'm 1st going to say.\\r\\n1:38:08\\r\\nCathal Flanagan: chat openai.\\r\\n1:38:19\\r\\nCathal Flanagan: Okay?\\r\\n1:38:21\\r\\nCathal Flanagan: And here I can. I can set the temperature if I want. Otherwise I can set the. You can just use the default value. I can pass in the model that I wanted to use in this case. Let's keep this simple. I'm just gonna go Gpt, 4. O,\\r\\n1:38:22\\r\\nCathal Flanagan: okay, I could say, it's 3.5 turbo depending on, you know, the more complex the task, the more complex the model you're going to want to use. So, like 3 5 turbo, we might do a decent job here. But like this is where you'll be thinking about model choice. This is a simple task. Is this like classification? That would be 3, 5 turbo. If this is a little bit more complex, probably want to do a 4. 0, if it's very complex, maybe you're moving on to one of the reasoning models. But remember, the trade off is cost and time there. Right?\\r\\n1:38:38\\r\\nCathal Flanagan: Okay, I need to pass in the Api key. But of course it needs that.\\r\\n1:39:09\\r\\nCathal Flanagan: And so the Api key equals the open AI key.\\r\\n1:39:13\\r\\nCathal Flanagan: Okay? And the next piece of the code says.\\r\\n1:39:19\\r\\nCathal Flanagan: chain equals load, summarize chain. I'm passing in the Llm.\\r\\n1:39:22\\r\\nCathal Flanagan: And then, importantly, what's called the chain type.\\r\\n1:39:27\\r\\nCathal Flanagan: Okay, this is mapreduce. I'm going to change this to stuff and tell you about it in just a moment.\\r\\n1:39:31\\r\\nCathal Flanagan: Okay, but let's just do this. I'm gonna say, response equals chain, dot invoke.\\r\\n1:39:36\\r\\nCathal Flanagan: And I'm going to pass in the 1st 3 pages, right? Pages 0 2, 3.\\r\\n1:39:46\\r\\nCathal Flanagan: And I'm going to print the response.\\r\\n1:39:57\\r\\nCathal Flanagan: okay?\\r\\n1:40:06\\r\\nCathal Flanagan: And so and actually, let me just say, rise here. And I can put inside larger.\\r\\n1:40:07\\r\\nCathal Flanagan: Okay, so what's happening here is in 4 5 lines of code.\\r\\n1:40:12\\r\\nCathal Flanagan: I have now been able to summarize that document\\r\\n1:40:22\\r\\nCathal Flanagan: right? And again, all, every, all the other things we did last week are kind of abstracted away from us.\\r\\n1:40:25\\r\\nCathal Flanagan: Okay, now we have control over them if we want in the future. But they're not there for us at the moment, and then you can see what is being returned is both the document itself plus the\\r\\n1:40:31\\r\\nCathal Flanagan: This summarization, and the way to actually see the actual summary here is in this case, I have res, and part of the output will be what's called output\\r\\n1:40:43\\r\\nCathal Flanagan: text.\\r\\n1:40:55\\r\\nCathal Flanagan: Okay? And this is how I can see the actual summarization itself.\\r\\n1:40:56\\r\\nCathal Flanagan: Okay, so this is a summary that it's giving me, which is useful.\\r\\n1:41:03\\r\\nCathal Flanagan: But honestly, it's a little bit too, for both. It's not exactly the way I want it. I want 3 bullet points, that's all.\\r\\n1:41:08\\r\\nCathal Flanagan: So how do I actually do that? How do I change this code so that I have the ability to give\\r\\n1:41:15\\r\\nCathal Flanagan: the model or the information exactly as I want the actual output to be.\\r\\n1:41:23\\r\\nCathal Flanagan: In this particular case, I can define what's called a prompt template.\\r\\n1:41:30\\r\\nCathal Flanagan: Okay, where I give up templates.\\r\\n1:41:35\\r\\nCathal Flanagan: where I can pass in anything in the future.\\r\\n1:41:38\\r\\nCathal Flanagan: Okay. So again, this is where not, if we move from the kind of\\r\\n1:41:40\\r\\nCathal Flanagan: manual prompting we did last week to now we're beginning to build\\r\\n1:41:45\\r\\nCathal Flanagan: with a framework a system that can achieve better scale.\\r\\n1:41:50\\r\\nCathal Flanagan: Okay?\\r\\n1:41:54\\r\\nCathal Flanagan: And so if I go down here and I say\\r\\n1:41:55\\r\\nCathal Flanagan: the same thing. But I actually define prompt.\\r\\n1:42:00\\r\\nCathal Flanagan: Actually, I need to import it, I say, from 9 chain.\\r\\n1:42:04\\r\\nCathal Flanagan: Gosh prompts. You have a whole\\r\\n1:42:09\\r\\nCathal Flanagan: library of tools related to prompts, and I say, import prompt, template.\\r\\n1:42:12\\r\\nCathal Flanagan: Okay? And I say, prompt underscore templates.\\r\\n1:42:20\\r\\nCathal Flanagan: equals. And then I'm just going to give us instructions and instructions. I'm going to give us right now are\\r\\n1:42:25\\r\\nCathal Flanagan: to simply write a concise summary.\\r\\n1:42:32\\r\\nCathal Flanagan: So I'm gonna Martha Stewart. This\\r\\n1:42:36\\r\\nCathal Flanagan: write a concise summary in a maximum of 3 bullets of the following text, enclosed within 3 3 back tick 3 back ticks.\\r\\n1:42:39\\r\\nCathal Flanagan: And so this is interesting. And this goes to prompt engineering.\\r\\n1:42:49\\r\\nCathal Flanagan: Different models are trained with different types of input data.\\r\\n1:42:52\\r\\nCathal Flanagan: Okay, the open AI models are trained to understand triple backticks.\\r\\n1:42:58\\r\\nCathal Flanagan: Okay, what we're doing here is we're basically saying amongst this piece of text, I'm delineating what washes where?\\r\\n1:43:06\\r\\nCathal Flanagan: Okay.\\r\\n1:43:15\\r\\nCathal Flanagan: anthropic models. For example, if we were to do the same thing, we would actually go like this, we would use Xml tags.\\r\\n1:43:18\\r\\nCathal Flanagan: I would say text.\\r\\n1:43:25\\r\\nCathal Flanagan: and then we pass it the text, and then we will close it with text like this. So different models have different ways of dealing with kind of how to divide up.\\r\\n1:43:28\\r\\nCathal Flanagan: The text is all ultimately text that you're passing in. So you're helping it. Understand? You know, what's an example? What's a question? What's a what's an instruction? Okay? So the use of tags is actually, very, very useful. If someone's asked me to optimize their prompt. The very 1st thing I will do is I will take it apart. I will break it up and put tags everywhere\\r\\n1:43:39\\r\\nCathal Flanagan: with open AI models. I'll put triple back ticks.\\r\\n1:44:05\\r\\nCathal Flanagan: Anthropics, models always Xml tags.\\r\\n1:44:09\\r\\nCathal Flanagan: If in doubt, xml tags seem are being shown to kind of work across all models. Quite well.\\r\\n1:44:12\\r\\nCathal Flanagan: Okay. So here we're passing in in this. Notice these curly brackets. This means it's a variable. We're gonna pass something in here in place of the text.\\r\\n1:44:22\\r\\nCathal Flanagan: Okay? And so then here, I'm passing in. This is the prompt that I'm going to build from the prompt template. Okay? And there's a specific thing here which I'm calling text is a variable in. I could call this Santa Claus. It's just a variable.\\r\\n1:44:32\\r\\nCathal Flanagan: Okay.\\r\\n1:44:50\\r\\nCathal Flanagan: The next thing I'm going to do is what I did previously\\r\\n1:44:53\\r\\nCathal Flanagan: I had to find the model and the chain.\\r\\n1:44:56\\r\\nCathal Flanagan: Okay, and I'll just import those.\\r\\n1:45:00\\r\\nCathal Flanagan: So this is exactly the same as we had previously.\\r\\n1:45:08\\r\\nCathal Flanagan: Bear with me.\\r\\n1:45:16\\r\\nCathal Flanagan: Okay, so this is exactly the same quote as we had above.\\r\\n1:45:18\\r\\nCathal Flanagan: Okay, with the exception that I need to import the chain, let me confirm that. Yes, I do. The note above. Okay, I need to import the Llm chain. So here\\r\\n1:45:22\\r\\nCathal Flanagan: I'm going to add this piece of code where I'm just importing\\r\\n1:45:35\\r\\nCathal Flanagan: this Llm. Chain which gets called down here.\\r\\n1:45:43\\r\\nCathal Flanagan: Okay? Because the concept of line chain is basically chaining a lot of things together.\\r\\n1:45:47\\r\\nCathal Flanagan: Okay, so that's why you see, chain kind of scattered across here in terminology.\\r\\n1:45:52\\r\\nCathal Flanagan: But this is exactly kind of as we did before. The way this is quite different is that now\\r\\n1:45:56\\r\\nCathal Flanagan: I am passing in something called a stuffed documents chain.\\r\\n1:46:03\\r\\nCathal Flanagan: Okay? And I'm gonna explain what stuff is in the moment and then explain kind of different variations and why you would use one to the other.\\r\\n1:46:08\\r\\nCathal Flanagan: So if we import the stuff documents chain\\r\\n1:46:16\\r\\nCathal Flanagan: here, you see, I'm passing in just the Llm. That's above. But, importantly, I'm passing in the document. Variable name equals text. And so what I'm doing here is, I'm I'm telling it.\\r\\n1:46:22\\r\\nCathal Flanagan: I am going to give you text.\\r\\n1:46:35\\r\\nCathal Flanagan: You're going to use the standard template, prompt template that you have developed.\\r\\n1:46:37\\r\\nCathal Flanagan: And you're going to replace\\r\\n1:46:42\\r\\nCathal Flanagan: where it says the word, the variable text. You're gonna put in the text, I'm passing into you. Everything else in the prompt will stay exactly the same. But you are to replace\\r\\n1:46:44\\r\\nCathal Flanagan: the variable. In this case. We're calling a text\\r\\n1:46:54\\r\\nCathal Flanagan: with whatever I'm passing in like, for example, I call this Santa Claus. Okay, it's just a variable.\\r\\n1:46:59\\r\\nCathal Flanagan: So here, pass it in Santa.\\r\\n1:47:06\\r\\nCathal Flanagan: And when I run this, I'm then passing in the the documents. But what's different is\\r\\n1:47:10\\r\\nCathal Flanagan: when I look at the output Rev.\\r\\n1:47:19\\r\\nCathal Flanagan: Output text.\\r\\n1:47:24\\r\\nCathal Flanagan: It's now it's what's in Markdown. So a little bit difficult to see, it's now actually outputting it with a\\r\\n1:47:32\\r\\nCathal Flanagan: bullet point. So you can see the new lines here.\\r\\n1:47:40\\r\\nCathal Flanagan: Diva.\\r\\n1:47:43\\r\\nDima Timofeev: We have multiple questions there, this they can be clustered into the same notion. Why do we need to use templates rather than\\r\\n1:47:47\\r\\nDima Timofeev: just what we used in the previous lessons, and I would like to quickly address it. So\\r\\n1:47:58\\r\\nDima Timofeev: same works like one chain, and Charlie feel free to add, if I forget something to say. There are 2 main benefits. 1st of all, when you work with very small projects, it's very easy to control prompts yourself, however, when you go further and you do more and more and more sophisticated implementations, it's much harder, and you eventually will reinvent templates, prompt templates yourself. So it's this very sophisticated tool allows you to\\r\\n1:48:03\\r\\nDima Timofeev: create templates that are universally reusable and will be very useful for you in the future, and they will also improve maintainability of your code. But eventually, if you check implementation of these templates, yes, they will resolve\\r\\n1:48:32\\r\\nDima Timofeev: into this same prompt that we wrote in during the 1st lesson, and second big second. Large benefit is that different libraries like Chat Gpt. Sorry not chat. Gpt like Langchain and Lama Index, allow us to work with multiple providers. We just open the I Api directly\\r\\n1:48:46\\r\\nDima Timofeev: with Openai Python Library, but it's very hard to switch between them. So when we use link chain or similar frameworks, we can easily switch between different models with one or 2 lines of code changes without changing anything else, templates, libraries, etc. So 2 key benefits, supportability, extensibility and a support of multiple models and multiple providers.\\r\\n1:49:04\\r\\nCathal Flanagan: And it will become like with this simple example of summarizing a document.\\r\\n1:49:31\\r\\nCathal Flanagan: It's we're we're doing it because it's conceptually, it's it's following on from last week's class, so I hope you kind of kind of have some intuition around us. What I will say is, it will become abundantly clear why we need to use frameworks\\r\\n1:49:37\\r\\nCathal Flanagan: when we start moving into retrieval augmented generation. Okay? Because that's a lot of you're building a system. You're building a chatbot basically there. And there's a lot of pieces that you will not want to do everything from scratch. You'll want to use some tools that are pre coded or available for you.\\r\\n1:49:50\\r\\nCathal Flanagan: The other huge benefit that I already mentioned is document loaders.\\r\\n1:50:08\\r\\nCathal Flanagan: Right? If you look at from like chain, excuse me, all of the different\\r\\n1:50:13\\r\\nCathal Flanagan: systems that it is able to find information against. It's like quite enormous. So it's worth\\r\\n1:50:21\\r\\nCathal Flanagan: understanding, kind of like different integrations that they have. I guarantee you. If there's a system that you use. You know, pick your system crms, databases, etc. They make it very, very straightforward to actually\\r\\n1:50:31\\r\\nCathal Flanagan: to to load information from them. Right? That's just to give you a sense of like the different vector source alone. They actually support here left.\\r\\n1:50:48\\r\\nMahek Pavagadhi: It's just like that works with multiple cloud providers, right? Instead of using cloud formation that works with just aws, you can use terraform just like Langchain. It's just an example or an\\r\\n1:50:57\\r\\nMahek Pavagadhi: that that's not tied up to a specific cloud provider.\\r\\n1:51:12\\r\\nMahek Pavagadhi: But thank you.\\r\\n1:51:16\\r\\nCathal Flanagan: Okay, I wanna go back to a concept that I skipped over that. I said I would come back to.\\r\\n1:51:18\\r\\nCathal Flanagan: And it's this one called stuff versus\\r\\n1:51:23\\r\\nCathal Flanagan: you might remember I replaced something that was here. I replaced something called map root juice.\\r\\n1:51:26\\r\\nCathal Flanagan: So these are. And this is an important distinction. And it's very much depending on your use case of what one you would use.\\r\\n1:51:34\\r\\nCathal Flanagan: So stuff when you pass it in as I did.\\r\\n1:51:42\\r\\nCathal Flanagan: basically says, use all of the text that I'm passing in. Okay, and we're passing in 3 pages. This could be theoretically\\r\\n1:51:46\\r\\nCathal Flanagan: document. That is, 300 pages long. I have seen that the models have a limitation.\\r\\n1:51:56\\r\\nCathal Flanagan: It's called a context window limitation, which means the maximum number of tokens that they can actually\\r\\n1:52:04\\r\\nCathal Flanagan: take in and reason over at any one time is\\r\\n1:52:12\\r\\nCathal Flanagan: it's about a 300 page book at this point, right? Like different models, have different limitations. You can actually find those by looking on the models. Page of Openai, for example. And what you\\r\\n1:52:17\\r\\nCathal Flanagan: you'll see is here.\\r\\n1:52:31\\r\\nCathal Flanagan: The context window is what you're looking at right? And for most of the Openai models it's about 128,000 tokens for anthropic. It's about 200,000 tokens for deepseek. It's 64,000 tokens for Google's new model. It's 2 million tokens. Okay? So there is some big discrepancies between the different models that also can drive the decision of which model you will want to use.\\r\\n1:52:33\\r\\nCathal Flanagan: Well, 2 years ago, when we started this, we started with\\r\\n1:52:58\\r\\nCathal Flanagan: 4,000 words, being the maximum you could pass him.\\r\\n1:53:03\\r\\nCathal Flanagan: That was a real limitation. It meant that for most documents we were passing in we were not able to create summaries over the entire document.\\r\\n1:53:08\\r\\nCathal Flanagan: So the way that we dealt with that.\\r\\n1:53:17\\r\\nCathal Flanagan: and it's becoming less of a concern now was.\\r\\n1:53:20\\r\\nCathal Flanagan: instead of just giving an error and saying, that's not possible. There's too much text.\\r\\n1:53:24\\r\\nCathal Flanagan: The Lang Chain introduced something called mapreduce as an option. Here\\r\\n1:53:29\\r\\nCathal Flanagan: it basically allows you to create to break a document apart\\r\\n1:53:35\\r\\nCathal Flanagan: and have a summary of each page\\r\\n1:53:39\\r\\nCathal Flanagan: or each section depending on how you want to define it.\\r\\n1:53:43\\r\\nCathal Flanagan: and then it would create a final summary at the end, based upon the summaries of each page. So it was a summary of summaries.\\r\\n1:53:46\\r\\nCathal Flanagan: Okay, the stuff method means passing everything.\\r\\n1:53:54\\r\\nCathal Flanagan: Okay, pass in all the text\\r\\n1:54:00\\r\\nCathal Flanagan: it to some degree. There is a trade off\\r\\n1:54:04\\r\\nCathal Flanagan: with mapreduce. Because of the summary of a summary. You'll probably see that the actual quality, the output, may be a little bit more generic, right? Because it's not picking up on all of the levels of detail that you might wish\\r\\n1:54:08\\r\\nCathal Flanagan: But if you have a very, very long document quite frankly. It's kind of your only choice.\\r\\n1:54:22\\r\\nCathal Flanagan: Again, contact centers are growing. It's rare that we run into the errors it has used to happen all the time. I can tell you personally, the way that I run these things is I use stuff.\\r\\n1:54:28\\r\\nCathal Flanagan: I pass in everything unless I run into an error. And then I figure out, okay, I probably need to use the mapreduce method. So that is kind of the 2 options that you have to toggle between.\\r\\n1:54:39\\r\\nCathal Flanagan: it's, you know, it's up to you, depending on your use case and also depending on the model that you're actually using.\\r\\n1:54:51\\r\\nCathal Flanagan: Okay with dash.\\r\\n1:54:59\\r\\nCathal Flanagan: I want to give you your homework, which may or may not surprise you.\\r\\n1:55:01\\r\\nCathal Flanagan: So your homework is to create\\r\\n1:55:07\\r\\nCathal Flanagan: a summarization so to take the homework that you've done, or you can do something completely different.\\r\\n1:55:13\\r\\nCathal Flanagan: Okay, but I want you to\\r\\n1:55:19\\r\\nCathal Flanagan: to create the summarization that you did or do a new one\\r\\n1:55:22\\r\\nCathal Flanagan: using Langchain instead of the kind of manual process that you did last week. Okay? And I want you to compare the stuff and the map produce methods. Okay, for those who haven't coded before for those for whom this is relatively new. It's pretty straightforward, right? We're going to give you starter code. You're going to be able to run this\\r\\n1:55:26\\r\\nCathal Flanagan: in the simplest form. All you need to do is replace some text\\r\\n1:55:47\\r\\nCathal Flanagan: and change stuff to mapreduce or the other way around.\\r\\n1:55:50\\r\\nCathal Flanagan: But of course, we love to see people doing extra credit. And so there's so much to Linkedin. You're gonna love it in terms of the connections.\\r\\n1:55:54\\r\\nCathal Flanagan: and we already saw people being adventurous in the homework already. So look at the other connections. Imagine what else you can pull in for those who had the issue of giving Urls and then actually hallucinating the answer.\\r\\n1:56:04\\r\\nCathal Flanagan: well, use line chain, and it will actually really do a web script for you. That's very easy. You can probably do it around 5 lines of code so definitely explore that. And it is due, of course, as we previously had, just before class\\r\\n1:56:19\\r\\nCathal Flanagan: next week.\\r\\n1:56:32\\r\\nCathal Flanagan: Alright with that. We're exactly on time.\\r\\n1:56:34\\r\\nCathal Flanagan: thank you for showing up. We had a great office. We had a great paper club and office hours. Last weekend\\r\\n1:56:39\\r\\nCathal Flanagan: we will have. I expect Richard will probably have a you'll probably be doing a paper clip. Is that right?\\r\\n1:56:47\\r\\nRichard Ryan: Indeed, indeed we are, and Dima and I have been have been kicking back and forth what would be good? And Dima strongly suggests that there is a foundational chain of thought paper which relates to a lot of the material that we've been discussing, and that is in the Channel Club. We have the link to that in the Channel cool papers channel on slack.\\r\\n1:56:54\\r\\nCathal Flanagan: Excellent, and\\r\\n1:57:17\\r\\nCathal Flanagan: the office hours. I am going to do it on Monday evening of next week instead of Sunday. So I'll send out communications about that in the recap email. But that would be Monday evening for those who want to join. And as a reminder paper club is a student run initiative. Dmri will sometimes show up, not all the time. So it's very much for you guys to self organize and kind of volunteer. So Richard is kind of organizing it. But please\\r\\n1:57:20\\r\\nCathal Flanagan: please, please do volunteer, and and to to present the paper and then the office hours are very much a\\r\\n1:57:48\\r\\nCathal Flanagan: come and chat about anything you want. We don't really do. How much teaching English we just do a lot of talking and discussing a lot less of me just talking at you, which is welcome to everybody. And that'll be on next Monday evening.\\r\\n1:57:54\\r\\nCathal Flanagan: Otherwise we'll be on slack. Continue organizing meetups in person, and we'll stick around now for any questions that folks have otherwise. Thank you for another great week.\\r\\n1:58:08\\r\\nHadeel Ammari: The zoom links the same for all 3 meetings.\\r\\n1:58:21\\r\\nCathal Flanagan: All the same, Zoom link applies to every single thing we would do in this class.\\r\\n1:58:25\\r\\nHadeel Ammari: Thank you.\\r\\n1:58:30\\r\\nCathal Flanagan: Everything will be recorded.\\r\\n1:58:31\\r\\nCathal Flanagan: Yes, Dylan.\\r\\n1:58:37\\r\\nnaomipinto: OP.\\r\\n1:58:40\\r\\nnaomipinto: And say, Thank you for the class. So congratulations, so thank you.\\r\\n1:58:43\\r\\nCathal Flanagan: Too much.\\r\\n1:58:49\\r\\nDelin Shen: Oh, yeah, I just have a question back to the topic when you're when you're basically using a prompt to ask for a classification problem. And you are specified, you want to output as a Json format. So I'm just wondering. Let's say we're in a production environment where we have millions of pro requests every day, and we want to store the result in a fixed Json format for downstream usage. So if this is the usage.\\r\\n1:58:52\\r\\nDelin Shen: How, I mean, how reliable is lm, can we fully trust Lm. To return the result in the exact Json format we specified.\\r\\n1:59:16\\r\\nCathal Flanagan: No, you cannot.\\r\\n1:59:25\\r\\nCathal Flanagan: There is a there is a library or a tool that\\r\\n1:59:27\\r\\nCathal Flanagan: you might want to explore and we can talk about in in office hours, or even do it in office hours before class. If you want on a library called Pydantic, which is the general framework or tool that most people use for kind of enforcing more rigid formatting even then\\r\\n1:59:31\\r\\nCathal Flanagan: not guaranteed. But it gotten a lot better.\\r\\n1:59:50\\r\\nDelin Shen: You know.\\r\\n1:59:52\\r\\nCathal Flanagan: So I would say, it's it's a solved problem from my perspective, but it's never a guarantee. But you can, of course, have these frameworks. Other. Because, as you might imagine, the entire software engineering world\\r\\n1:59:53\\r\\nCathal Flanagan: needs this.\\r\\n2:00:04\\r\\nDima Timofeev: Yeah.\\r\\n2:00:06\\r\\nDima Timofeev: there is combination of tools. Pydantic is like the leader and structured output supported by open. I, for example, in combination with pydantic, usually provide you some level of confidence, but\\r\\n2:00:07\\r\\nDima Timofeev: it never guaranteed so. Unfortunately, this problem is not solved and due to the statistic nature of our lamps. I'm not sure that we'll get to the ultimate 100%.\\r\\n2:00:22\\r\\nHans Reisgies: But you can have a model check the answer. You can have another model. Check the response and correct right?\\r\\n2:00:34\\r\\nHans Reisgies: It's never about one.\\r\\n2:00:41\\r\\nDelin Shen: Got it? Thanks.\\r\\n2:00:49\\r\\nCathal Flanagan: I'm rush.\\r\\n2:00:51\\r\\nmahesh: So I I'm interested to see if you are planning to cover context management. You know, currently, I'm facing some\\r\\n2:00:54\\r\\nmahesh: at work. We are trying to figure out, how do we manage context, when you know, with multiple turns right? When one agent talks to another agent, and things like that. Are there any standard patterns, approaches, design patterns and things like that?\\r\\n2:01:08\\r\\nCathal Flanagan: not explicitly, as a like standalone topic, but in implicitly will come up when we review with agentic frameworks and having to effectively pass in. Well, it'll come up in 2 different ways. Actually, it will come up in the context of when we're kind of building the chat bot\\r\\n2:01:23\\r\\nCathal Flanagan: as part of a retrieval, augmented generation like we have to pass in memory. Right? So then, you have certain decisions you can make around.\\r\\n2:01:40\\r\\nCathal Flanagan: Are you passing in the entire history. Are you passing in just a certain limited number of windows? Are you passing in a summary of the previous conversation? So it'll come up in that perspective. It will also come up as we as it relates to\\r\\n2:01:48\\r\\nCathal Flanagan: agents.\\r\\n2:02:03\\r\\nCathal Flanagan: But I don't have a specific topic on like optimization there, because, quite frankly.\\r\\n2:02:05\\r\\nCathal Flanagan: much like a lot of this. There isn't really a best practice. It's kind of like empirically what seems to work. Well, given your use case.\\r\\n2:02:11\\r\\nmahesh: That's.\\r\\n2:02:19\\r\\nCathal Flanagan: Technical limitations. You have, like, you have extremely context, you need to use summarization, perhaps because you just simply are gonna run out of the context window.\\r\\n2:02:20\\r\\nmahesh: Right right now. I was also thinking, sorry if I have somebody else's raised. If there is an opportunity to layer context, right? You know, within the context itself, we kind of phase out, you know, as the flow goes from one agent to another agent, some of the oldest context may not be relevant anymore.\\r\\n2:02:30\\r\\nCathal Flanagan: That tends well, that tends to be more, you know, on the chat as well. That's where you have a K of like number of history, of messages that you would actually pass back.\\r\\n2:02:48\\r\\nmahesh: Hmm.\\r\\n2:02:57\\r\\nCathal Flanagan: Like usually just after a certain message. You just forget it.\\r\\n2:02:58\\r\\nmahesh: Okay. Okay.\\r\\n2:03:01\\r\\nmahesh: Cool. Alright. Thank you.\\r\\n2:03:03\\r\\nCathal Flanagan: What else have we got?\\r\\n2:03:06\\r\\nCathal Flanagan: Jeremy?\\r\\n2:03:10\\r\\nJeremy McCormick: Yeah, I had a really simple question. So is the template. Gonna be that you're gonna put the starter notebook in the class\\r\\n2:03:12\\r\\nJeremy McCormick: recap like you did last week. Is that kind of how we get the the notebook to work with for the homework.\\r\\n2:03:20\\r\\nCathal Flanagan: Right. I'll share right now as well. But yes, the every every class you the day after, because it's midnight here. I'm going to send a later. The second day I will send a recap email with all the assets links, including this starter for the homework.\\r\\n2:03:26\\r\\nJeremy McCormick: Okay. Great. Thanks.\\r\\n2:03:43\\r\\nCathal Flanagan: Work.\\r\\n2:03:44\\r\\nCathal Flanagan: Yes, aren't you.\\r\\n2:03:47\\r\\nPranshu Tiwari: Yeah, thank you.\\r\\n2:03:49\\r\\nPranshu Tiwari: So sorry. I think I missed the question earlier.\\r\\n2:03:51\\r\\nPranshu Tiwari: and I had been missing these questions, probably because I've been using zoom. After a long time I couldn't. I didn't know how to raise my hand, so I realized it now. So I've been. I am from mathematical background just to give a brief context, and that's why my questions might be slightly out of place. But I wasn't into textual processing in Core AI until I came it now, and I realized it has changed a lot. So my 1st question is that when we see I I saw the operator model.\\r\\n2:03:54\\r\\nPranshu Tiwari: So is it. Can we say the operator model is nothing but a conventional prediction plus optimization algorithm, where a prediction happens to be the large language model\\r\\n2:04:24\\r\\nPranshu Tiwari: and optimization happens to be something like a reinforcement learning than a conventional dynamic programming or genetic, genetic\\r\\n2:04:38\\r\\nPranshu Tiwari: or genetic genetic programming. Because we're putting in the steps. And they are trying to find the most optimized, optimized way to reach a solution. That was my 1st question.\\r\\n2:04:46\\r\\nCathal Flanagan: Well, so the\\r\\n2:05:01\\r\\nCathal Flanagan: straightforward answer is, nobody knows, because they haven't. They haven't written a paper or given us any insight into exactly how they have trained or optimized that\\r\\n2:05:05\\r\\nCathal Flanagan: what we do know largely from anthropic rather than Openai is.\\r\\n2:05:13\\r\\nCathal Flanagan: It is largely based on the input from screenshots. Okay and making some decisions.\\r\\n2:05:17\\r\\nCathal Flanagan: The reason it's a separate model is because, of course, the\\r\\n2:05:26\\r\\nCathal Flanagan: the what it has been like the reinforcement elements at the end of like its actions, and what it's trying to do are instead of being outputting an answer, it's outputting a command to a mouse and a keyboard\\r\\n2:05:31\\r\\nCathal Flanagan: pricing. But\\r\\n2:05:42\\r\\nCathal Flanagan: the largest kind of if you think about like what really makes us the unlocks, the power, it's effectively the ability to\\r\\n2:05:45\\r\\nCathal Flanagan: in understand what's happening on the screen and the way it does that is is taking many, many screenshots\\r\\n2:05:52\\r\\nCathal Flanagan: very quickly and updating, based upon, for example, what you'll see often with the operator model is.\\r\\n2:05:59\\r\\nCathal Flanagan: it will misclick into a section.\\r\\n2:06:06\\r\\nCathal Flanagan: It will then take another screenshot. It will be an extra second. It understands that it hasn't achieved what I was expecting to achieve, and then, you know, we'll try clicking again or moving the mouse slightly left or slightly right, and the nice thing is that generally will output and show you that thought process.\\r\\n2:06:09\\r\\nCathal Flanagan: So that's kind of like.\\r\\n2:06:25\\r\\nCathal Flanagan: That's what we know.\\r\\n2:06:28\\r\\nCathal Flanagan: Exactly. The methods of the reward functions of that reinforcement element like we simply just don't have clarity into.\\r\\n2:06:30\\r\\nPranshu Tiwari: Okay? And my second question is in one of the tables you had shown cost per cost in terms of dollars per token.\\r\\n2:06:37\\r\\nPranshu Tiwari: So I'm just imagining things out.\\r\\n2:06:46\\r\\nPranshu Tiwari: and I think this is where I think Deanne had told us, made a comment.\\r\\n2:06:49\\r\\nPranshu Tiwari: Can we see certain large language models which have\\r\\n2:06:55\\r\\nPranshu Tiwari: which in, I think, they have tokens. But each token may have embedded vector so anyone which has shortened embedded vector per token may have less cost of training? Or is it that the the way they have configured their neural network\\r\\n2:07:00\\r\\nPranshu Tiwari: is such\\r\\n2:07:18\\r\\nPranshu Tiwari: that the cost is cheaper, or is it the data is less. Or is it any of the 3.\\r\\n2:07:20\\r\\nCathal Flanagan: So it is charged on a per token basis.\\r\\n2:07:28\\r\\nCathal Flanagan: and generally the cost, you will see, will be will be per 1 million token that tends to be, how default them. And it goes to what is a token for the purposes of this class. I'm very comfortable, basically saying people when they hear the word token. They can think about the words.\\r\\n2:07:32\\r\\nCathal Flanagan: They they can think about it with the word word. Okay. So a token is a word. Technically, it's not. Technically, it's actually more equivalent, like a syllable. Right? We're kind of break a word apart. It might also include some white space before or after the word okay, like this space between words. So\\r\\n2:07:49\\r\\nCathal Flanagan: it is\\r\\n2:08:08\\r\\nCathal Flanagan: like it might take a very long word. Trying to think about a good one at the moment.\\r\\n2:08:11\\r\\nCathal Flanagan: like New York, it might break New and York apart\\r\\n2:08:17\\r\\nCathal Flanagan: right? And those would be 2 tokens, and you're paying double the price. But maybe you get lucky, and then it's like it considers New York to be all of one token right. Then you only pay\\r\\n2:08:21\\r\\nCathal Flanagan: for what the price of one. You don't have a lot of control over that when you pass in the command. If the 1st thing it does is it breaks out those apart into tokens\\r\\n2:08:33\\r\\nCathal Flanagan: and then creates embeddings. But you're getting power. You're getting\\r\\n2:08:44\\r\\nCathal Flanagan: charged based upon the number of words that you're passing in and out.\\r\\n2:08:48\\r\\nCathal Flanagan: Right? So you're actually, you see, different pricing models for different models.\\r\\n2:08:54\\r\\nCathal Flanagan: right? Because effectively, it's more expensive on the output than the input so they'll have different different pricing for input and output tokens and to complicate things even further with the new reasoning models. There's about 30% of the tokens that are generated. You never get to see.\\r\\n2:08:58\\r\\nCathal Flanagan: at least not on the open AI side. They charge you for those 2. So the actual thinking that the model has to do it's outputting, reasoning tokens.\\r\\n2:09:16\\r\\nCathal Flanagan: You're gonna pay for those, but you're never going to see them\\r\\n2:09:25\\r\\nCathal Flanagan: surprise. You have to trust them to some degree. So you should bake in. But if you're doing an estimate and your input and output are going to be a hundred 1,000 in 4,000 out.\\r\\n2:09:28\\r\\nCathal Flanagan: Multiply by 1.3. You're probably going to pay an extra 30% for reasoning tokens.\\r\\n2:09:39\\r\\nPranshu Tiwari: Thank you.\\r\\n2:09:44\\r\\nDima Timofeev: And, Charlie, if you allow me, I also would like to add.\\r\\n2:09:45\\r\\nDima Timofeev: try to make it a bit clearer. Tokens are learned properties. You can train your own tokenization model. It's very simple algorithm which can be implemented like in 3 in 3 screens. And every tokenization model is the same.\\r\\n2:09:49\\r\\nDima Timofeev: And the idea that you have, you're taking huge corpus of data words. And then you slowly start learning these tokens. And eventually you derive to some number of tokens, for example, for Gpt. 3,\\r\\n2:10:06\\r\\nDima Timofeev: it's the vocabulary size, is you? So you eventually derive to 50\\r\\n2:10:19\\r\\nDima Timofeev: a bit more than 50,000 unique tokens for chat. Gpt. 4. It's around. If if I recall correctly, around 100 or 120,000 tokens.\\r\\n2:10:25\\r\\nDima Timofeev: and then each unique token is basically then represented in your embedding space. But again, what you once you learn this property, you don't touch it anymore. You don't pay it for anymore. And then you just try to guess. We call it vocabulary. So we eventually have this\\r\\n2:10:37\\r\\nDima Timofeev: line with 50,000 or 100,000 tokens for words, and we just try to pick which probabilities for each of this word which one to pick. Next, I I will post in a in a second into our chat link to the tool where you can play with different organization models from different model from different providers, including Openai.\\r\\n2:10:55\\r\\nPranshu Tiwari: Thank you.\\r\\n2:11:20\\r\\nCathal Flanagan: Amarash, did you have your hand up from before, or have you a separate question.\\r\\n2:11:24\\r\\nAmarsh Anand: Oh, no, I do have a question. So generally when people say that we are training a model, or we are training AI, right? So let's take an example. You know. Let's say that I'm running a small, the pharmaceutical. I'm not even a company. Let's say that I'm just a pharmacist.\\r\\n2:11:28\\r\\nAmarsh Anand: right. I've got a single shop. I got some stuff, you know, some medicine coming in, and I want to use AI right? So\\r\\n2:11:44\\r\\nAmarsh Anand: if I say that I need to train the model.\\r\\n2:11:52\\r\\nAmarsh Anand: What does that mean? So what comes to my mind as what you have been explaining in the previous lecture, and what we read in the paper about attention is that all these large language models like Gpd, 4 and stuff they've been trained\\r\\n2:11:56\\r\\nAmarsh Anand: right? But they don't have any specifics about, let's say pharmacy. And let's say that I am operating in a certain type of pharmacy. Let's say about baldness. For example, right hair fall.\\r\\n2:12:08\\r\\nAmarsh Anand: What does training mean really that we just need to put more of our instruction manuals, right or other pamphlets? Right that we have in is that what training is or.\\r\\n2:12:19\\r\\nCathal Flanagan: Yeah, it's a great question. It's a it's an overloaded term. And quite frankly, it's a term that if you start hearing people saying that they're training their own models.\\r\\n2:12:29\\r\\nCathal Flanagan: You will want to take a take it with a healthy degree of skeptic skepticism. It's kind of people say it, but it's not actually, technically true.\\r\\n2:12:37\\r\\nCathal Flanagan: So let's break it apart for most\\r\\n2:12:45\\r\\nCathal Flanagan: enterprises who are working in this space and applying AI.\\r\\n2:12:50\\r\\nCathal Flanagan: They're not training their own models.\\r\\n2:12:54\\r\\nCathal Flanagan: They're using an Api, and they're using it to do useful things.\\r\\n2:12:56\\r\\nCathal Flanagan: When or why would you train your own model?\\r\\n2:13:02\\r\\nCathal Flanagan: One reason is security you do not want to use open AI's Apis or anybody else's.\\r\\n2:13:08\\r\\nCathal Flanagan: Therefore you have to train your own.\\r\\n2:13:15\\r\\nCathal Flanagan: Okay, take open a deep seek. You can train this on your own data.\\r\\n2:13:17\\r\\nCathal Flanagan: That's gonna be a very rare case. Okay, it's not so that won't exist.\\r\\n2:13:23\\r\\nCathal Flanagan: But you know, Openai Microsoft, they're all like most people use the cloud right now there are secure. They are like safe to, you know. They don't have to, you know, retention on their enterprise, policies, etc.\\r\\n2:13:27\\r\\nCathal Flanagan: when people will train their own models in the enterprise.\\r\\n2:13:40\\r\\nCathal Flanagan: It tends to be that they will fine tune, take an existing model and fine tune it maybe based upon their own data.\\r\\n2:13:43\\r\\nAmarsh Anand: Okay? And so that that process called fine tuning. When people say we are training effectively, they're saying that they're fine tuning. It.\\r\\n2:13:51\\r\\nCathal Flanagan: Yes, but there's a really important nuance. And this is where we'll start to get controversial. And this is actually a really good topic for the Paper Club.\\r\\n2:13:58\\r\\nCathal Flanagan: This is a good paper I'll bring to the Paper club on this. Actually, maybe we're going to talk about in Paper Club this week.\\r\\n2:14:06\\r\\nCathal Flanagan: I am going to make the assertion\\r\\n2:14:12\\r\\nCathal Flanagan: that most businesses who claim that they are think that they should. Fine tune, should not fine tune, because\\r\\n2:14:14\\r\\nCathal Flanagan: you should not fine tune for knowledge injection.\\r\\n2:14:23\\r\\nCathal Flanagan: You should fine tune for behavior.\\r\\n2:14:27\\r\\nAmarsh Anand: Watch, everything else.\\r\\n2:14:30\\r\\nCathal Flanagan: Okay you mentioned you're a pharmacist.\\r\\n2:14:31\\r\\nCathal Flanagan: You might be like, oh, the model, maybe doesn't know everything I know in my enterprise about pharmacy.\\r\\n2:14:34\\r\\nCathal Flanagan: I am going to fine tune a model just on my data. So I'm going to give it up.\\r\\n2:14:41\\r\\nCathal Flanagan: It has been shown that a far, far better approach is, instead of tuning a model.\\r\\n2:14:47\\r\\nCathal Flanagan: are the last piece of the models over your data\\r\\n2:14:55\\r\\nCathal Flanagan: where you're trying to inject that knowledge into the model, you're actually better off using retrieval, augmented generation.\\r\\n2:14:58\\r\\nCathal Flanagan: Raj.\\r\\n2:15:05\\r\\nCathal Flanagan: Why, a, it performs better empirically.\\r\\n2:15:07\\r\\nCathal Flanagan: 2 fine tuning is expensive, and the second you do it. It's out of date\\r\\n2:15:11\\r\\nCathal Flanagan: right? But rye retrieval elemental generation stays up to date.\\r\\n2:15:17\\r\\nCathal Flanagan: So there are some exceptions to that. Like, for example.\\r\\n2:15:21\\r\\nCathal Flanagan: there's some companies out there that like code. For example, they'll come in and they'll fine tune over your code base. I can see that as a reasonable use case, because your your code is very specific to the way you write it. But again, you're starting to move more into the behavior of the model, like, I want you to output things in a very specific way rather than learn a lot of new information.\\r\\n2:15:26\\r\\nCathal Flanagan: The answer is like, if you had unlimited budget and time and expertise like both rag and fine tuning are the right combination. But if you want to take about like, think about where you get most uplift in the enterprise, it's going to be from rag on questionably. So.\\r\\n2:15:48\\r\\nCathal Flanagan: okay, now, so that's kind of like answering the question of like fine tuning over your knowledge corpus.\\r\\n2:16:01\\r\\nCathal Flanagan: There is a reason you would fine tune a different type of model. However.\\r\\n2:16:09\\r\\nCathal Flanagan: the reason it's 2 different. The other type of fine tuning you can do is like, I want to build a classification model. I want to do that really, really? Well, I want to give it historical examples. That's a good use case for fine tuning. Because if you're fine tuning the behavior of the model more than the knowledge of the model.\\r\\n2:16:14\\r\\nCathal Flanagan: the bigger use case that I expect we'll start to see is people fine tuning, embedding models.\\r\\n2:16:31\\r\\nCathal Flanagan: and we haven't really touched embedding models. We're going to talk about them next week, as it relates to drag. But effectively. What are we in control of in the enterprise?\\r\\n2:16:37\\r\\nCathal Flanagan: We can control retrieval, better retrieval, better results that the model can use to reason over.\\r\\n2:16:45\\r\\nCathal Flanagan: Okay, that's very much still within our control. And it's very much not a cell phone.\\r\\n2:16:54\\r\\nCathal Flanagan: I certainly like this has been top of mind for me. I can share. Maybe I can get something to come and talk about it, actually. But since it's public\\r\\n2:17:00\\r\\nCathal Flanagan: like my team, for example, fine-tuned, an embeddings model over financial tax\\r\\n2:17:08\\r\\nCathal Flanagan: right. And we saw significant uplift in our retrievals results compared to using the open AI Embeddings model because\\r\\n2:17:14\\r\\nCathal Flanagan: our text understands terms like swap and curves deepener. A lot more than the generic kind of use of those terms in the text that that from reddish, that\\r\\n2:17:21\\r\\nCathal Flanagan: open AI might have used. So that's that there are certain cases for fine tuning, but the general thing of people making both statements today on earnings, calls, and other places, saying, We're we're we're fine tuning or sorry we're training our own model.\\r\\n2:17:33\\r\\nCathal Flanagan: I'm my 1st question is, why sorry? That's my route.\\r\\n2:17:47\\r\\nAmarsh Anand: Okay, so very, very quick.\\r\\n2:17:53\\r\\nAmarsh Anand: just so that I understand it. So when we say, most likely they're not training. But when we say somebody is fine tuning. There are 2 different things we talked about adding to the knowledge and the behavior. Now, what we saw earlier in the lecture, the kind of examples that you gave your chain of thought and things like that. You would say that that is also part of fine tuning, like, if I give examples right to any machine.\\r\\n2:17:56\\r\\nCathal Flanagan: Oh, because fine tuning means something very specific where you're actually changing the weights within the model model itself. Right?\\r\\n2:18:17\\r\\nCathal Flanagan: What we are doing is, we're prompting the model to behave in a certain way. So that's what most people mean when they see their their\\r\\n2:18:25\\r\\nCathal Flanagan: training. The model like.\\r\\n2:18:33\\r\\nCathal Flanagan: if you tell if you have got your cousin, or like your your teenager. And you said, Here's how I want you to do this analysis. And here's step one step 2, you would say you're training them how to conduct an analysis. It's very reasonable term to use right. But it isn't\\r\\n2:18:35\\r\\nCathal Flanagan: technically correct as as we think about these models, fine tuning or tuning the models or training the models\\r\\n2:18:51\\r\\nCathal Flanagan: means we're we're actually changing the numeric representation of the weights themselves.\\r\\n2:18:58\\r\\nCathal Flanagan: or or given the data that we have\\r\\n2:19:03\\r\\nCathal Flanagan: most people when they think about training, they're literally like\\r\\n2:19:05\\r\\nCathal Flanagan: they're coaching. I would say it's better word as coaching the model using problem engineering.\\r\\n2:19:08\\r\\nAmarsh Anand: Okay, thanks. Thank you.\\r\\n2:19:13\\r\\nRichard Ryan: And Char. So, Charlie, your assertion was the training that that training or excuse me, fine tuning versus prompt engineering is or is not appropriate for businesses.\\r\\n2:19:15\\r\\nCathal Flanagan: It's not that it's not appropriate. It's just not the 1st thing they should be using. It's an optimization or like it all. It all depends on the problem. But if we think about most businesses that both are trying to build right now is a retrieve generation system for, like asking questions over tens of millions of documents within their organization. Potentially.\\r\\n2:19:28\\r\\nCathal Flanagan: they were not fine tuning a model to do that.\\r\\n2:19:49\\r\\nCathal Flanagan: They're they're building in right system.\\r\\n2:19:53\\r\\nPranshu Tiwari: Yes, and sorry to jump in, but this topic is so interesting, and I know there's a couple of hands in. So what I understand, therefore, is that prompt engineering will therefore guide\\r\\n2:19:55\\r\\nPranshu Tiwari: the scientist to decide whether we need to, whether we need to fine tune the model, or whether we need to develop a rag, or whether we don't need to do anything. So fine tuning is the so prompt engineering would be the 1st step\\r\\n2:20:06\\r\\nPranshu Tiwari: to the bigger decision of of changing the weights completely. Thank you.\\r\\n2:20:20\\r\\nCathal Flanagan: Again.\\r\\n2:20:26\\r\\nCathal Flanagan: I I when you, if you have the use case that you're actually fine tuning a model\\r\\n2:20:27\\r\\nCathal Flanagan: particularly to inject knowledge, please let me know.\\r\\n2:20:32\\r\\nCathal Flanagan: Because a I want to learn what I what what I don't understand of what you're missing or else I want to challenge\\r\\n2:20:36\\r\\nCathal Flanagan: the the assertion that you need to do this.\\r\\n2:20:43\\r\\nAmisha S.: While we're on the subject. Could you also help differentiate between pre-training versus fine tuning.\\r\\n2:20:49\\r\\nCathal Flanagan: Yeah, sure thing. So pre training is\\r\\n2:20:56\\r\\nCathal Flanagan: what Openai has done. For example, with the Gpt 4 model, where they took\\r\\n2:20:59\\r\\nCathal Flanagan: all the text on the Internet, plus plus plus plus as much as they could find.\\r\\n2:21:05\\r\\nCathal Flanagan: And they tuned a model to predict the next word.\\r\\n2:21:09\\r\\nCathal Flanagan: Okay?\\r\\n2:21:14\\r\\nCathal Flanagan: And it's a kind of base model language understanding.\\r\\n2:21:15\\r\\nCathal Flanagan: Okay, you can actually use some of these models like an argument, etc.\\r\\n2:21:22\\r\\nCathal Flanagan: The pre-training models that they come out with are pretty bad.\\r\\n2:21:27\\r\\nCathal Flanagan: Okay.\\r\\n2:21:32\\r\\nCathal Flanagan: like what I mean by that is, if you were and I to use them in the same way we use Chat Gbt, you would not recognize the type of output of the experience. Okay.\\r\\n2:21:33\\r\\nCathal Flanagan: what fine tuning does specifically, the type of fine tuning, supervised fine tuning that turns the Gpt for base model\\r\\n2:21:44\\r\\nCathal Flanagan: into a chat. Gbt, like experience, or even the type of the model. So there's 2 different types of models when they're put out. One is the base model, and one is then called an instruct model. The instruct model is fine tuned to have the type of conversations\\r\\n2:21:54\\r\\nCathal Flanagan: right where they they actually have.\\r\\n2:22:09\\r\\nCathal Flanagan: Generally, it's in Africa. They have labelers who are going through. And they're basically having conversations. And then the model is giving certain answers. And they're choosing between the best one right? And they're giving feedback to the model systematically. And the model, then, is using that feedback to tune itself to get better at having those conversations\\r\\n2:22:12\\r\\nCathal Flanagan: as a total side note. That's also the reason that you see prevalence of words like delve, DELV. E. At a higher rate\\r\\n2:22:32\\r\\nCathal Flanagan: in Chat Gpt than\\r\\n2:22:41\\r\\nCathal Flanagan: other things. So I I the reason I know if someone's submitted an essay and they're cheating, it's because there's certain words that are occurring in Chat Gpt at a higher rate. Why? Because they're more prevalent in Africa.\\r\\n2:22:44\\r\\nCathal Flanagan: right where people actually, they recognize they like those words more.\\r\\n2:22:56\\r\\nCathal Flanagan: So there is these like telltale signs of like that of how that model is tuned. So\\r\\n2:23:00\\r\\nCathal Flanagan: phase model. The role is formed.\\r\\n2:23:07\\r\\nCathal Flanagan: The instruct model is more chat like experience.\\r\\n2:23:10\\r\\nCathal Flanagan: And then, of course, they tune it for safety. And all these other type of things to make it.\\r\\n2:23:15\\r\\nCathal Flanagan: You know, the chat Gpt that we know today.\\r\\n2:23:19\\r\\nCathal Flanagan: there's actually a fantastic talk on exactly this topic.\\r\\n2:23:22\\r\\nCathal Flanagan: I will put it in the channel it's from.\\r\\n2:23:27\\r\\nCathal Flanagan: It might actually oh, my God. Have we mentioned him so many times, Andre Karthi?\\r\\n2:23:32\\r\\nCathal Flanagan: He actually had gave a great talk at the Microsoft Build Conference, where he walked through exactly this of like the different stages of the model development process.\\r\\n2:23:37\\r\\nCathal Flanagan: Totally worth.\\r\\n2:23:49\\r\\nCathal Flanagan: I watch.\\r\\n2:23:50\\r\\nAmisha S.: Thanks, sean.\\r\\n2:23:56\\r\\nRichard Ryan: He might have a link to that on his website. Charlie website. Yeah.\\r\\n2:23:56\\r\\nCathal Flanagan: Yeah, let me let me put it in here. And then there's just one slide that's like, excellent. Where he just walks through this.\\r\\n2:24:01\\r\\nCathal Flanagan: I'm putting it in the chat right now. Who else has a question.\\r\\n2:24:10\\r\\nSarit Arora: I had one question I think this is maybe sort of unrelated. But\\r\\n2:24:18\\r\\nSarit Arora: one of the key concepts that you mentioned was that ability to have the prompt\\r\\n2:24:22\\r\\nSarit Arora: separately will help us scale this better. Right?\\r\\n2:24:28\\r\\nSarit Arora: So if say, multiple of these agents are working, and each of them is using their own prompts, and it's feeding into something else as an output like a Json.\\r\\n2:24:32\\r\\nSarit Arora: How do we? How do we build a system such that there is a traceability back into\\r\\n2:24:43\\r\\nSarit Arora: what was done when, so that in case, if, say downstream agent does something which causes an error or\\r\\n2:24:50\\r\\nSarit Arora: causes something which was undesired, so that you are able to troubleshoot and go back.\\r\\n2:24:57\\r\\nCathal Flanagan: Yeah, it's still a work in progress. The answer right now is, you pay for us through either land chain who have an offering here, or a company called Galileo. They have the 2 better options here which do traces of\\r\\n2:25:03\\r\\nCathal Flanagan: basically the entire reasoning process and different models getting called different times.\\r\\n2:25:15\\r\\nCathal Flanagan: Those are kind of the 2 2 best I've seen.\\r\\n2:25:21\\r\\nSarit Arora: Yeah. Thanks.\\r\\n2:25:24\\r\\nCathal Flanagan: And are we down to the last 2? Yeah, who else do we have here.\\r\\n2:25:31\\r\\nMartin Kuba: Not really.\\r\\n2:25:36\\r\\nSergey Zagorskiy: So.\\r\\n2:25:36\\r\\nCathal Flanagan: First, st and then we'll come to Martin Short after.\\r\\n2:25:37\\r\\nSergey Zagorskiy: Thank you. I have a question about probabilities when you include them into the answer. Is it the probability of the token is the class prediction? Or is it something else? And the reason, I ask is that in a multi-class\\r\\n2:25:41\\r\\nSergey Zagorskiy: classification task I would want model to provide probabilities.\\r\\n2:25:57\\r\\nSergey Zagorskiy: The 3 top classes, and I wonder if\\r\\n2:26:02\\r\\nSergey Zagorskiy: there is a way to achieve this.\\r\\n2:26:07\\r\\nCathal Flanagan: So sorry. I'm not sure\\r\\n2:26:13\\r\\nCathal Flanagan: it is a multi-class classification over\\r\\n2:26:15\\r\\nCathal Flanagan: possible words that it thinks are most likely to be next.\\r\\n2:26:18\\r\\nCathal Flanagan: it's a very, very large multi-class because it's potentially over a lot of different words.\\r\\n2:26:25\\r\\nCathal Flanagan: But I'm not sure I don't think I don't think that's the question.\\r\\n2:26:33\\r\\nSergey Zagorskiy: The question is rather like you mentioned, I have a task of multi-class classification, like, I classify reviews, and each of the reviews themselves. Several classes at once, like mentioning different things.\\r\\n2:26:36\\r\\nSergey Zagorskiy: Right? So. And I want the probability of like\\r\\n2:26:50\\r\\nSergey Zagorskiy: the predicted classes, each of them to be provided. Is there a ways to achieve it.\\r\\n2:26:57\\r\\nCathal Flanagan: Oh, I see it not from the model directly. Oh, sorry! It depends what you mean.\\r\\n2:27:02\\r\\nCathal Flanagan: You mean from the log props that I showed you tonight. You won't be able to.\\r\\n2:27:10\\r\\nCathal Flanagan: It's a good question. I don't know if we can do multi class classification. I've\\r\\n2:27:17\\r\\nCathal Flanagan: I believe it probably can.\\r\\n2:27:24\\r\\nCathal Flanagan: we should check it. That would be a good follow up. I don't want to mislead you. I've only used it for binary classification. I would be surprised if it can't. In fact, I'm pretty sure that it can.\\r\\n2:27:29\\r\\nCathal Flanagan: But you've got severe limitations.\\r\\n2:27:41\\r\\nCathal Flanagan: Anything really above 20 classes.\\r\\n2:27:45\\r\\nCathal Flanagan: You're gonna start seeing real degradation of performance is my intuition. But we should check\\r\\n2:27:48\\r\\nCathal Flanagan: yeah, it's pretty straightforward. We can take the just take what we've done tonight, and just like amend it and put in some additional classes, and you'll find out pretty quickly. I'm pretty sure it'll work, I but I I can't say for exact certain.\\r\\n2:27:57\\r\\nSergey Zagorskiy: Okay. Thank you.\\r\\n2:28:16\\r\\nCathal Flanagan: Morning.\\r\\n2:28:18\\r\\nMartin Kuba: Okay, yeah. So I had a question about\\r\\n2:28:20\\r\\nMartin Kuba: today. You talked about different classes of models\\r\\n2:28:23\\r\\nMartin Kuba: like the workhorse model, the the bigger models like the generic, more general models and\\r\\n2:28:27\\r\\nMartin Kuba: the reasoning ones. I. So for for the workhorse models.\\r\\n2:28:32\\r\\nMartin Kuba: let's say that they're trained on something very specific. They still need to understand language, right? So like is, are there like certain, like standard, like sets of data to teach\\r\\n2:28:36\\r\\nMartin Kuba: every model just like basic understanding of language.\\r\\n2:28:48\\r\\nCathal Flanagan: Well, they're all the same model. They're sorry when I said they're distilled models. There's just smaller versions of the same model. So, for example, anthropic or openai.\\r\\n2:28:52\\r\\nCathal Flanagan: they will train the larger model.\\r\\n2:29:00\\r\\nCathal Flanagan: So, for example, anthropic, they have 3 versions of the same model.\\r\\n2:29:03\\r\\nCathal Flanagan: and they just have a very large one, a medium, and a small one. They're exactly the same model.\\r\\n2:29:07\\r\\nCathal Flanagan: It's just that there's the level of precision, right? The amount of storage that they have in terms of how they store the numerical representation of those words, the weights\\r\\n2:29:12\\r\\nCathal Flanagan: is smaller, takes up less memory, so it does. It loses some intelligence in favor of being\\r\\n2:29:23\\r\\nCathal Flanagan: faster and cheaper and easier to serve the model which gives, they allows them to give it cheaper to you.\\r\\n2:29:34\\r\\nCathal Flanagan: So. But actually, they're all kind of trained. They're all the same model. They're just distill what's called distilled versions of the model itself.\\r\\n2:29:41\\r\\nCathal Flanagan: What's really interesting is because of that.\\r\\n2:29:50\\r\\nCathal Flanagan: With prompt engineering you can generally guess\\r\\n2:29:54\\r\\nCathal Flanagan: the best level, the top level performance from the lowest cost model.\\r\\n2:29:59\\r\\nCathal Flanagan: If you really really prompt engineers. Well, that has been like anthropic if they came in, that's exactly what they would tell you.\\r\\n2:30:06\\r\\nCathal Flanagan: Right? You can. Workhorse models can be just as smart with very very good, prompt engineer.\\r\\n2:30:14\\r\\nMartin Kuba: Okay. So maybe I misunderstood about the workhorse models. I thought that they were like more specialized\\r\\n2:30:21\\r\\nMartin Kuba: on something like, maybe, like better recording.\\r\\n2:30:27\\r\\nCathal Flanagan: Older, cheaper, faster.\\r\\n2:30:30\\r\\nMartin Kuba: Okay. Okay.\\r\\n2:30:31\\r\\nCathal Flanagan: Now there, there are specialized models as well. The point I was making is the specialized models empirically are not showing your like. The the models trained specifically for finance, specifically for\\r\\n2:30:34\\r\\nCathal Flanagan: healthcare, specifically for law. At the moment they're not showing themselves as performing better in terms of the knowledge that they have\\r\\n2:30:46\\r\\nCathal Flanagan: about compared to like the Gpt general models.\\r\\n2:30:56\\r\\nMartin Kuba: Okay.\\r\\n2:31:01\\r\\nMartin Kuba: Thank you.\\r\\n2:31:04\\r\\nCathal Flanagan: Alright, I think we've we've exhausted everyone thank you for your attention. We appreciate. You guys showing up and participating as much continue the conversation in slack.\\r\\n2:31:08\\r\\nCathal Flanagan: hopefully. See you in one of the coffee hours, or the office hours, or paper club, or something over the weekend otherwise we'll see you next week. We'll definitely have an office hours before class. We'll figure out the topic in advance.\\r\\n2:31:20\\r\\nCathal Flanagan: Thank you. Folks appreciate it.\\r\\n2:31:34\\r\\nJeremy McCormick: Thanks, so much.\\r\\n2:31:36\\r\\nDima Timofeev: Thank you, everybody.\\r\\n2:31:37\\r\\nDima Timofeev: Goodbye.\\r\\n2:31:38\\r\\nJANE: Thank you.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='453bef78-1268-4c26-aa54-c9bb75e3e529', embedding=None, metadata={'file_path': '/content/Tech16_Captions/Lecture3.txt', 'file_name': 'Lecture3.txt', 'file_type': 'text/plain', 'file_size': 172022, 'creation_date': '2025-02-20', 'last_modified_date': '2025-02-20'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Lecture 3 Captions\\r\\n\\r\\nCathal Flanagan: Okay, hopefully, we are recording excellent so Hi, everyone welcome back.\\r\\n0:03\\r\\nCathal Flanagan: Week, 3, definitely going. Always going faster. We would like. But interesting times,\\r\\n0:14\\r\\nCathal Flanagan: let me share my screen and we'll we'll get going here.\\r\\n0:25\\r\\nCathal Flanagan: So\\r\\n0:35\\r\\nCathal Flanagan: tonight, we're gonna dive deep into retrieval augmented generation, we're actually gonna spend the next 2 weeks on it.\\r\\n0:37\\r\\nCathal Flanagan: So we've already talked about a lot. It is kind of the that\\r\\n0:46\\r\\nCathal Flanagan: workhorse of all things that we do in the enterprise these days. Really, that's you know, most AI systems are retrieval augmented generation systems. So\\r\\n0:52\\r\\nCathal Flanagan: we're gonna go into a good amount of depth tonight, and a good amount of code\\r\\n1:01\\r\\nCathal Flanagan: the next 2 weeks will actually be the the heaviest coding\\r\\n1:05\\r\\nCathal Flanagan: sessions largely within the semester. Because we're building, you know, kind of systems that you might build build at work. Or at least they they kind of they the structure of of them. Not to be intimidated. Don't stay with us. We're gonna walk through them into good amount of detail. And afterwards you okay? Well, commented notebooks. That kind of you also can, will help you walk through the concepts.\\r\\n1:10\\r\\nCathal Flanagan: But, as usual, our\\r\\n1:35\\r\\nCathal Flanagan: class mission is to help lower the barrier to use some of these Lm tools in everyday business settings.\\r\\n1:38\\r\\nCathal Flanagan: Hopefully, you're ready\\r\\n1:46\\r\\nCathal Flanagan: thinking about this when you're at work. You know some of the tools that we're building how you might apply them yourself.\\r\\n1:47\\r\\nCathal Flanagan: and I think that will probably hopefully get you in even more inspiration after tonight.\\r\\n1:56\\r\\nCathal Flanagan: as a reminder values of the class. We are very focused on providing support to those who are just starting out.\\r\\n2:02\\r\\nCathal Flanagan: focusing on the practical. So those 2 will come together tonight. You know we are going to be doing a decent amount of coding. There are some folks I know who for whom coding is new. We're having interesting conversation in the office hours where I was basically saying.\\r\\n2:09\\r\\nCathal Flanagan: you know, in this area, where the things are moving very quickly, we're all using tools and libraries that\\r\\n2:25\\r\\nCathal Flanagan: other people have written, that we all, don't, you know, understand every module and everything that what's happening underneath the hood in the same way. You don't understand how a car is working when you're driving it per se and that's fine, like. Ultimately, you want to have a good intuition around the building blocks.\\r\\n2:33\\r\\nCathal Flanagan: How we're putting things together at a high level, how they're what is kind of working but just you know, don't feel intimidated. If you're starting out.\\r\\n2:53\\r\\nCathal Flanagan: It's enough kind of just understand the very high level concepts. You know, we're tonight. We're going to be taking knowledge bases from documents.\\r\\n3:03\\r\\nCathal Flanagan: breaking them apart, searching over them, and giving that information to the model\\r\\n3:12\\r\\nCathal Flanagan: rather than having the model work, its work, its magic to answer.\\r\\n3:18\\r\\nCathal Flanagan: So you know, I definitely we are we while we're getting into more conf, more technically conf.\\r\\n3:22\\r\\nCathal Flanagan: technically.\\r\\n3:29\\r\\nCathal Flanagan: challenging concepts. I, you know we want to take a step back at points. Make sure that you feel comfortable. And again don't feel intimidated by the code. If you're you know, at the very high level. If all you work out is\\r\\n3:31\\r\\nCathal Flanagan: of this, these sessions is of a high level understanding of how rag works. That's perfectly. That's that's a win in itself.\\r\\n3:46\\r\\nCathal Flanagan: And then, you know, as usual, we hopefully will continue to provide some interesting knowledge and insight along the way. Whether it's just related to what's been in the news this week or you know other conversations around\\r\\n3:55\\r\\nCathal Flanagan: AI investments.\\r\\n4:08\\r\\nCathal Flanagan: I'm glad to see the community is strong and getting stronger. We have a bunch of people, either in research. I think we might even have a few people in a watch party tonight, which is pretty fun. And\\r\\n4:14\\r\\nCathal Flanagan: you know, we want to continue to to have folks meet up\\r\\n4:30\\r\\nCathal Flanagan: bay area, obviously home to a lot of folks taking the class.\\r\\n4:35\\r\\nCathal Flanagan: I know there's a number of different meetups happening either in San Francisco or down the bay itself. So definitely.\\r\\n4:39\\r\\nCathal Flanagan: even if you just put up on slack, there's a handful of people meeting for coffee. Just invite others, and go in and have fun and meet other people from the class.\\r\\n4:48\\r\\nCathal Flanagan: Same in international locations.\\r\\n4:57\\r\\nCathal Flanagan: I will be in\\r\\n5:01\\r\\nCathal Flanagan: Abu Dhabi next week, myself and Hong Kong, and then Japan the week after that. So anyone is in any of those locations or let me know, and we're we'd be love. I'd love to\\r\\n5:04\\r\\nCathal Flanagan: catch up with with folks.\\r\\n5:16\\r\\nCathal Flanagan: Dima still hasn't gone viral on social media. So we're gonna continue to\\r\\n5:22\\r\\nCathal Flanagan: to to try and embarrass him. Hashtag Stanford, Tech, 16 or hashtags. Tech 16. If you're so obliged that you want to post anything from the class on social media, we fully encourage it. Don't be shy.\\r\\n5:27\\r\\nDima Timofeev: I'm not getting younger guys. 5 more years, and I will not dance anymore for Tiktok.\\r\\n5:43\\r\\nCathal Flanagan: Exactly. So.\\r\\n5:49\\r\\nCathal Flanagan: I'm gonna I'm gonna avoid my jokes if I don't deem as only fans for now. But you know, maybe in an office hours.\\r\\n5:53\\r\\nCathal Flanagan: Aha!\\r\\n6:00\\r\\nCathal Flanagan: Alright! So we're on week 3. Tonight's all about rag\\r\\n6:03\\r\\nCathal Flanagan: Next week we're just gonna continue the rag journey and then, after that. You know, we're gonna be going into kind of moving beyond open AI type models. And looking at open source models, talking about fine tuning kind of. As I said, we're at the hump right now. It's gonna get a little bit more technically challenging more code. In the next 2 weeks. Bear with us. We're gonna get through it, I promise. That's the much more tractable, you might think, and actually more interesting\\r\\n6:08\\r\\nCathal Flanagan: for homework. You're gonna be able to explore lots of data connectors, and just kind of be amazed at all the different things you could apply these AI tools to.\\r\\n6:35\\r\\nCathal Flanagan: Homework review. I did. Anyone have a chance to look at other folks homeworks and see anything of interest? Or did anyone have any interesting insights from the homework that they did.\\r\\n6:46\\r\\nJeremy McCormick: It's really easy to go over your token limit if you don't, if you don't watch it, I found that. Yeah.\\r\\n7:00\\r\\nCathal Flanagan: What were you? What were you doing that was causing you to go over the token limit.\\r\\n7:06\\r\\nJeremy McCormick: If you just send it. You know big blocks of text from like things you want it to use as context, like, I was doing something with a standard I work for work with for work.\\r\\n7:11\\r\\nJeremy McCormick: And or I was comparing, like 2 standard different versions of standards, documents, and I found that the map reduced worked well for staying inside the token limit, because if you just send the raw text, it's very. It's pretty easy to go. If it's a big documents, it seems pretty easy to just exceed the the request limit.\\r\\n7:22\\r\\nCathal Flanagan: Yep. Interesting. Anybody else have any observations.\\r\\n7:43\\r\\nPranshu Tiwari: No, actually, this is pranshu. I wanted to actually get the mathematical\\r\\n7:47\\r\\nPranshu Tiwari: difference between 2. I messed up documentation as well as the mapreduce, and also between different Apis. So I tried to use log probability and try to put the questions. Objectivity like, what does it show in terms of yes or no. So I think my log props wasn't working. So if somebody can give me the Api for which log props work. It would be fine for me.\\r\\n7:54\\r\\nCathal Flanagan: I'm sorry. Just tell me, just tell us what you're trying to do again. You were trying to get the mathematical differences between them. I I don't fully understand that use case.\\r\\n8:24\\r\\nPranshu Tiwari: Okay? So so in the prompt, initially, I had put the prompt as\\r\\n8:33\\r\\nPranshu Tiwari: you know, as something which can give subjective answer. But then I thought, Hey, is there a way to evaluate the Apis, or even the even the functions by putting questions in yes or no format.\\r\\n8:40\\r\\nPranshu Tiwari: and rather than saying yes or no, I just wanted to get the lock probabilities\\r\\n8:55\\r\\nPranshu Tiwari: and try to create entropy function for my answers, so that I can differentiate and say, Hey, actually, this Api is working better, or maybe this this function, like\\r\\n8:59\\r\\nPranshu Tiwari: mapreduce is working better than stuff staff documents. But my log props wasn't working, so I was getting answers in yes and no. But I wasn't get able to get the probability.\\r\\n9:13\\r\\nCathal Flanagan: Okay, interesting. I added.\\r\\n9:24\\r\\nCathal Flanagan: We I would suspect that. So when you're using log probs, you're effectively it's when you're calling the open AI Api directly. When you're using the Line Chain library, you're using kind of their abstraction on top of that\\r\\n9:28\\r\\nCathal Flanagan: which may or may not have things like log probs a as an input that you're able to enable right? So.\\r\\n9:44\\r\\nPranshu Tiwari: Yeah.\\r\\n9:52\\r\\nCathal Flanagan: It wouldn't naturally like that type of function of like. Give me a classification and give me a log. Prompt. Wouldn't naturally kind of.\\r\\n9:52\\r\\nCathal Flanagan: So that wouldn't make sense. I understand what you're doing on what you're trying to achieve. But that would be something that wouldn't just kind of naturally fall out of the Summarization Library.\\r\\n10:00\\r\\nPranshu Tiwari: Okay.\\r\\n10:11\\r\\nCathal Flanagan: You could recreate that yourself by, you know, in the very in the 1st homework, if you took that, and then kind of built your own version of kind of for summarization. But yeah, you're not going to like if you're when you're passing into the method. For where you're passing in mapreduce.\\r\\n10:12\\r\\nCathal Flanagan: the ability to turn on log policy or just simply doesn't exist.\\r\\n10:29\\r\\nPranshu Tiwari: Okay, okay, that's why. Okay, thank you. Maybe I could try the earlier room. But yeah, that's fine.\\r\\n10:32\\r\\nTony Arteaga: So.\\r\\n10:38\\r\\nCathal Flanagan: Assuming that\\r\\n10:40\\r\\nTony Arteaga: The context window token limit isn't an issue.\\r\\n10:42\\r\\nTony Arteaga: The main difference between well, the main difference between mapreduce and stuff is the fact that you can chunk with mapreduce. But putting that aside for a minute, prompt windows keep getting bigger and bigger nowadays. So, putting that aside for a minute, the quality of the summary between mapreduce and stuff.\\r\\n10:48\\r\\nTony Arteaga: Meaning my limited example. It seemed to be pretty equal.\\r\\n11:10\\r\\nTony Arteaga: But do do you have an opinion? If, if, if the the context window limit isn't an issue, as to what is there a difference between the summaries that the tube will generate or not?\\r\\n11:15\\r\\nCathal Flanagan: Yeah, people will. My own observation is that.\\r\\n11:28\\r\\nCathal Flanagan: And and it makes sense that in mapreduce summarization might lack detail\\r\\n11:34\\r\\nCathal Flanagan: or might not pick up on salient details. Why? Because effectively, what you're ultimately creating is a summary of summaries.\\r\\n11:41\\r\\nCathal Flanagan: right? So much like a photocopy of photocopies.\\r\\n11:49\\r\\nCathal Flanagan: Would love. It would basically just it would lack some, some detail so\\r\\n11:53\\r\\nCathal Flanagan: that would tend to be the reason for\\r\\n11:59\\r\\nCathal Flanagan: for not using it and for using stuff when possible. But there is another one. There is another method or or type of one that I know. Some people discovered. Where did other folks. What other one method did people use that, they observed, might have added\\r\\n12:04\\r\\nCathal Flanagan: something different or extra.\\r\\n12:23\\r\\nSunil Samel: Refine.\\r\\n12:25\\r\\nTheo Koulis: Terrifying.\\r\\n12:26\\r\\nCathal Flanagan: Interesting. Okay, so tell me, what does what does refined do.\\r\\n12:27\\r\\nTheo Koulis: It. So it's almost like the mapreduce. But what it does is essentially it updates\\r\\n12:32\\r\\nTheo Koulis: the current summary based on new information that's given to it.\\r\\n12:41\\r\\nTheo Koulis: And if it doesn't think that\\r\\n12:46\\r\\nTheo Koulis: if if if it seems that there's no new information to add, then it it doesn't update. And so\\r\\n12:49\\r\\nTheo Koulis: it's court. It's sort of like adaptive in that sense.\\r\\n12:56\\r\\nCathal Flanagan: Right? Specifically, it takes the 1st chunk of\\r\\n13:01\\r\\nCathal Flanagan: of information and it starts. It summarizes that, and then for your point, it adds it summar. It starts looking at the new information and seeing if there's anything in addition that's new or interesting, that it should append or add to that summary. It does take it, as you know, same. It takes a long time.\\r\\n13:05\\r\\nCathal Flanagan: is. It might. It's interesting in things like research where maybe you'll have an abstract on the front page, which kind of is a summary of the paper in itself.\\r\\n13:22\\r\\nCathal Flanagan: and\\r\\n13:32\\r\\nCathal Flanagan: that will be like 90% of all the information that will need to be summarized, and then it just starts adding on throughout the paper, like, certainly, as you get towards the end.\\r\\n13:33\\r\\nCathal Flanagan: looking into references and note footnotes.\\r\\n13:42\\r\\nCathal Flanagan: You're you'd expect that be less interesting things to summarize there. So so that any of these are right or wrong. It's just kind of depending on your particular use case. It's worth trying. All 3, some of them from a technical perspective.\\r\\n13:45\\r\\nCathal Flanagan: You might be forced to use mapreduce or refine because you can't fit everything in the context. Meg.\\r\\n14:00\\r\\nCathal Flanagan: but yeah, just definitely interesting to to observe different approaches and kind of again, it goes to the fact that this is largely art rather than science, depending on your particular use case to kind of see what works best for you.\\r\\n14:06\\r\\nCathal Flanagan: My head.\\r\\n14:19\\r\\nDima Timofeev: Thanks.\\r\\n14:20\\r\\nmahesh: Yeah, I was. In my test I was. I was planning to give a movie story.\\r\\n14:22\\r\\nmahesh: And as part of the summary. I wanted to extract different characters in that story. I could not figure out how that you know how I can extract all the characters in that story?\\r\\n14:30\\r\\nmahesh: Is there a. Is there a way.\\r\\n14:42\\r\\nCathal Flanagan: Okay. So you wanted to go through the entire document and extract all of the all of the the characters interesting.\\r\\n14:45\\r\\nCathal Flanagan: So I guess it depends on how you think about that task. Right?\\r\\n14:53\\r\\nCathal Flanagan: Is that a summarization task?\\r\\n14:59\\r\\nCathal Flanagan: Probably not.\\r\\n15:03\\r\\nmahesh: Yeah, yeah, it's yeah. Obviously, it's not a summarization task. But you know, I wanted to do that one extra thing. Right? Okay, I got the summarization, but I wanted to.\\r\\n15:05\\r\\nCathal Flanagan: And if that's not a summarization task and don't use the summarization methods to do this, do it right? That would probably be a better task for the functionality we saw in the 1st class. Right? We're just passing in an instruction and passing in the raw text, and, you know, giving a clear prompt of what to do and how to do it and giving it some examples.\\r\\n15:15\\r\\nCathal Flanagan: It could potentially be a use case for tonight for retrieval, augmented generation. But actually, it's an interesting one to think about as we're going through it. Because.\\r\\n15:35\\r\\nCathal Flanagan: do I? Can I tell you that\\r\\n15:47\\r\\nCathal Flanagan: that task would be well suited to retrieve augmented generation. Possibly not. Why? Because of the retrieval part. Retrieval, augmented generation means that we are searching over text, finding relevant paragraphs and returning just those paragraphs or chunks of text.\\r\\n15:49\\r\\nCathal Flanagan: And you want that to be more exhausted if you want to consider all the text so\\r\\n16:08\\r\\nCathal Flanagan: for that one particular task where you only have one document, I would actually be suggesting you're trying to pass in all of it, and then giving it a very, very good, prompt, with examples of what those characters look like, because, again, you want it to be very exhaustive and be guaranteed that it's a considering all the possible types.\\r\\n16:14\\r\\nCathal Flanagan: Thank you.\\r\\n16:35\\r\\nCathal Flanagan: And then, yes, we'll take one more, Spitha, and then we'll move on.\\r\\n16:37\\r\\nSmitha Chennu: Yeah, Hi, so I did the summarization exercise. But I didn't find any difference in the results. But the time taken for mapreduce versus stuff deferred. I think Mapreduce was probably close to 1 min versus the other one was 10 seconds. So is that expected? And I was just wondering if the homework was done correctly, because I\\r\\n16:41\\r\\nSmitha Chennu: I tried to use all the methods which you guys shared.\\r\\n17:05\\r\\nCathal Flanagan: So try. Is that expected? Or something went wrong there? And I,\\r\\n17:09\\r\\nCathal Flanagan: can anyone tell why that is expected.\\r\\n17:14\\r\\nSmitha Chennu: Sorry.\\r\\n17:18\\r\\nCathal Flanagan: I'm I'm asking the class. Does anyone? Is that expected? And does anyone can anyone give intuition, or what around? Why, that would be expected.\\r\\n17:19\\r\\nSunil Samel: Yeah, thank, you.\\r\\n17:29\\r\\nAmarsh Anand: These Http calls right? You can actually see that on the console.\\r\\n17:30\\r\\nSunil Samel: Well, I thought the answer was that the context windows of these models has now grown so big\\r\\n17:37\\r\\nSunil Samel: that you can't really get unless you have very large documents with very interconnected kind of logic. You cannot really make out the difference between stuff and mapreduce.\\r\\n17:43\\r\\nCathal Flanagan: Yeah. So the.\\r\\n17:57\\r\\nAmarsh Anand: The the difference here.\\r\\n18:02\\r\\nCathal Flanagan: Would be actually that\\r\\n18:05\\r\\nCathal Flanagan: map we would not expect map. Mapreduce would possibly not be able to pick up on the same level of detail.\\r\\n18:09\\r\\nCathal Flanagan: Now you may not be observing. You may be observing similar. And that's great.\\r\\n18:15\\r\\nCathal Flanagan: The\\r\\n18:21\\r\\nCathal Flanagan: a year ago we would have had we would have been forced to use Mapreduce because the context length of the models was just simply not long enough for most tasks, and we already had some folks in the class who said they were running into context like errors. So\\r\\n18:23\\r\\nCathal Flanagan: you know, right? Model right? Right methodology for the right task. If you are, if your task\\r\\n18:39\\r\\nCathal Flanagan: will not exceed the context window.\\r\\n18:45\\r\\nCathal Flanagan: Then you will to do stuff, in my opinion.\\r\\n18:49\\r\\nCathal Flanagan: If you and again, this means once you're not cost conscious as well. Right?\\r\\n18:52\\r\\nCathal Flanagan: But if you're\\r\\n18:59\\r\\nCathal Flanagan: if you are running into a context window error which you can check, for, you can always do an if right, if it's if it tries with stuff. If it's getting an error, then you move to a mapreduce right? Which solves the problem of\\r\\n19:02\\r\\nCathal Flanagan: text volume that's exceeding the actual window.\\r\\n19:17\\r\\nAmarsh Anand: Isn't it true that the time taken when I noticed exactly the same? But in my case I noticed that the time taken is because it takes a long time to make an Http connection. So if you have got 50 chunks.\\r\\n19:22\\r\\nAmarsh Anand: then it's gonna take 50 times that time to actually upload whatever content you have\\r\\n19:34\\r\\nAmarsh Anand: as opposed to loading all of it at once.\\r\\n19:39\\r\\nCathal Flanagan: That's.\\r\\n19:43\\r\\nDima Timofeev: They argue that the latency, if you have meaningful Internet connection, latency, to do 50 requests, especially even in a sequential order, is nothing close to the inference time that a land will take.\\r\\n19:43\\r\\nDima Timofeev: so probably it's more about server side inference rather than Http. Intent latency.\\r\\n19:58\\r\\nCathal Flanagan: But listen, it's a real consideration the dimensions here are model complexity, like Mo. How good the model is.\\r\\n20:09\\r\\nCathal Flanagan: Reasoning models will take longer than a 4 0 model, for example.\\r\\n20:17\\r\\nCathal Flanagan: context length, right? A Gemini model will give you 2 million tokens versus a hundred. Thou 128,000 for an open AI model cost\\r\\n20:23\\r\\nCathal Flanagan: and speed. These are all dimensions, it and it. This is where it's an art, you know.\\r\\n20:34\\r\\nCathal Flanagan: You effectively have to figure out what is the right model. What is right? Methodology here for the task at hand.\\r\\n20:39\\r\\nSmitha Chennu: Oh, thank you!\\r\\n20:47\\r\\nCathal Flanagan: Sure thing.\\r\\n20:48\\r\\nCathal Flanagan: Cool, Jeremy, I'm gonna come back to you. Maybe in the break just because I want to move on.\\r\\n20:50\\r\\nTheo Koulis: Okay.\\r\\n20:56\\r\\nCathal Flanagan: Awesome. Okay? So we've spoken about this concept a lot tonight. It's time to kind of dive deeper into it.\\r\\n20:57\\r\\nCathal Flanagan: Retrieval, augmented generation.\\r\\n21:06\\r\\nCathal Flanagan: I'm going to start using the term rag from now on, because that's how we commonly refer to it.\\r\\n21:08\\r\\nCathal Flanagan: Why do we do rag?\\r\\n21:15\\r\\nCathal Flanagan: Well, if I could have everyone write down one thing it would be. This Openai is not a database.\\r\\n21:19\\r\\nCathal Flanagan: We use it as a database in our everyday lives, but it's we shouldn't.\\r\\n21:29\\r\\nCathal Flanagan: Why should we not\\r\\n21:37\\r\\nCathal Flanagan: one? It has a knowledge cutoff date, the core model in Openai. If you use it through the Api\\r\\n21:39\\r\\nCathal Flanagan: and ask it. Who is the President? It will tell you the President is Joe Biden.\\r\\n21:46\\r\\nCathal Flanagan: Why it has. It was trained up until October of 2023.\\r\\n21:51\\r\\nCathal Flanagan: Okay, it only knows things about the world up until that date.\\r\\n21:57\\r\\nCathal Flanagan: even when you try and use it for information that was accessible before October 2023.\\r\\n22:04\\r\\nCathal Flanagan: Research suggests that about 3% of the time\\r\\n22:10\\r\\nCathal Flanagan: you would, if you try and use it to access its internal information or knowledge, it will hallucinate, which is a fancy way of saying it makes things up\\r\\n22:14\\r\\nCathal Flanagan: which if we're trying to use these models and systems in the enterprise.\\r\\n22:23\\r\\nCathal Flanagan: if we, if we think that they're going to lie to us 3% of the time. It's like, if you were dealing with a human. And you thought they were trained going to lie to you at that sort of rate.\\r\\n22:28\\r\\nCathal Flanagan: You would lose trust pretty quickly. Right? And quite frankly, you would just stop using the system, because.\\r\\n22:38\\r\\nCathal Flanagan: you know, we need to have high levels of high levels of\\r\\n22:45\\r\\nCathal Flanagan: conviction that the systems we're using are kind of giving us clear price information. Now, there's certainly a higher tolerance when it comes to AI systems. I think people understand that we're at the bleeding edge of in terms of technology here. And you know, we take on a certain amount of risk when we're working with them. There's ways we can minimize that risk. We're going to talk with them in follow up classes.\\r\\n22:51\\r\\nCathal Flanagan: But you know it's a known limitation. It's it's\\r\\n23:13\\r\\nCathal Flanagan: part of the reason we do not want to use the information. The model it was trained on, but rather we want to provide the information to the model, to make its decision for its consideration.\\r\\n23:16\\r\\nCathal Flanagan: And then, you know, the largest kind of reason that we don't use Llms in the enterprise. As for their knowledge is because they don't have the knowledge that we need.\\r\\n23:29\\r\\nCathal Flanagan: even though they've been trained up until October of 2023.\\r\\n23:38\\r\\nCathal Flanagan: Really.\\r\\n23:42\\r\\nCathal Flanagan: most of the information that is most valuable to us in the enterprise is within our own knowledge bases. It's within sharepoint. It's within onenote. It's within Google Docs. It's within Crm systems like salesforce workday.\\r\\n23:45\\r\\nCathal Flanagan: You know, that's where the the knowledge of our organization resides. And of course that is not accessible to the model unless we actually make it accessible to the model.\\r\\n24:00\\r\\nCathal Flanagan: Okay? And then when we make it accessible, well, we have to make it accessible. Well.\\r\\n24:10\\r\\nCathal Flanagan: we have to be able to have the model interact\\r\\n24:14\\r\\nCathal Flanagan: well with those systems, be able to search over them, get relevant results know how to interpret those results.\\r\\n24:17\\r\\nCathal Flanagan: So that's where the concept of retrieval augmented generation comes in.\\r\\n24:25\\r\\nCathal Flanagan: Let's break that down.\\r\\n24:32\\r\\nCathal Flanagan: The key word is retrieval\\r\\n24:34\\r\\nCathal Flanagan: when we actually ask a question, give a command to a model. From now on.\\r\\n24:38\\r\\nCathal Flanagan: We are not expecting the model to automatically answer that question.\\r\\n24:45\\r\\nCathal Flanagan: Right? We are. We are the ones who are responsible. Much like in the summarization task, we are responsible for\\r\\n24:49\\r\\nCathal Flanagan: retrieving the most relevant information that could be the most relevant information within a single document.\\r\\n24:56\\r\\nCathal Flanagan: or it could be retrieving the most relevant document amongst tens of millions of documents.\\r\\n25:03\\r\\nCathal Flanagan: more often and very quickly, that will become millions of documents rather than single documents. But of course, we start with single documents, and we build from there\\r\\n25:08\\r\\nCathal Flanagan: we do a lookup.\\r\\n25:17\\r\\nCathal Flanagan: We find relevant paragraphs within text, we find relevant documents within corpuses.\\r\\n25:19\\r\\nCathal Flanagan: We retrieve those, and only those are passed into the model itself.\\r\\n25:24\\r\\nCathal Flanagan: And the model. We'd like to say we use it for its skills and not its knowledge.\\r\\n25:30\\r\\nCathal Flanagan: The model is smart enough to understand. Given the question that we are asking\\r\\n25:35\\r\\nCathal Flanagan: or the command that we're giving\\r\\n25:41\\r\\nCathal Flanagan: with the kind of passing that information in it will use just that to actually answer the question itself, and then, of course, it will generate a response. Much like we've already been doing in this class.\\r\\n25:44\\r\\nCathal Flanagan: and so introducing this retrieval step\\r\\n25:55\\r\\nCathal Flanagan: is key, and is kind of at the heart of everything we do, as it relates to using large language models in the enterprise.\\r\\n25:59\\r\\nCathal Flanagan: Let me pause there in case there's any questions on just the concept of retrieval augmented generation.\\r\\n26:09\\r\\nCathal Flanagan: Aj.\\r\\n26:14\\r\\nAjay Dawar: I'm sorry. Does that mean\\r\\n26:16\\r\\nAjay Dawar: that in this diagram, where it says question, plus whatever was retrieved? And then, Lns generator, does that mean that\\r\\n26:18\\r\\nAjay Dawar: we're not.\\r\\n26:26\\r\\nAjay Dawar: We're we're asking the Llm. Not to be influenced by\\r\\n26:28\\r\\nAjay Dawar: the knowledge that it may have already. And we're just trying to use some abstraction of its\\r\\n26:33\\r\\nAjay Dawar: neural net models.\\r\\n26:40\\r\\nCathal Flanagan: That is correct.\\r\\n26:42\\r\\nCathal Flanagan: We actually will explicitly be\\r\\n26:44\\r\\nCathal Flanagan: next week we'll be talking to about advanced rank, and one of the things we'll be talking about is how to actually prompt the knowledge, the model and giving it\\r\\n26:48\\r\\nCathal Flanagan: permission to disappoint us.\\r\\n26:57\\r\\nCathal Flanagan: because even when we do this, even when we actually give the model the right information.\\r\\n27:01\\r\\nCathal Flanagan: even if it, if we don't actually give us all of the information that it requires to make a decision\\r\\n27:06\\r\\nCathal Flanagan: or to answer a question, it wants to be helpful.\\r\\n27:12\\r\\nCathal Flanagan: So therefore it doesn't want to disappoint us.\\r\\n27:16\\r\\nCathal Flanagan: There's 2 things I always append to my quotes or to my prompts or my system prompts more than anything else. Right? Because you can put in a prompt that will apply to all of the prompts you pass in.\\r\\n27:19\\r\\nCathal Flanagan: I put in the words, be concise\\r\\n27:30\\r\\nCathal Flanagan: when I say I add the words, if you do not know the answer.\\r\\n27:32\\r\\nCathal Flanagan: or you do not the information to answer the question. Do not make it up. Just say, I don't know.\\r\\n27:36\\r\\nCathal Flanagan: Okay, give the model permission to disappoint you.\\r\\n27:43\\r\\nCathal Flanagan: Okay, it will actually go a long way in particularly retrieval of generation is a big help.\\r\\n27:46\\r\\nCathal Flanagan: But it's not.\\r\\n27:52\\r\\nCathal Flanagan: you know. We still need to give the model the ability to to say, I don't know much like. Honestly, it's much like a human. We actually, if you're talking to junior people on your team you're like, don't make you know. Just say I don't know if you don't know the answer, and you know we'll we'll we'll get to a better place.\\r\\n27:53\\r\\nAjay Dawar: Quick follow up on that. So in that case, how would the Llm. Even understand the question, if it has no\\r\\n28:10\\r\\nAjay Dawar: outside of the knowledge base, no other knowledge base to go. Rely on.\\r\\n28:18\\r\\nCathal Flanagan: Though\\r\\n28:23\\r\\nCathal Flanagan: how does it? Or something? Well, we're passing in the question right and smart enough to understand that what we were, the instruction we were passing is use only the context provided to answer the question. Right? So it's not that it doesn't know. It's it's a language model. The language that's getting passed in. It's just that\\r\\n28:25\\r\\nCathal Flanagan: it will generally follow the instruction of only considering the knowledge\\r\\n28:42\\r\\nCathal Flanagan: that we're passing in to answer that question. So\\r\\n28:48\\r\\nCathal Flanagan: the instruction. It is good at following instruction right? And and you know, when given that, and that is, tend to respect it not always, but\\r\\n28:51\\r\\nCathal Flanagan: to them\\r\\n29:03\\r\\nCathal Flanagan: Mahesh.\\r\\n29:09\\r\\nmahesh: Yeah, so here, when we say, smart retriever?\\r\\n29:11\\r\\nmahesh: What does it really mean? Do we need? Do we need to write that code which will understand the question. And then we know\\r\\n29:16\\r\\nmahesh: you know, what is the relevant document for that question, and then pass it to Llm. Or is it something else?\\r\\n29:24\\r\\nCathal Flanagan: Yes, and that's where you're gonna build it. So you're gonna see it. The key here is.\\r\\n29:31\\r\\nCathal Flanagan: how do we find the most relevant\\r\\n29:36\\r\\nCathal Flanagan: document or portion of text within a document.\\r\\n29:40\\r\\nCathal Flanagan: This goes back to the 1st thing we learned about in the 1st class embeddings.\\r\\n29:45\\r\\nCathal Flanagan: Okay, remember when we convert text\\r\\n29:52\\r\\nCathal Flanagan: to for a machine to understand. We convert it into a numerical representation of that text.\\r\\n29:58\\r\\nCathal Flanagan: So what happens is when we actually store these documents, we? We're gonna break them apart for storage, and we chunk them.\\r\\n30:05\\r\\nCathal Flanagan: We convert them to their numeric representation, using embeddings and an embeddings model. And you can choose which embeddings model you want to use.\\r\\n30:15\\r\\nCathal Flanagan: and that numeric representation of those different chunks of text they get stored.\\r\\n30:24\\r\\nCathal Flanagan: Okay, it's stored in a database which we're going to follow back to database July.\\r\\n30:33\\r\\nCathal Flanagan: And when you ask a question.\\r\\n30:38\\r\\nCathal Flanagan: the numeric representation of the embeddings of that question\\r\\n30:41\\r\\nCathal Flanagan: are mathematically compared to all of the different chunks\\r\\n30:46\\r\\nCathal Flanagan: of text that are within the database.\\r\\n30:51\\r\\nCathal Flanagan: Okay? And there's a similarity metric cosine. Similarity is the standard one that's used to do a similarity where it's finding, hey? What are the most similar numeric representations of my query?\\r\\n30:54\\r\\nCathal Flanagan: 2 pieces of text that I can find, and it will grab 5, 10 of those\\r\\n31:06\\r\\nCathal Flanagan: bring those back, and those are the chunks that are retrieved and passed into the model as context.\\r\\n31:12\\r\\nCathal Flanagan: Okay, so that's what's happening in the back end everything gets converted into into numerical representations, including the query itself.\\r\\n31:20\\r\\nCathal Flanagan: We do a similarity score between all of the chunks of text\\r\\n31:29\\r\\nCathal Flanagan: within the database, and we find\\r\\n31:35\\r\\nCathal Flanagan: 5, 1020, of the most relevant chunks. Grab them and pass them back.\\r\\n31:38\\r\\nCathal Flanagan: Let me pause there.\\r\\n31:44\\r\\nCathal Flanagan: scenario. You've had your hand up for a while. Do you have a question on on this? No prudish.\\r\\n31:48\\r\\nDarren Pitts: Question for you. The the information that is passed to the Llm. Is not persistent, so it does not remain. It's only used once for that\\r\\n31:55\\r\\nDarren Pitts: to to generate the response to that prompt.\\r\\n32:04\\r\\nCathal Flanagan: That's correct. But remember, when we break up text, we will want to store that. So it's persistent. So it's available\\r\\n32:07\\r\\nCathal Flanagan: last year.\\r\\n32:14\\r\\nCathal Flanagan: Okay? And that's that's where we're gonna invoke something called a vector, database. Tonight.\\r\\n32:16\\r\\nPranshu Tiwari: It's.\\r\\n32:20\\r\\nCathal Flanagan: I bet I should. Sorry. Demon's head will explode.\\r\\n32:21\\r\\nCathal Flanagan: We will invoke a vector, it's called a vector, store vector, database and a vector, store. Slightly different. Same concept\\r\\n32:24\\r\\nCathal Flanagan: for our purpose.\\r\\n32:33\\r\\nDarren Pitts: Okay.\\r\\n32:33\\r\\nPranshu Tiwari: Yeah.\\r\\n32:35\\r\\nPranshu Tiwari: so yeah, sorry. So I was just trying to understand this. So let's say, we are working in electrical industry.\\r\\n32:35\\r\\nPranshu Tiwari: And the word transformer is not the transformer which we use in this class, but is an electrical transformer.\\r\\n32:43\\r\\nPranshu Tiwari: So we'll pass on the analytical systems which they have in the knowledge system.\\r\\n32:51\\r\\nPranshu Tiwari: and it would try to find out the the transformer of the language of the question and transformer there. So if the if the similarity of the same word\\r\\n32:57\\r\\nPranshu Tiwari: is not there, then they would say, I don't know the answer, because the word is still the same.\\r\\n33:10\\r\\nPranshu Tiwari: But the relevant is different. So I'm not very clear with how they are doing similarity, because the word is still this transformer.\\r\\n33:16\\r\\nCathal Flanagan: That's a grace call out because\\r\\n33:24\\r\\nCathal Flanagan: it depends what embeddings model you use. Right? So we will be using a standard open and open and AI embeddings model by default. This week next week we'll look at some different options.\\r\\n33:29\\r\\nCathal Flanagan: and\\r\\n33:40\\r\\nCathal Flanagan: that's trained on the general understanding of the word transformer, right? Which, for your point, is probably more prevalent in electrical engineering than in, you know. Like other terminology, like the word transformer, as it relates to AI.\\r\\n33:42\\r\\nCathal Flanagan: So when it's doing a similarity. And you're asking, you know you're asking questions about it. It may just not be able to find their most relevant content for that. Which is why, you see.\\r\\n33:57\\r\\nCathal Flanagan: firms ourselves include my own firm included, who will train embeddings, models specifically for the domain in which they operate. And so I'll share this after the class. But\\r\\n34:11\\r\\nCathal Flanagan: if you type in green back bears\\r\\n34:25\\r\\nCathal Flanagan: just type in greenback bears and fis, and then it it should automatically show up and fiscal hawks. You can actually read my, the paper that my team published recently specifically on this topic of actually training and embeddings model specifically for finance and finance jargon, right? Rather than using the open AI Embeddings model specific because\\r\\n34:30\\r\\nCathal Flanagan: finance and like other domains, are filled with jargon, that when you're doing these retrieval systems look up. It is actually they can otherwise get confused.\\r\\n34:53\\r\\nCathal Flanagan: So it's a very good intuition that you have\\r\\n35:02\\r\\nCathal Flanagan: and you know, but it's it's what we're going to talk about, kind of how to overcome in this class, Dima.\\r\\n35:06\\r\\nDima Timofeev: We have a question in chat that I think we need to address for everybody is there\\r\\n35:14\\r\\nDima Timofeev: when we do informational retrieval?\\r\\n35:21\\r\\nDima Timofeev: Is it going to overwrite existing knowledge in the Llm.\\r\\n35:25\\r\\nCathal Flanagan: So it depends on how you think about that.\\r\\n35:31\\r\\nCathal Flanagan: 1st of all, the model is the model. The model that we're using will never change right in terms of the app. Like, if we're using the open AI model, that's the model that we're using for this task.\\r\\n35:35\\r\\nCathal Flanagan: we're never going to update them, the knowledge within us. Using this method, there's another method you could use called fine tuning, which would that we're not going to do that. And next week, or even later this evening, depending, if we have much time, will argue about the trade-offs there. Some people would argue, you would fine tune that. But using retrieval, augmented generation. No, you're not updating the knowledge within the model itself.\\r\\n35:46\\r\\nCathal Flanagan: Remember, we don't really want to use the models for their knowledge.\\r\\n36:10\\r\\nCathal Flanagan: Anyway, what we're doing is we are updating\\r\\n36:14\\r\\nCathal Flanagan: the information the model has to work with. Right? So it's\\r\\n36:20\\r\\nCathal Flanagan: only considering just the information that we're retrieving. And it's being limited to just that\\r\\n36:23\\r\\nCathal Flanagan: to answer whatever the question is that is being asked\\r\\n36:29\\r\\nCathal Flanagan: hopefully, that answers the question.\\r\\n36:35\\r\\nCathal Flanagan: Okay, so let's look at some problem statements here.\\r\\n36:38\\r\\nCathal Flanagan: If I said, what does Michael Mubasan's increasing returns? Research PC.\\r\\n36:43\\r\\nCathal Flanagan: To chat Gpt.\\r\\n36:51\\r\\nCathal Flanagan: it will disappoint me. It will tell me that it can't find that particular type of that piece of research. He's authored many different pieces, but it can't actually find that particular one.\\r\\n36:53\\r\\nCathal Flanagan: If I ask what are our company's travel expense policies.\\r\\n37:05\\r\\nCathal Flanagan: Well, it's not connected into workday. It doesn't know\\r\\n37:10\\r\\nCathal Flanagan: right? It's not within the knowledge base that it has.\\r\\n37:15\\r\\nCathal Flanagan: So therefore, you know, it's going to give you a disappointing result\\r\\n37:18\\r\\nCathal Flanagan: when we ask a question to chat Gbt, the standard kind of question response to both the Api, but also the chat Gbt system that we all use, whether it's Gemini or or\\r\\n37:24\\r\\nCathal Flanagan: chat Gpt, or any other system, is, we ask a question. We get a response directly from the model itself.\\r\\n37:36\\r\\nCathal Flanagan: Okay, we're what we do with rag\\r\\n37:42\\r\\nCathal Flanagan: is we introduced the concept that when you ask a question.\\r\\n37:46\\r\\nCathal Flanagan: we are going out 1st to a vector store\\r\\n37:52\\r\\nCathal Flanagan: which has a retrieval system built in that is basically storing that information and looking up the most relevant data that it can find within that system, assuming that there's relevant information to be found.\\r\\n37:55\\r\\nCathal Flanagan: And it's passing that along with the query that you have created\\r\\n38:11\\r\\nCathal Flanagan: to the model which is then passing back its response directly to you.\\r\\n38:17\\r\\nCathal Flanagan: Okay, so we're introducing a new element here\\r\\n38:21\\r\\nCathal Flanagan: to this entire workflow. And we are responsible for building this element. Okay, this vector store, the way that we want it.\\r\\n38:25\\r\\nCathal Flanagan: So really, it's about what we call indexing data. So taking all that data that exists elsewhere and putting it into a vector store in a way that we can. We are happy with, okay, that we can.\\r\\n38:35\\r\\nCathal Flanagan: we can query.\\r\\n38:50\\r\\nCathal Flanagan: And there are many different options here.\\r\\n38:52\\r\\nCathal Flanagan: Some have different trade-offs and costs than others.\\r\\n38:55\\r\\nCathal Flanagan: Some are open source. Some are closed source.\\r\\n38:58\\r\\nCathal Flanagan: Tonight, we're going to be using a vector store called chroma. dB, because it is open source, free.\\r\\n39:01\\r\\nCathal Flanagan: And you know, like we would say, lightweight, it's kind of easy to run on the machine that we're working within\\r\\n39:09\\r\\nCathal Flanagan: others. Other options are within the traditional kind of vector, store universe. Pinecone is very popular. It's very good from the enterprise perspective.\\r\\n39:16\\r\\nCathal Flanagan: It's also very expensive.\\r\\n39:26\\r\\nCathal Flanagan: And you need to deploy your journey in a cloud environment.\\r\\n39:28\\r\\nCathal Flanagan: So you know, there is a slight difference, of course, to what we're building today versus what you might use in the enterprise.\\r\\n39:32\\r\\nCathal Flanagan: It's not like people don't use chrome in the enterprise. They certainly do or any of the others. But it's just a different scale.\\r\\n39:38\\r\\nCathal Flanagan: right? But it's the same concept that we would be working on\\r\\n39:45\\r\\nCathal Flanagan: some of the other reasons that you might use. Different choices are.\\r\\n39:48\\r\\nCathal Flanagan: We're not just wanting to store the documents themselves, but oftentimes we want to store a lot of other rich metadata about the documents, because the 1st step of actually being able to filter on metadata\\r\\n39:52\\r\\nCathal Flanagan: is a great help in reducing the search space that we actually have to kind of do a a map. A search over so metadata filtering tends to be a very popular\\r\\n40:04\\r\\nCathal Flanagan: step as part of building these systems as well.\\r\\n40:16\\r\\nCathal Flanagan: The other concept that we're going to talk about\\r\\n40:21\\r\\nCathal Flanagan: or you're going to hear a lot about is something called chunking\\r\\n40:25\\r\\nCathal Flanagan: and chunking is basically how we decide how to break apart the text that we're working with.\\r\\n40:29\\r\\nCathal Flanagan: So remember tonight, we're going to work with Pdfs.\\r\\n40:36\\r\\nCathal Flanagan: we're gonna work with an Pdf, that's over a hundred pages long. But of course you might have\\r\\n40:40\\r\\nCathal Flanagan: hundreds of documents that are all hundreds of pages of long.\\r\\n40:44\\r\\nCathal Flanagan: How do you break those documents apart\\r\\n40:48\\r\\nCathal Flanagan: in a way that you are keeping\\r\\n40:52\\r\\nCathal Flanagan: as much kind of relevant content together.\\r\\n40:58\\r\\nCathal Flanagan: But still respecting, you know, things like context. Length windows. Again, as context lengths are length are becoming broader. This chunking debate and strategy is becoming less and less important. A year ago we would have been talking about, you know. Really, Chunky, being a major driver or or a\\r\\n41:02\\r\\nCathal Flanagan: or an optimization that people might be trying to make their retrieval systems better.\\r\\n41:22\\r\\nCathal Flanagan: It's becoming less important now, but still important. And it's still definitely an element of building these systems to be aware of.\\r\\n41:27\\r\\nCathal Flanagan: And then, of course, the concept of embedding I've already touched upon, which is.\\r\\n41:35\\r\\nCathal Flanagan: you know, the way that we actually do.\\r\\n41:40\\r\\nCathal Flanagan: The retrieval amongst in these actual vector stores is\\r\\n41:43\\r\\nCathal Flanagan: everything is getting converted into numerical representation of the text.\\r\\n41:48\\r\\nCathal Flanagan: And we're matching. We're doing a similarity lookup on the numerical representation of our query against the numerical representations of the chunks, the paragraphs of text or pages of text that we've embedded and finding the most relevant one.\\r\\n41:53\\r\\nCathal Flanagan: probably the most relevant. Well, you do one, but most people do at least 5, 1020.\\r\\n42:09\\r\\nCathal Flanagan: And so let's build a system\\r\\n42:16\\r\\nCathal Flanagan: initially using this document. This is a outlook from Goldman Sachs on 2025 economy.\\r\\n42:19\\r\\nCathal Flanagan: And it's 118 pages long. So let's use it as a kind of knowledge base on which to build a retrieval augmented generation system\\r\\n42:28\\r\\nCathal Flanagan: cool. Alright. So we're gonna break out and actually do some live coding.\\r\\n42:38\\r\\nCathal Flanagan: I am going to call this glass free\\r\\n42:42\\r\\nCathal Flanagan: in class. That was such a safe take.\\r\\n42:49\\r\\nCathal Flanagan: Maybe I shall share the code\\r\\n42:52\\r\\nCathal Flanagan: perfect, and then I'll put it in like. So I'm putting the\\r\\n43:16\\r\\nCathal Flanagan: the the code directly in the slack channel, and you're welcome to Code. Live with me you're welcome to follow along. You're welcome to just refresh the actual notebook and run it at different points of time, or you're welcome just to sit back and kind of think about it as a higher level. You know, as I mentioned, we're fully aware that there are some folks in the class\\r\\n43:23\\r\\nCathal Flanagan: who you know, who aren't engineers. They don't code perfectly fine. I want you to set back. I want you to have a high level concept of kind of what we're doing with these documents, how we're actually breaking them apart, what we're creating.\\r\\n43:40\\r\\nCathal Flanagan: As I mentioned, you don't actually need\\r\\n43:55\\r\\nCathal Flanagan: to fully understand all the different type parts of the code itself. Just kind of understand the flow of what's happening here.\\r\\n43:57\\r\\nCathal Flanagan: So let me just put this in our tech 16 lecture channel.\\r\\n44:04\\r\\nCathal Flanagan: Sorry my machine sounds like it's going to explode. So that's being a little bit slow today.\\r\\n44:16\\r\\nCathal Flanagan: Okay, I'm pasting it in there, and it is.\\r\\n44:25\\r\\nCathal Flanagan: Therefore I don't know how much use it.\\r\\n44:29\\r\\nCathal Flanagan: Alright. So let's start with something simple. We're just gonna do the standard imports.\\r\\n44:31\\r\\nCathal Flanagan: But what we're going to do this week is we're going to use a library called Llama Index. Okay, lama index is like Langchain. It's an open source framework\\r\\n44:38\\r\\nCathal Flanagan: that a lot of engineers and developers within the AI community use. And again, much like Langchain, it makes abstracts a lot of the complexity. It makes it quite easy to build these systems quickly, but it's very powerful. It makes it allows us to connect it to different systems, etc. There is a high degree of overlap between Langchain and Lama Index.\\r\\n44:48\\r\\nCathal Flanagan: They're, you know, they would be considered somewhat competitors, but they're also somewhat complementary. Linkedin will have connectors. That Llama index. Doesn't lama Index will be better. Retrieval augmented generation, for example.\\r\\n45:10\\r\\nCathal Flanagan: line chain might have more modules for building agents which we'll talk about later in the semester. So you know, again, it's about being aware of the different libraries within the ecosystem. There's not one to rule them all. It tends to be a combination. I will make the case that llama index when you're building retrieval, augmented generation systems is\\r\\n45:22\\r\\nCathal Flanagan: slightly better because it's designed to do that was kind of their their foundation. They're not trying to do everything they're trying to do that one thing quite well.\\r\\n45:42\\r\\nCathal Flanagan: So how do we actually install this? I'm going to say, Pip, install llama.\\r\\n45:50\\r\\nCathal Flanagan: underscore or dash index. Simple as that. That's how we get lama index available to us to use\\r\\n45:58\\r\\nCathal Flanagan: much like in previous weeks. We actually just need to make a version of the open AI model available to us. So we're going to do that\\r\\n46:06\\r\\nCathal Flanagan: by pacing in the code that you have seen previously many times. Which is the code to\\r\\n46:15\\r\\nCathal Flanagan: that's just installing the code to bring in our\\r\\n46:23\\r\\nCathal Flanagan: Api key from Openai into the system and then invoke it. So you've seen that code before? Pretty straightforward. We're just invoking the open AI client and key. So I'm just gonna grab that access.\\r\\n46:29\\r\\nCathal Flanagan: Okay, I'm going to be working with Pdfs here.\\r\\n46:47\\r\\nCathal Flanagan: Right? So I need to install a piece of software that can read Pdfs.\\r\\n46:53\\r\\nCathal Flanagan: the what very common one that we're going to use here is called pi. Pdf.\\r\\n46:59\\r\\nCathal Flanagan: I'm going to say, Pip.\\r\\n47:04\\r\\nCathal Flanagan: install 5 pdf, it must be listening to me because it auto completed that perfect though\\r\\n47:06\\r\\nCathal Flanagan: we're going to install the Pdf reader.\\r\\n47:12\\r\\nCathal Flanagan: okay? And then I want to grab the actual\\r\\n47:15\\r\\nCathal Flanagan: pdf, that we're going to be working with. So it's available on Goldman Sachs website. I'm going to grab it directly from there, using a magical command called Wget, which just goes out and grabs a file from the Internet.\\r\\n47:19\\r\\nCathal Flanagan: So here, I'm saying, W. Guess.\\r\\n47:32\\r\\nCathal Flanagan: give it the actual address, run it. And it's gonna go to that web page and grab that Pdf document for me. If I click on the files here on the left hand side, you'll see. Yes, indeed. I now have that Goldman Sachs, Pdf\\r\\n47:38\\r\\nCathal Flanagan: in the file system on the machine that I can now start building against.\\r\\n47:52\\r\\nCathal Flanagan: Okay.\\r\\n47:58\\r\\nCathal Flanagan: what else do I need to do again? We're just doing setup here. We're getting the files that we need, the systems that we need to start building.\\r\\n48:00\\r\\nCathal Flanagan: Next thing I need is I want to just put the open AI key into the environment into the ecosystem here. Because llama index.\\r\\n48:06\\r\\nCathal Flanagan: we'll look for that.\\r\\n48:15\\r\\nCathal Flanagan: Okay? So it looks at within the general environment for something very specific called the open AI key.\\r\\n48:17\\r\\nCathal Flanagan: So I'm just going to say, import OS, okay, I'm gonna say, OS dot\\r\\n48:24\\r\\nCathal Flanagan: environment open AI key equals the Openai key that I imported earlier. Okay, so all I'm doing\\r\\n48:32\\r\\nCathal Flanagan: just setting the opening. I key within the environment\\r\\n48:42\\r\\nCathal Flanagan: again, boilerplate, just getting things ready. The system will need that.\\r\\n48:45\\r\\nCathal Flanagan: Let's get into actually building this retrieval augmented generation system.\\r\\n48:50\\r\\nCathal Flanagan: So here, I'm going to start off by saying from Llama Index, next dot for import vector store\\r\\n48:56\\r\\nCathal Flanagan: index.\\r\\n49:18\\r\\nCathal Flanagan: Okay and simple directory reader, these are 2 things.\\r\\n49:20\\r\\nCathal Flanagan: Vector store index is where I'm going to be creating that in that index, that vector store.\\r\\n49:26\\r\\nCathal Flanagan: Okay.\\r\\n49:34\\r\\nCathal Flanagan: Simple Directory. Reader, I need to read from the Directory that we're in. I need to be able to read those files. Okay? So\\r\\n49:35\\r\\nCathal Flanagan: pretty intuitive. What they're doing right.\\r\\n49:41\\r\\nCathal Flanagan: So let's just run that code.\\r\\n49:45\\r\\nCathal Flanagan: And then I need to say, Okay, where are the documents? Okay, where are the documents that I'm going to load into the vector store. But I want to later. Query.\\r\\n49:54\\r\\nCathal Flanagan: okay, so that's let that finish.\\r\\n50:04\\r\\nCathal Flanagan: It's taking so long\\r\\n50:20\\r\\nCathal Flanagan: again.\\r\\n50:28\\r\\nCathal Flanagan: Okay, I finished.\\r\\n50:29\\r\\nCathal Flanagan: And so now, I'm just going to say\\r\\n50:31\\r\\nCathal Flanagan: documents\\r\\n50:39\\r\\nCathal Flanagan: equals simple directory. Reader.\\r\\n50:44\\r\\nCathal Flanagan: Here, I'm passing in\\r\\n50:50\\r\\nCathal Flanagan: this little dot forward slash. I'll explain that in a moment. Okay, load data.\\r\\n50:56\\r\\nCathal Flanagan: and then I can say, give it the name of the file.\\r\\n51:04\\r\\nCathal Flanagan: 2025. I just look at this file here.\\r\\n51:07\\r\\nCathal Flanagan: 2025 dash is G dash, outlook, dot. Pdf, if I run this.\\r\\n51:13\\r\\nCathal Flanagan: it's now going to load that file\\r\\n51:23\\r\\nCathal Flanagan: into the documents. Okay? And we could take a look at the actual documents themselves.\\r\\n51:26\\r\\nCathal Flanagan: So if I say documents, Nope.\\r\\n51:32\\r\\nDima Timofeev: Early. Could you please drop the link to the call app into slack.\\r\\n51:42\\r\\nCathal Flanagan: Oh, I\\r\\n51:48\\r\\nCathal Flanagan: you know I it didn't. I press. I put it in. I didn't press enter. I just pressed enter. It should be going. Now.\\r\\n51:49\\r\\nCathal Flanagan: what else is being there? We go. It's in the tech 16 lecture perfect. Alright. So\\r\\n51:57\\r\\nCathal Flanagan: let's have a look at the document. The document is\\r\\n52:06\\r\\nCathal Flanagan: the text from the Pdf. The raw text from the Pdf that we have. That we have\\r\\n52:11\\r\\nCathal Flanagan: brought in here.\\r\\n52:20\\r\\nCathal Flanagan: So\\r\\n52:21\\r\\nCathal Flanagan: you can see this is the\\r\\n52:26\\r\\nCathal Flanagan: some of the initial information within us.\\r\\n52:28\\r\\nCathal Flanagan: This is page 0, right? You can see the text, keep on tracking. That's literally the the front page.\\r\\n52:31\\r\\nCathal Flanagan: Want to look at. For example, page 10,\\r\\n52:39\\r\\nCathal Flanagan: I can see the content on page 10. Here, for example, right in the text.\\r\\n52:44\\r\\nCathal Flanagan: okay, so lots of other information. But I this is the key information, and actually has the text of the actual pages themselves.\\r\\n52:51\\r\\nCathal Flanagan: And so now I want to.\\r\\n52:59\\r\\nCathal Flanagan: I have the text power stats from the Pdf.\\r\\n53:02\\r\\nCathal Flanagan: and I want to now create the vector. Store that I'm going to be able to query against.\\r\\n53:07\\r\\nCathal Flanagan: And so going forward, we're now going to start using the word index. Okay? Because we're gonna start creating an index.\\r\\n53:13\\r\\nCathal Flanagan: And when we're we'll say we're creating an index. Or we're querying from the index. That's the terminology that we tend to use.\\r\\n53:19\\r\\nCathal Flanagan: Or I'm indexing a set of documents.\\r\\n53:26\\r\\nCathal Flanagan: Okay, index equals vector. Store index\\r\\n53:29\\r\\nCathal Flanagan: dot from documents. And I'm passing in the documents I want it to create. And this is the power of Linkedin or of Lama index. Excuse me, whereby I'm just passing in the documents that we've parsed. And it's going to create this index for us.\\r\\n53:35\\r\\nCathal Flanagan: Okay?\\r\\n53:50\\r\\nCathal Flanagan: And so it's create what's happening under the hood here is it's creating\\r\\n53:52\\r\\nCathal Flanagan: a vector representation for each of the documents. Okay? So the embeddings remember\\r\\n53:59\\r\\nCathal Flanagan: that we match in Class One.\\r\\n54:06\\r\\nCathal Flanagan: There's the way we go from\\r\\n54:08\\r\\nCathal Flanagan: a piece of a word to numerical representation of the word. So what's happening under the hood here is. It's for all those documents it's bringing them in.\\r\\n54:11\\r\\nCathal Flanagan: It's breaking them apart.\\r\\n54:21\\r\\nCathal Flanagan: It's converting them to numeric representation.\\r\\n54:24\\r\\nCathal Flanagan: using embeddings and storing that up and happen very, very quickly. Okay, and that all happens in one line of code\\r\\n54:28\\r\\nCathal Flanagan: right now.\\r\\n54:36\\r\\nPranshu Tiwari: Yeah, so sorry sorry to break you up. So the\\r\\n54:39\\r\\nPranshu Tiwari: the tokens associated with those words\\r\\n54:45\\r\\nPranshu Tiwari: will still be the same. So it's come back to the same question which we asked about, let's say about financial. Let's say, genie index. So maybe the meaning\\r\\n54:49\\r\\nPranshu Tiwari: I mean.\\r\\n55:00\\r\\nPranshu Tiwari: since there's no learning which is happening here. So I believe there's no learning happening here. It is just extracting the tokens.\\r\\n55:02\\r\\nDima Timofeev: It would forward the question into the tag. 16 lecture channel I just described in 6 steps. What is going on.\\r\\n55:11\\r\\nCathal Flanagan: Perfect.\\r\\n55:21\\r\\nCathal Flanagan: So Dima\\r\\n55:22\\r\\nCathal Flanagan: has already answered this question. If you scroll up in that channel, I can really see it. Excellent, David. Thank you.\\r\\n55:24\\r\\nCathal Flanagan: But to answer the question directly. There is no learning going on here. It's a purely of conversion of text to this numerical representation, which that they said numerical representation, which will be the basis of everything we do going forward in terms of the retrieval.\\r\\n55:34\\r\\nCathal Flanagan: Okay.\\r\\n55:51\\r\\nCathal Flanagan: the way that you convert that text is your choice of embeddings model by default under the hood. Here it's using the open AI Embeddings model. You have the ability to change that. Next week we're going to talk about going from did what we're describing this week. We call it naive rag. It's simple rag, right? Of course, it's where we should start.\\r\\n55:53\\r\\nCathal Flanagan: We are going to start adding complexity. Next week. We're going to start adding the ability to change embeddings models. We're gonna add the ability to do something called re-ranking, which is another element that you can add to these systems right now, let's make sure that we kind of get a good sense for how this simple introduction to retrieve augmented generation works.\\r\\n56:11\\r\\nCathal Flanagan: So I have to recap what I've done so far.\\r\\n56:32\\r\\nCathal Flanagan: I've loaded up a Pdf, I've parsed the Pdf.\\r\\n56:35\\r\\nCathal Flanagan: And I've created the index, which means I've created the numerical representation of those that text\\r\\n56:40\\r\\nCathal Flanagan: that we can then query, but I need to query it right? And to query something.\\r\\n56:47\\r\\nCathal Flanagan: I actually need to turn this\\r\\n56:52\\r\\nCathal Flanagan: index into what was called a query engine.\\r\\n56:55\\r\\nCathal Flanagan: Okay, I need to make it accessible.\\r\\n56:59\\r\\nCathal Flanagan: So the way I do that is, I say, query, underscore engine\\r\\n57:03\\r\\nCathal Flanagan: equals index.as query engine.\\r\\n57:09\\r\\nCathal Flanagan: Okay? Because I need to make this into a database type of system that is searchable.\\r\\n57:14\\r\\nCathal Flanagan: Okay?\\r\\n57:21\\r\\nCathal Flanagan: And when I actually then have this query engine, it allows me to ask questions against it.\\r\\n57:23\\r\\nCathal Flanagan: Okay? Because remember, what needs to happen is\\r\\n57:28\\r\\nCathal Flanagan: we're going to ask a question against the query engine.\\r\\n57:31\\r\\nCathal Flanagan: It is actually going to do the translation of the query that we're passing in into numerical representation, and then the actual matching\\r\\n57:35\\r\\nCathal Flanagan: okay? And then passing all that into the actual model. And again.\\r\\n57:44\\r\\nCathal Flanagan: for me, it still blows my mind that this, all happens within like 4 lines of code. Ultimately, that's the power of these systems. So if I say, response equals\\r\\n57:48\\r\\nCathal Flanagan: query, engine.\\r\\n58:00\\r\\nCathal Flanagan: okay? And I say, I ask a query. Let's say the outlook of 20 for 2025. Right? For let's say\\r\\n58:02\\r\\nCathal Flanagan: for us.\\r\\n58:10\\r\\nCathal Flanagan: Gdp, okay, remember, this document is about the economic outlook from Goldman Sachs. So when I run this.\\r\\n58:13\\r\\nCathal Flanagan: what's happening is it's take. It's converting that text into numeric representation. It's doing a a similarity lookup finding\\r\\n58:22\\r\\nCathal Flanagan: 5 or 10 pieces of text within that document that is retrieving.\\r\\n58:34\\r\\nCathal Flanagan: passing those into the Openai model.\\r\\n58:41\\r\\nCathal Flanagan: and then with our original question. And then, if I top print response.\\r\\n58:44\\r\\nCathal Flanagan: it tells me the outlook for 2025 for us, Gdp is expected to show growth with real Gdp expanded by 2.3%.\\r\\n58:59\\r\\nCathal Flanagan: Okay, so think about what we've just done.\\r\\n59:07\\r\\nCathal Flanagan: We have taken a knowledge base in this case, 118 page. Pdf, we've broken it apart.\\r\\n59:13\\r\\nCathal Flanagan: We've created the numeric representation of those pages.\\r\\n59:21\\r\\nCathal Flanagan: We've created a query engine.\\r\\n59:27\\r\\nCathal Flanagan: We've run a a question and gathered the relevant paragraph, pass it into open. AI gotten a response\\r\\n59:29\\r\\nCathal Flanagan: and actually answer that question in 1, 2, 3, 4, 5 lines of code, which is pretty cool.\\r\\n59:37\\r\\nRavi Shankar: Right.\\r\\n59:50\\r\\nCathal Flanagan: And then to answer a question that was actually asked in the chat.\\r\\n59:51\\r\\nCathal Flanagan: This is the power of this here.\\r\\n59:54\\r\\nCathal Flanagan: I'm loading information from one document, one pdf.\\r\\n59:57\\r\\nCathal Flanagan: but this is a simple directory. Reader.\\r\\n1:00:02\\r\\nCathal Flanagan: if I wanted, if I remove this and I had a directory that had 5 Pdfs. 10 Pdfs\\r\\n1:00:05\\r\\nCathal Flanagan: 100 Pdfs a thousand Pdfs.\\r\\n1:00:13\\r\\nCathal Flanagan: It will actually load up all of the Pdfs and index them so you could do all the files within a particular folder and make them searchable and accessible.\\r\\n1:00:16\\r\\nCathal Flanagan: To a to your retrieval, augmented generation\\r\\n1:00:27\\r\\nCathal Flanagan: system. So pretty pretty powerful stuff.\\r\\n1:00:31\\r\\nCathal Flanagan: Okay? I see we have a bunch of questions. It's 11 0. 3. Now, I want to give you guys a break, because that was kind of a lot. And I know there's going to be some questions to answer, and I understand some people want to have a bio break. So let's come back at 1150, sorry, 11, 8, 15, Pacific time 1115 Eastern time. So 10 min from now, and we'll stick around and answer any questions for those who have them.\\r\\n1:00:37\\r\\nSarit Arora: Yeah, I have one question I think you mentioned that this could also be scaled up when you have multiple documents. Right.\\r\\n1:01:04\\r\\nSarit Arora: So this works well, if there are only documents that you're searching in. But what if there are like multimodal set of things that we're looking into where one could be a document, the other could be a video. The 3rd could be a like a 3D vector file. And the question could be related to any of any of those kinds like, how how is this taken care in that context?\\r\\n1:01:14\\r\\nCathal Flanagan: It's not in this type of system it will only work with text, even if there is. Pdf, even if there is images and\\r\\n1:01:35\\r\\nCathal Flanagan: charts within a document, it will simply ignore them.\\r\\n1:01:44\\r\\nCathal Flanagan: Multimodal capabilities, for retrieval system are still quite difficult in the enterprise.\\r\\n1:01:49\\r\\nCathal Flanagan: Really the best you're able to do today at scale is actually\\r\\n1:01:58\\r\\nCathal Flanagan: is to convert them into images or convert images into text\\r\\n1:02:04\\r\\nCathal Flanagan: that you can then reason over, and you can possibly take those images and pass them into the as part of the context.\\r\\n1:02:11\\r\\nCathal Flanagan: That you're passing in. It depends on the type of model that you're using. So I would say, for most tasks today.\\r\\n1:02:20\\r\\nCathal Flanagan: multimodal in the enterprise, or like passing in other things other than text is pretty inaccessible.\\r\\n1:02:28\\r\\nCathal Flanagan: and even the way you would do it today, most people would just convert those into text itself.\\r\\n1:02:37\\r\\nCathal Flanagan: Now, there are some very specialized tools that do\\r\\n1:02:43\\r\\nCathal Flanagan: that do retrieval over video, for example. But that would not be this type of system.\\r\\n1:02:46\\r\\nSarit Arora: Got it? Yeah, thanks.\\r\\n1:02:54\\r\\nCathal Flanagan: Under the hood for a lot of those, even the ones that are doing it over video, a lot of them are converting from whatever the\\r\\n1:02:56\\r\\nCathal Flanagan: modality is into text. We're just discovering\\r\\n1:03:03\\r\\nCathal Flanagan: how to really leverage the multimodality capability of the newer models.\\r\\n1:03:07\\r\\nSarit Arora: Got it? Yeah, thank you.\\r\\n1:03:13\\r\\nCathal Flanagan: Andre.\\r\\n1:03:15\\r\\nAndrey Skripkin: Yeah, thanks. Just quick housekeeping guys in on your screen. You can see the hands, and it's a queue. So if you're on the left you go, and then next next next just just how zoom works. Question\\r\\n1:03:15\\r\\nAndrey Skripkin: so can we say that with the rag with rag we're kind of a training a mini mini model.\\r\\n1:03:31\\r\\nAndrey Skripkin: is it? Is it accurate statement?\\r\\n1:03:38\\r\\nCathal Flanagan: We can't. At no point is there any training occurring? We are using the model we're pass. We're being smart about how we're passing in context.\\r\\n1:03:41\\r\\nCathal Flanagan: But no, at no point is your training that's occurring in real time.\\r\\n1:03:50\\r\\nCathal Flanagan: There is the for training at different points. For example, I just mentioned at the beginning of class, how you might, theoretically, if you were more advanced, train your own embeddings model for specific domains or tasks. I would not imagine that most people would want to do that.\\r\\n1:03:54\\r\\nCathal Flanagan: The idea of fine tuning a model will come up later in the semester\\r\\n1:04:11\\r\\nCathal Flanagan: at the same time. That's not what's occurring here. And actually, I would. My argument will be, we can talk about a little bit later that most people\\r\\n1:04:16\\r\\nCathal Flanagan: we should not be doing that. Certainly the efficacy of it is far less than\\r\\n1:04:25\\r\\nCathal Flanagan: than building a better way. Better rag. It's hard enough to build a rag system so.\\r\\n1:04:31\\r\\nAndrey Skripkin: Thank you. And and the second question is.\\r\\n1:04:37\\r\\nAndrey Skripkin: if we already provide the the entire like data in Pdf, like everything that we need, why we need to ask Chatgpt anything or Openai anything. Why we need to send anything there. If if we have the Pdf. And we just need to ask, like, whatever is in Pdf.\\r\\n1:04:39\\r\\nCathal Flanagan: You mean? Why can't you just do a control net.\\r\\n1:04:56\\r\\nAndrey Skripkin: No, no, no, I mean like like it looks like this.\\r\\n1:04:59\\r\\nRavi Shankar: We are just using the models capability language capability to, you know, interact and give the answer in much more human form. At this point.\\r\\n1:05:04\\r\\nAndrey Skripkin: I see? Okay, yeah, yeah. The question is, basically, we don't need like, like, the the model, open AI model for like to to get this data. This data is located in Pdf, I mean.\\r\\n1:05:15\\r\\nCathal Flanagan: We're not using the model for its, for its ability to get the data. We're getting the data. That's our responsibility. We're using the model to do something with the data that we provide\\r\\n1:05:29\\r\\nCathal Flanagan: right? That could be, take this data and create a table. It could be answer a question. It could be summarized. It could be, you know, compare text.\\r\\n1:05:40\\r\\nCathal Flanagan: You know, we are using the models for the skills that they have not the knowledge that they have.\\r\\n1:05:50\\r\\nAndrey Skripkin: Okay. Okay.\\r\\n1:05:56\\r\\nCathal Flanagan: Responsible for giving it them knowledge.\\r\\n1:05:57\\r\\nAndrey Skripkin: Thanks.\\r\\n1:06:00\\r\\nCathal Flanagan: Sure Mahesh.\\r\\n1:06:01\\r\\nmahesh: Yeah, I have 2 questions. One is in in this example here. When you do a query engine, it doesn't look like we used any. Llm.\\r\\n1:06:03\\r\\nmahesh: Is that a\\r\\n1:06:13\\r\\nmahesh: is that so, you know. Isn't that sufficient? I think it's kind of somewhat related to the previous person's question. See here, we never used any. Llm, right? You just created a vector, store index. And you queried. And then we got an answer.\\r\\n1:06:15\\r\\nCathal Flanagan: It's kind of magic.\\r\\n1:06:30\\r\\nCathal Flanagan: And so it's the\\r\\n1:06:31\\r\\nCathal Flanagan: it's the fact. The power of the the what we're actually using is they understand that you're going to want to use an Llm. As part of this. So by default under the hood, they realize most people at this point are still using the open AI models. So once you but the once you have passed in somewhere in your environment an Api key that they can use, they take care of everything else, of calling the actual model itself. So all of that code that you and I wrote in Class One.\\r\\n1:06:33\\r\\nCathal Flanagan: They've already written for us, and they're already calling that right.\\r\\n1:07:03\\r\\nmahesh: I see.\\r\\n1:07:08\\r\\nCathal Flanagan: All of that away. It's what people ever would expect to. The only thing they need from us is a key.\\r\\n1:07:09\\r\\nmahesh: I see, I see. So including this query is basically, you know, probably they are internally calling an Llm. To query the screen.\\r\\n1:07:16\\r\\nCathal Flanagan: They have to.\\r\\n1:07:23\\r\\nmahesh: Yeah.\\r\\n1:07:24\\r\\nmahesh: Sorry. Second question on the multimodel thing. Right? I know we. We may not have a single model which can do a multimodel. What I'm curious to know. Is there a model which you know? Let's say, if I give a design of a home.\\r\\n1:07:25\\r\\nmahesh: and if I want to search for all the doors?\\r\\n1:07:41\\r\\nmahesh: Are there models which can do that.\\r\\n1:07:43\\r\\nCathal Flanagan: There are models that will say they'll do it. They just will do a bad job at the moment.\\r\\n1:07:46\\r\\nmahesh: I see.\\r\\n1:07:51\\r\\nCathal Flanagan: Cool. Cool, Dina, are you around.\\r\\n1:07:53\\r\\nDima Timofeev: Yes, I am.\\r\\n1:07:58\\r\\nCathal Flanagan: Cool. I have to take a bio break. Can you maybe take 2 questions and then I'll I'll come back.\\r\\n1:08:00\\r\\nDima Timofeev: Sure.\\r\\n1:08:05\\r\\nCathal Flanagan: Yankee.\\r\\n1:08:07\\r\\nYankai Su: It is. My question is, when we do like this query on the Pdf.\\r\\n1:08:10\\r\\nYankai Su: Is the answer only from the Pdf document, or it could also, from the large language, model.\\r\\n1:08:19\\r\\nDima Timofeev: So knowledge presumably will come from the document.\\r\\n1:08:28\\r\\nDima Timofeev: but it's arguable. A model can inject something. Imagine you have a large, long book.\\r\\n1:08:34\\r\\nDima Timofeev: and you have an idea or a question on your mind.\\r\\n1:08:42\\r\\nDima Timofeev: and you found 3 or 5 most relevant paragraphs to the question on your mind.\\r\\n1:08:45\\r\\nDima Timofeev: Now you take these questions, these paragraphs put it into the prompt, and you take your question and you ask your question with these 5 paragraphs in your context window. This is how it will work.\\r\\n1:08:51\\r\\nDima Timofeev: Does this make sense.\\r\\n1:09:09\\r\\nYankai Su: To.\\r\\n1:09:10\\r\\nYankai Su: So the answer only from like to the Pdf document is not from really the large language model.\\r\\n1:09:11\\r\\nDima Timofeev: We cannot guarantee it. It's not like given as a property. However, you can say you can add extra prompt like use. Only these chunks, or don't try to hallucinate. If the information is not provided with these 5 paragraphs of text. Say, I don't know. We can reinforce this behavior, but we can never reach like 100\\r\\n1:09:19\\r\\nDima Timofeev: guarantee.\\r\\n1:09:45\\r\\nCathal Flanagan: And you know, let me let me demonstrate that here, right like. So this is a document about\\r\\n1:09:49\\r\\nCathal Flanagan: about the economic outlook. So let's\\r\\n1:09:54\\r\\nCathal Flanagan: let's ask a question that clearly we don't expect to be in the context. So what is the recipe\\r\\n1:09:57\\r\\nCathal Flanagan: or apple bye?\\r\\n1:10:05\\r\\nCathal Flanagan: Right?\\r\\n1:10:10\\r\\nCathal Flanagan: I'm sorry I cannot provide an answer to the query as is unrelated to the context information provided.\\r\\n1:10:13\\r\\nCathal Flanagan: So if I ask Openai, what is the recipe for apple pie, do we think that it knows us.\\r\\n1:10:19\\r\\nYankai Su: Yes.\\r\\n1:10:27\\r\\nYankai Su: Okay.\\r\\n1:10:28\\r\\nCathal Flanagan: But the way that the system is set up it's on the back end. It's effectively being prompted to say.\\r\\n1:10:29\\r\\nCathal Flanagan: only use this information that is being provided. So even though the model knows the answer to this question, it's respecting the limits that are being placed upon it.\\r\\n1:10:35\\r\\nTim van Driel: Is it possible? Make sense, see the prompt that is submitted to the Llm.\\r\\n1:10:46\\r\\nCathal Flanagan: Is I will pull that out and show it to you later on.\\r\\n1:10:50\\r\\nYankai Su: Yeah, Doug, it's cool.\\r\\n1:10:56\\r\\nCathal Flanagan: Cool.\\r\\n1:11:00\\r\\nCathal Flanagan: Who else? Dylan.\\r\\n1:11:01\\r\\nDelin Shen: Yes, actually, it's it's quite fun. Because I was about to ask ask this question because I tried the exact same thing. I use it at them in paper, but and I asked how to make noodle, but somehow it was able to give me the instruction of how to make noodle like mix of dough and water. So and also when I I didn't do print print response, I just do response instead. So I can also see to answer that totally unrelated question. It's still quotes\\r\\n1:11:04\\r\\nDelin Shen: a paragraph from the document I provided.\\r\\n1:11:32\\r\\nDelin Shen: So just want to understand, how does that work.\\r\\n1:11:35\\r\\nCathal Flanagan: Why, why, that's a great question. Sorry I'm not sure why this is not\\r\\n1:11:38\\r\\nCathal Flanagan: So why is it doing that?\\r\\n1:11:43\\r\\nCathal Flanagan: The sorry just gonna put this back for us. Gdp, the\\r\\n1:11:45\\r\\nCathal Flanagan: the system is a query engine.\\r\\n1:11:58\\r\\nCathal Flanagan: right? It is doing a similarity match.\\r\\n1:12:04\\r\\nCathal Flanagan: It will always find similar things rise.\\r\\n1:12:07\\r\\nCathal Flanagan: no matter what you ask, it will find a SIM. Some things that are similar or not.\\r\\n1:12:12\\r\\nCathal Flanagan: That is\\r\\n1:12:19\\r\\nCathal Flanagan: the nature of kind of the lookup. Right? If you do, if you ask for how similar 2 piece of text are numerically, it will give you an answer.\\r\\n1:12:21\\r\\nCathal Flanagan: They\\r\\n1:12:30\\r\\nCathal Flanagan: key is in the prompting, as you're seeing. You can prompt it that effectively. It's it can make a decision as to whether or not that information will be relevant right? We just saw an example of that. You can definitely attack this with prompt hacking. You could definitely kind of have have it break some of those safety instructions.\\r\\n1:12:31\\r\\nCathal Flanagan: you know. But that's but yes, it's it's it's fully expected that it would retrieve\\r\\n1:12:52\\r\\nCathal Flanagan: relevant paragraphs or sorry retrieve paragraphs. And then but it is smart enough in as you've seen in a lot of cases to make a determination as to whether or not those paragraphs are relevant. So\\r\\n1:12:59\\r\\nCathal Flanagan: it's an interesting one. There's a whole debate around.\\r\\n1:13:13\\r\\nCathal Flanagan: even how it should be retrieving what parts you want, a diversity of of opinion. Should it be all the things that are most similar. You know this this retrieval is not, isn't. It's still very much a work in progress of how to build the best retrieval type of systems.\\r\\n1:13:16\\r\\nDelin Shen: So when we later, when we decide to actually use call lm, but it's still\\r\\n1:13:33\\r\\nDelin Shen: I mean, will this Lm still\\r\\n1:13:41\\r\\nDelin Shen: be used for the query stage or it would just extract whatever the original documents say, without any further processing.\\r\\n1:13:44\\r\\nCathal Flanagan: I'm not sure exactly what you mean. Sorry what was what you were saying? Something else.\\r\\n1:13:54\\r\\nDelin Shen: So right now, right now we didn't choose to use our end, but somehow it is using it in a subtle way. Right? So I was saying later, when we decide to use the rack information to\\r\\n1:13:58\\r\\nDelin Shen: to call lm, how will they receive the information? Would it just retrieve the raw text from documents, will, or it will, retrieve the raw text and then pass it to Lm. To process it 1st and then pass it to another. Lm.\\r\\n1:14:09\\r\\nCathal Flanagan: So this is using an Llm. This is using the open AI model under the hood.\\r\\n1:14:24\\r\\nCathal Flanagan: so I'm not exactly sure I I'm not sure if you're\\r\\n1:14:32\\r\\nCathal Flanagan: aware of that, or if you're confused about that.\\r\\n1:14:36\\r\\nDelin Shen: Yes, I'm aware of it's using Lm, now.\\r\\n1:14:40\\r\\nDelin Shen: but we didn't specify to use it right.\\r\\n1:14:44\\r\\nCathal Flanagan: Well, we we spent.\\r\\n1:14:47\\r\\nRavi Shankar: And give me key right.\\r\\n1:14:48\\r\\nCathal Flanagan: Going to use an Llm. Right? It's just a default right? Like to not use an Lm here like it just that's the entire point is that we're we are wanting. It's building prompts to pass into an Lm, so to you know, the opportunity of not using Llm. Wouldn't make a whole lot of sense.\\r\\n1:14:50\\r\\nJoe Seiwert III: Is it because you set the environment variable of the open AI key? Is that how it knows to use the open AI model.\\r\\n1:15:11\\r\\nCathal Flanagan: That's correct.\\r\\n1:15:20\\r\\nCathal Flanagan: By default, you can change that. You can pass in. We'll we're gonna update some of the settings right now and give you a sense of how you get more control over this different elements of it, including how you would. Actually, you can also then, switch to use Gemini anthropic, whatever model that you might wish.\\r\\n1:15:22\\r\\nDarren Pitts: Can you give a word or 2 about the military applications I'm thinking about? A cousin of mine who's in the\\r\\n1:15:39\\r\\nDarren Pitts: the spy business. And\\r\\n1:15:45\\r\\nDarren Pitts: is this information encrypted? How does a military or the Nsa. For that matter, say, would use\\r\\n1:15:48\\r\\nDarren Pitts: a rag per se from a security classification, perspective?\\r\\n1:15:56\\r\\nDarren Pitts: Or would they not.\\r\\n1:16:03\\r\\nCathal Flanagan: Honestly, that's an interesting question. I haven't I? I? The answer is, I don't know.\\r\\n1:16:06\\r\\nCathal Flanagan: put it like this.\\r\\n1:16:12\\r\\nCathal Flanagan: I'll abstract this a little bit more from intelligence, which is just permissioning right? Like.\\r\\n1:16:15\\r\\nCathal Flanagan: you can build these systems in a way that\\r\\n1:16:20\\r\\nCathal Flanagan: you know you're passing in the user identifier, and then only that user that only we will. The user will only be able to search over things that they actually have permission to search over right? So with the production grade scale systems, they come with authentication built in so\\r\\n1:16:23\\r\\nCathal Flanagan: you know. And then the way that the the\\r\\n1:16:42\\r\\nCathal Flanagan: think about what the Nsa. Or other folks might be doing. You know they're looking for needles in a haystack right? And allows this allows them another opportunity to to make connections to find out those hidden that you know, those in that information and large large corpuses of text. So I'm completely sure they're using it.\\r\\n1:16:45\\r\\nCathal Flanagan: But you know not a whole lot different than we are, I suspect. I know.\\r\\n1:17:07\\r\\nDima Timofeev: Like to. Add 2 more points, beauty of rack that you don't need to expose your entire data\\r\\n1:17:11\\r\\nDima Timofeev: to open AI or entropic, or whoever\\r\\n1:17:18\\r\\nDima Timofeev: so you can sort, and you can only give small pieces\\r\\n1:17:23\\r\\nDima Timofeev: outside. And there is. It's it's number one second. Eventually, if you're super secretive, you can move to your to hosting your own large language model, but it's usually hard enough and expensive enough.\\r\\n1:17:27\\r\\nDima Timofeev: And 3, rd there is like a full tier of startups who are doing. Imagine, let's take a very simple example. Medical records, medical records are highly protected, and when you send them to Lm, especially when you do mapreduce. And you try to analyze why, everybody is getting cough over the last 2 weeks\\r\\n1:17:38\\r\\nDima Timofeev: from 15 different doctors. You don't want to give names of specific patients, and there is a full business multiple startups. I know at least one in San Francisco which replaces names or any other pis. So private information\\r\\n1:17:59\\r\\nDima Timofeev: with virtual tokens which do not actually give the information, but at the same time they guarantee that it will not break, and will not make any quality degradation to your model. So they construct Pupkins in such a way that Lm. Can preserve those tokens with the same meaning, and then they just decode it back when they show results to doctors who are researchers who are doing aggregations.\\r\\n1:18:14\\r\\nDarren Pitts: Got it. Thank you. Appreciate that.\\r\\n1:18:40\\r\\nPranshu Tiwari: Yeah. So I had a question.\\r\\n1:18:42\\r\\nDarren Pitts: On the military side. I'll let you know. I'll ask my cousin.\\r\\n1:18:43\\r\\nDarren Pitts: who doesn't think too much about the AI thing, how they're using it.\\r\\n1:18:47\\r\\nPranshu Tiwari: Yeah. So I so my question was, I think, which is very similar.\\r\\n1:18:52\\r\\nCathal Flanagan: Okay, we're gonna pause the questions there, because I need to move on with the lecture. But we'll stick around after the lecture to answer any further questions, or you can ask questions in the live chat as well, and demon, and you will reply to them.\\r\\n1:18:57\\r\\nCathal Flanagan: cool cause I need to move on\\r\\n1:19:08\\r\\nCathal Flanagan: with that to get through the content for folks.\\r\\n1:19:10\\r\\nCathal Flanagan: Okay, so\\r\\n1:19:13\\r\\nCathal Flanagan: we, what have we done? So far? We've taken a knowledge base. We've broken it apart. We put it into a vector database that we are able to query against. Okay, we can ask many questions against that data. For example, we asked the question about what's the outlook for us? Gdp, maybe we can also ask.\\r\\n1:19:16\\r\\nCathal Flanagan: what's the outlook for?\\r\\n1:19:34\\r\\nCathal Flanagan: Let's say, developing markets.\\r\\n1:19:38\\r\\nCathal Flanagan: Okay, look at the response expects to face challenges. Several Latin American countries are experiencing heavy losses. Okay.\\r\\n1:19:45\\r\\nCathal Flanagan: there's a you know. It's giving us a decent answer here.\\r\\n1:19:57\\r\\nCathal Flanagan: If I ask the next question, and I say, Okay.\\r\\n1:20:01\\r\\nCathal Flanagan: what did I just ask you?\\r\\n1:20:05\\r\\nCathal Flanagan: You ask for an interpretation of the content, provided?\\r\\n1:20:09\\r\\nCathal Flanagan: No, that's not correct, if need be exact question.\\r\\n1:20:13\\r\\nCathal Flanagan: Wait. Now, it's saying, what should I just ask you?\\r\\n1:20:27\\r\\nCathal Flanagan: Okay. So if I run this again.\\r\\n1:20:29\\r\\nCathal Flanagan: question, what is the outlook for developing markets?\\r\\n1:20:33\\r\\nCathal Flanagan: Run it again.\\r\\n1:20:37\\r\\nCathal Flanagan: What I just ask you, right? See, it's basically\\r\\n1:20:41\\r\\nCathal Flanagan: it's actually all it's doing is giving us this feature response.\\r\\n1:20:45\\r\\nCathal Flanagan: The model doesn't have memory. Okay? And so when we're building these retrieval, augmented generation.\\r\\n1:20:49\\r\\nCathal Flanagan: often, we are actually building a chatbot.\\r\\n1:20:57\\r\\nCathal Flanagan: Okay, we're building a chatbot over a document, a corpus of documents, millions of documents, potentially okay. And so we need to build these in a way that the actual systems have memory that they can reason over. Okay, so it's not enough just to ask question. We need to actually give the system memory that it can actually use.\\r\\n1:21:00\\r\\nCathal Flanagan: So how do we actually add memory\\r\\n1:21:23\\r\\nCathal Flanagan: to the system that we have just built.\\r\\n1:21:25\\r\\nCathal Flanagan: Okay, well, this goes back to what we did\\r\\n1:21:28\\r\\nCathal Flanagan: when we were actually building the query engine.\\r\\n1:21:33\\r\\nCathal Flanagan: Okay, we built a query engine that is focused on\\r\\n1:21:37\\r\\nCathal Flanagan: being able to just index the information and actually ask a single question over it for some things that is correct\\r\\n1:21:43\\r\\nCathal Flanagan: plus most of the use cases we have, we want to have a multi\\r\\n1:21:51\\r\\nCathal Flanagan: turn conversation. So we need to use something slightly different within\\r\\n1:21:55\\r\\nCathal Flanagan: within Mama index. Specifically, we're going to create instead of a query engine. We're going to create a chat engine.\\r\\n1:22:00\\r\\nCathal Flanagan: Okay? And here I can say, chat underscore engine.\\r\\n1:22:07\\r\\nCathal Flanagan: And remember, I have created the index previously that has all of the documents. That's why this is kind of we do it in these steps. I have the index, and then I could do something with them.\\r\\n1:22:12\\r\\nCathal Flanagan: So here I have the index. And I'm saying as chat engine, okay, I can also say\\r\\n1:22:22\\r\\nCathal Flanagan: I can pass in the mode. Some different models have different ways of kind of\\r\\n1:22:29\\r\\nCathal Flanagan: interacting with the chat. I can explicitly say, for example, chat, mode, scroll mode equals open. AI.\\r\\n1:22:33\\r\\nCathal Flanagan: That's 1 particular type.\\r\\n1:22:45\\r\\nCathal Flanagan: Okay?\\r\\n1:22:47\\r\\nCathal Flanagan: And I'm going to say, here we can see the open AI\\r\\n1:22:49\\r\\nCathal Flanagan: Chat mode. It uses the open AI agent answer with a query, engine tool. There are other things react. Agent is something that we're going to talk about in a couple of weeks time. It's kind of an agentic framework.\\r\\n1:22:57\\r\\nCathal Flanagan: Simple chat contents. Question is another one right?\\r\\n1:23:09\\r\\nCathal Flanagan: Which condenses the question itself. Right? Use the retriever to get context.\\r\\n1:23:14\\r\\nCathal Flanagan: Let's just stick with open AI right now, because that's kind of what we're focused on today.\\r\\n1:23:20\\r\\nCathal Flanagan: Okay? And I'm then also going to say verbose oops purpose.\\r\\n1:23:24\\r\\nCathal Flanagan: Let's see, for both equals. True.\\r\\n1:23:35\\r\\nCathal Flanagan: So just for both means, I'm going to see more detail.\\r\\n1:23:40\\r\\nCathal Flanagan: Okay? And the output. And so if I run that and then I say, response equals chat, engine.\\r\\n1:23:43\\r\\nCathal Flanagan: dash, chass.\\r\\n1:23:59\\r\\nCathal Flanagan: Let's just say, Hello, start a conversation.\\r\\n1:24:01\\r\\nCathal Flanagan: Hello, there!\\r\\n1:24:04\\r\\nCathal Flanagan: French thoughts.\\r\\n1:24:11\\r\\nCathal Flanagan: Okay, the this is the verbose. This is what being verbose means. It's now giving me the entire show. Me what's happening.\\r\\n1:24:20\\r\\nCathal Flanagan: Okay? So the user added a message to memory, which was, Hello there. And the response from the model is, Hi, how may I assist you today?\\r\\n1:24:27\\r\\nCathal Flanagan: Okay, let's then add a another question that we're going to ask. I'll say, response equals chass engine.\\r\\n1:24:36\\r\\nCathal Flanagan: dark chats.\\r\\n1:24:54\\r\\nCathal Flanagan: What is the like?\\r\\n1:24:58\\r\\nCathal Flanagan: The words of a recession in 2025 French?\\r\\n1:25:01\\r\\nCathal Flanagan: And who's false\\r\\n1:25:16\\r\\nCathal Flanagan: in the art?\\r\\n1:25:36\\r\\nCathal Flanagan: Okay? So let's look what happened again. Remember, we're seeing the verbosity. We're seeing the output here because we turn on verbose equals. True.\\r\\n1:25:52\\r\\nCathal Flanagan: what happened? The user added to the memory. What is the likelihood of recessions? The memory now remembers what we actually asked.\\r\\n1:26:00\\r\\nCathal Flanagan: It's calling a function.\\r\\n1:26:08\\r\\nCathal Flanagan: Okay, it's calling the query engine tool with this output.\\r\\n1:26:11\\r\\nCathal Flanagan: and then the model it responded. The output we got was, likelihood of recession is is 20 in 2025, close to the long term average\\r\\n1:26:16\\r\\nCathal Flanagan: that is using the information right to actually give you the output.\\r\\n1:26:27\\r\\nCathal Flanagan: Okay, so\\r\\n1:26:33\\r\\nCathal Flanagan: this is what's going on under the hood. But this is the actual answer to the question. If we turn verbose equals false.\\r\\n1:26:35\\r\\nCathal Flanagan: then we wouldn't see any of the other stuff that we're seeing. But this gives us a sense of actually, what's happening underneath the hood?\\r\\n1:26:42\\r\\nCathal Flanagan: Okay?\\r\\n1:26:49\\r\\nCathal Flanagan: And then I can say, response equals chat engine got cheers plus. Did I just ask you.\\r\\n1:26:50\\r\\nDima Timofeev: I would like to take 30 seconds detour and address question which appeared in multiple contexts. But the question is approximately the same. People are asking about adding data and adding data sources. And I would probably postpone this question until the next lesson, because Charlie will show you chroma, which is vector, store. Right now, we store all the data in memory.\\r\\n1:27:15\\r\\nDima Timofeev: But next time we're gonna learn how we can store it on the persistent storage and how we can add more documents, more data, etcetera, and the second one.\\r\\n1:27:40\\r\\nCathal Flanagan: I get there in about 10 min. In fact.\\r\\n1:27:49\\r\\nDima Timofeev: Oh, nice! So in 10 min. And the second part of the question is, how can I bias my data and say, Please use the latest one, or give it more weight so Charlie will also talk about what is called ranking and re-ranking of data within the database. So you will also see how to bias towards specific examples.\\r\\n1:27:51\\r\\nCathal Flanagan: So yes, re-ranking will come next week in advanced rank. What I will say is, there's 2 different ways to think about. That re-ranking is a way of kind of like re-ranking the results for biases as Dima was outlining. But if you're actually thinking about, how do I at scale think about the more recent documents to consider, for example. Well, that would be metadata right? That would be kind of the creation date of a document itself\\r\\n1:28:13\\r\\nCathal Flanagan: that we would be passing it along with.\\r\\n1:28:40\\r\\nCathal Flanagan: the information so different vector databases support different levels of metadata and metadata filtering.\\r\\n1:28:43\\r\\nCathal Flanagan: So it's very important that you would be building these in production to give that push.\\r\\n1:28:49\\r\\nCathal Flanagan: To what type of metadata filtering will you require now, or in the future, and choosing your vector, database. Or perhaps you don't even want to use a vector, database\\r\\n1:28:54\\r\\nCathal Flanagan: right? Vector, databases were very much the topic du jour about a year ago.\\r\\n1:29:04\\r\\nCathal Flanagan: I don't hear people using them as much these days, because I think a lot of folks have run into the realization that they are not sufficient for a lot of enterprise use cases because\\r\\n1:29:10\\r\\nCathal Flanagan: they are very good at doing exactly what we're describing here today, which is building chat bot type systems.\\r\\n1:29:22\\r\\nCathal Flanagan: But in an enterprise when you have large corpuses of documents.\\r\\n1:29:28\\r\\nCathal Flanagan: Almost certainly you're going to. You want to use that\\r\\n1:29:31\\r\\nCathal Flanagan: same back end that you're building here to power front end to kind of browse the documents to link out to it. Have a document viewer, for example.\\r\\n1:29:36\\r\\nCathal Flanagan: So\\r\\n1:29:45\\r\\nCathal Flanagan: what I'm seeing a loss in terms of enterprise adoption is they started. People start with vector databases and are now transitioning to hybrid systems things like elastic search, open search, that type of thing. So\\r\\n1:29:46\\r\\nCathal Flanagan: these are. If those don't mean anything to you, that's that's fine. It just means we're not going to cover them in this class because they're kind of more on the technical data engineering side. Vector. Stores are more than enough for what we would require. But that's definitely a trend. I'm seeing you know a lot of folks moving to hybrid systems that can power front ends plus also chat box.\\r\\n1:30:01\\r\\nCathal Flanagan: What's the trade off? I would talk about it afterwards, or on further sessions around how to think about kind of the trade-offs are available there.\\r\\n1:30:24\\r\\nCathal Flanagan: Awesome.\\r\\n1:30:33\\r\\nCathal Flanagan: So let's add some settings here at the moment. You have now got the ability to build a\\r\\n1:30:34\\r\\nCathal Flanagan: functioning retrieval, augmented generation Chatbot, over your data in less than 10 lines of code.\\r\\n1:30:42\\r\\nCathal Flanagan: But of course, inevitably, there will be things you want to change. There'll be options that you want to to expand upon. For example, people talked about the ability to change different models.\\r\\n1:30:49\\r\\nCathal Flanagan: other things that people inevitably when they're talking about this space, they're gonna want to do things like chunking. People think care a lot about chunking as a concept. It's becoming\\r\\n1:31:01\\r\\nCathal Flanagan: less important as context windows go up. But you know, people care a lot.\\r\\n1:31:13\\r\\nCathal Flanagan: So what I want to show you now is actually how to start changing some of the settings.\\r\\n1:31:18\\r\\nCathal Flanagan: Okay, we click on text when I say, changing settings\\r\\n1:31:24\\r\\nCathal Flanagan: specifically, I'm going to talk to you today about chunking.\\r\\n1:31:34\\r\\nCathal Flanagan: So by default, when we're breaking these documents apart, these\\r\\n1:31:42\\r\\nCathal Flanagan: we are chunking them into about a thousand\\r\\n1:31:47\\r\\nCathal Flanagan: characters. That's kind of the default for llama index.\\r\\n1:31:52\\r\\nCathal Flanagan: Maybe you want it to be larger. Maybe you wanted to. Just kind of you're finding that the amount of information that's retrieving is too small. I have a large context window. I'm pretty happy to kind of use more of the context window and retrieve a lot more of those documents.\\r\\n1:31:56\\r\\nCathal Flanagan: So I want to change the default chunk size that's available within lama index.\\r\\n1:32:11\\r\\nCathal Flanagan: How do I do this from Llama underscore index dos for import settings, settings dos.\\r\\n1:32:18\\r\\nCathal Flanagan: Chunk size. Okay? By default, it's 1024. Let's say we're gonna chunk everything. And we're gonna do 10,000\\r\\n1:32:43\\r\\nCathal Flanagan: characters, right settings.\\r\\n1:32:50\\r\\nCathal Flanagan: Das Chunk overlap. Maybe I want that to be 200 characters. Now, what is a chunk overlap?\\r\\n1:32:54\\r\\nCathal Flanagan: Let's see if I can find the example here. Chunking visualization.\\r\\n1:33:03\\r\\nCathal Flanagan: Create one here. If you want to look it up from hugging face.\\r\\n1:33:23\\r\\nCathal Flanagan: Okay, when given a piece of text, right? You can effectively\\r\\n1:33:36\\r\\nCathal Flanagan: choose.\\r\\n1:33:46\\r\\nCathal Flanagan: You can effectively choose how you want to chunk it up.\\r\\n1:33:51\\r\\nCathal Flanagan: Right? So, for example, the longer the chunk chunk length, for example, here\\r\\n1:33:54\\r\\nCathal Flanagan: you can see in the colors here how much of the text stays together. So they in the smallest way, you would basically chunk by each individual word right, which means every\\r\\n1:34:03\\r\\nCathal Flanagan: which is that she has a minimum of 50 characters, which is why you're seeing that dash.\\r\\n1:34:15\\r\\nCathal Flanagan: If you go up to 500 here, you basically seeing entire paragraphs together.\\r\\n1:34:19\\r\\nCathal Flanagan: Being chart right? So if we go to, let's say\\r\\n1:34:26\\r\\nCathal Flanagan: the default for Llama Index, if we can make it like this is 1, 20,024, it's going to give an error. Okay? 500, I guess, is the Max here? Let's go back to 500,\\r\\n1:34:30\\r\\nCathal Flanagan: and then the overlap is effectively how much of the text is overlapping?\\r\\n1:34:45\\r\\nCathal Flanagan: In terms of the chunk. So should you include, not just\\r\\n1:34:54\\r\\nCathal Flanagan: should you just break the text apart. But should you purposely have some overlap between them.\\r\\n1:34:59\\r\\nCathal Flanagan: Why would you do that? Because sometimes it's helpful that you want the same text to be\\r\\n1:35:04\\r\\nCathal Flanagan: if you've got, you know.\\r\\n1:35:12\\r\\nCathal Flanagan: half a paragraph that's chunked, and the other half you kind of want them to largely be returned together. So it's kind of a\\r\\n1:35:14\\r\\nCathal Flanagan: again. A lot of people spend a lot of time thinking about chunking strategies right here. If I went to chunk overlap.\\r\\n1:35:22\\r\\nCathal Flanagan: So if you're reading about this. This is with 0 overlap. Look at the word, perhaps, and eventually for.\\r\\n1:35:29\\r\\nCathal Flanagan: And then if I see a certain amount of overlap.\\r\\n1:35:36\\r\\nCathal Flanagan: Okay, then you're gonna see\\r\\n1:35:40\\r\\nCathal Flanagan: there. Well, you can't really see it very well, but there is an overlap.\\r\\n1:35:43\\r\\nCathal Flanagan: That is occurring.\\r\\n1:35:47\\r\\nCathal Flanagan: So how do you actually set that\\r\\n1:35:49\\r\\nCathal Flanagan: chunk overlap. Set a number here, set a number of characters.\\r\\n1:35:55\\r\\nCathal Flanagan: My advice here is, if you're reading the the literature they're going to over index on chunking strategies.\\r\\n1:35:59\\r\\nCathal Flanagan: That was because of the context limitations we had. I would be overly generous at this point. Rather than you know, being too stingy. I certainly would increase it from a thousand to maybe a couple of 1,000 characters.\\r\\n1:36:08\\r\\nCathal Flanagan: When you do that, and you know you change the chunking strategy. I'm just simply just going to\\r\\n1:36:24\\r\\nCathal Flanagan: bring in the code we saw earlier, which was the\\r\\n1:36:32\\r\\nCathal Flanagan: how to, you know, creating a vector index and a query engine, just to kind of give you a sense. But it's exactly the same codes that we saw earlier.\\r\\n1:36:35\\r\\nCathal Flanagan: Okay, outlook for Gdp, like creating a vector, stores exactly the same code that we walked through earlier. The difference here is, we're just simply allowing the model to retrieve more text.\\r\\n1:36:48\\r\\nCathal Flanagan: right? More text and more opportunity. It has to potentially give you a a richer answer. We still doing a good job earlier, because, of course, it is\\r\\n1:37:01\\r\\nCathal Flanagan: quite clear it's, you know, looking for outlook for us. Gdp, but perhaps you're asking questions which are a little bit more ambiguous.\\r\\n1:37:10\\r\\nCathal Flanagan: The larger the chunks that are, returned, the larger the likelihood that the model will have what it needs to actually answer the question.\\r\\n1:37:17\\r\\nCathal Flanagan: Okay, I see that. Come back\\r\\n1:37:26\\r\\nCathal Flanagan: giving your response. Let's see.\\r\\n1:37:39\\r\\nCathal Flanagan: Trying to think this\\r\\n1:37:42\\r\\nCathal Flanagan: response.\\r\\n1:37:49\\r\\nCathal Flanagan: it is being slow tonight.\\r\\n1:38:05\\r\\nCathal Flanagan: Okay, well, this is this is going to be a little underwhelming, because it's just going to give the same responses earlier, largely because this is a pretty standard question. But when it does come back it will show us\\r\\n1:38:15\\r\\nCathal Flanagan: you know you can play with this change the chunking strategy. See the different effects? Certainly. Put the chunks down to a small number and try it, and you will see that the results get pretty bad pretty quickly, and you can work up from there.\\r\\n1:38:26\\r\\nCathal Flanagan: Okay, I'm not sure why, that is being as slow as it is, but we will\\r\\n1:38:41\\r\\nCathal Flanagan: move on in the interest of time. I'm going to stop this, and then\\r\\n1:38:49\\r\\nCathal Flanagan: rerun it all. Just so. We're kind of getting a fresh session. I think it's because I have a multiple collabs that are open.\\r\\n1:38:54\\r\\nCathal Flanagan: Okay, while this is reloading? Let me pause for any questions, Teama. Do we have any questions in the chat that we should be addressing.\\r\\n1:39:02\\r\\nDima Timofeev: No, we are good for now.\\r\\n1:39:12\\r\\nCathal Flanagan: Good for now. Okay. Ravi, do you want to jump in.\\r\\n1:39:15\\r\\nRavi Shankar: Yeah, sure. So\\r\\n1:39:18\\r\\nRavi Shankar: right now, we are doing this rag on the Pdf document. But if we need to do it on a combination of a Pdf. And then there is some data stored in a relational database in a table.\\r\\n1:39:20\\r\\nRavi Shankar: How we, you know, feed that to that for embedding.\\r\\n1:39:32\\r\\nCathal Flanagan: So.\\r\\n1:39:38\\r\\nRavi Shankar: Answer questions. I just don't want to look at the Pdf. But I want to look at the data which is in the table. Say, benefits, information or payment histories.\\r\\n1:39:39\\r\\nCathal Flanagan: So that's still a very much a\\r\\n1:39:49\\r\\nCathal Flanagan: an active area of development. It requires having a good schema for the actual table itself\\r\\n1:39:52\\r\\nCathal Flanagan: snowflake, or furthest ahead. Here they have something called the fork, snowflake cortex analyst, which is\\r\\n1:40:01\\r\\nCathal Flanagan: allowing you to pass off natural language questions to them. It actually is still a lot of work on the Snowflake engineering side, where you still have to manually create the schema\\r\\n1:40:08\\r\\nCathal Flanagan: for the actual table itself. So\\r\\n1:40:21\\r\\nCathal Flanagan: it is not something that's just kind of one import here, and it's made accessible. There is ability within lama index to do exactly what you're asking. But I would just suggest to you that if you're trying it out you're going to be pretty underwhelmed so definitely. And you know, for homework, the homework is actually going to be to choose another data connection. Right? So I'll show you here a sense set of them.\\r\\n1:40:25\\r\\nCathal Flanagan: So here you can see the set of all of the readers that llama index supports.\\r\\n1:40:59\\r\\nCathal Flanagan: So\\r\\n1:41:09\\r\\nCathal Flanagan: a lot of these being difficult.\\r\\n1:41:18\\r\\nCathal Flanagan: so here, you see.\\r\\n1:41:26\\r\\nCathal Flanagan: you'll see all the different systems. And your homework is that you're gonna need to take any of these systems as you apply. So if you want to try it on on a database that you have in mind. You certainly can. You can see all different integrations\\r\\n1:41:28\\r\\nCathal Flanagan: here.\\r\\n1:41:39\\r\\nCathal Flanagan: Okay, so you see things like\\r\\n1:41:41\\r\\nCathal Flanagan: Rss feeders. You see things like Wikipedia Youtube. Slack reddish sec filings, telegram, whatsapp\\r\\n1:41:44\\r\\nCathal Flanagan: spotify? I'm not. Oh, here we go. Snowflake. Reader right again, if the connection exists.\\r\\n1:41:57\\r\\nCathal Flanagan: but without a really good schema, the model is not going to know how to actually form the query. Well, so that could be a it's a good takeaway at the moment. The the way that we do it is we do both. We will actually have\\r\\n1:42:06\\r\\nCathal Flanagan: a system that will query the unstructured table data. The actual text itself will also have a query that will that will do a that will do. Query, structure, data, return a response, and then we will combine them somewhere downstream. It isn't kind of one\\r\\n1:42:20\\r\\nCathal Flanagan: one thing you can call from non index and do both together particularly well. And again, as I mentioned.\\r\\n1:42:37\\r\\nCathal Flanagan: structured data is a real force, real point of failure for most of these systems. Still today, even though Snowflake and others will advertise that they have us solve problem. I can promise you. They certainly do not.\\r\\n1:42:42\\r\\nRavi Shankar: Okay.\\r\\n1:42:56\\r\\nCathal Flanagan: Cool. Let's get all these questions afterwards. But I'll take them in order. Here, Joe.\\r\\n1:42:58\\r\\nJoe Seiwert III: So can you? Review real quick the chunking concept and and how it changes the responses that you get. And then the other.\\r\\n1:43:06\\r\\nJoe Seiwert III: The other thing was.\\r\\n1:43:17\\r\\nJoe Seiwert III: is there a similar idea in like a Rdb like a relational database to chunking? Or are you better just exporting everything to a flat text file and and loading that.\\r\\n1:43:19\\r\\nCathal Flanagan: And awesome.\\r\\n1:43:33\\r\\nDima Timofeev: Don't mind, Charlie, I will load this question a bit more, and we'll also ask you to address what is the upsides and downsides, of getting chunks larger or smaller.\\r\\n1:43:34\\r\\nCathal Flanagan: Okay, well, let's just take. Let's take an extreme example.\\r\\n1:43:48\\r\\nCathal Flanagan: If we have a chunk size of one.\\r\\n1:43:52\\r\\nCathal Flanagan: or let's say they have a chunk size of let's call it 10.\\r\\n1:43:58\\r\\nCathal Flanagan: We are effectively. When you ask a question\\r\\n1:44:03\\r\\nCathal Flanagan: like us. Gdp, we are going to retrieve\\r\\n1:44:07\\r\\nCathal Flanagan: one word, maybe 2 words, Max.\\r\\n1:44:10\\r\\nCathal Flanagan: okay from a document. Is that enough to give for the model to answer the question of what the Us. Gdp is right, like we're passing in before the text from the document. So if I asked you, hey, what is Goldman Sachs saying about us? Gdp and and I pass back the word\\r\\n1:44:14\\r\\nCathal Flanagan: Gdp, is that enough for you to understand the answer?\\r\\n1:44:32\\r\\nCathal Flanagan: Now, if I pass back the words, Us. Gdp is expected.\\r\\n1:44:39\\r\\nCathal Flanagan: That's 4 or 5 words. Is that enough to answer the question.\\r\\n1:44:44\\r\\nCathal Flanagan: No, Aj, I can see your. I can see you responding here. I appreciate that.\\r\\n1:44:50\\r\\nCathal Flanagan: So that's that's the way to think about. This is\\r\\n1:44:56\\r\\nCathal Flanagan: technically that those are chunks. Those are small chunks, but those are chunks.\\r\\n1:44:59\\r\\nCathal Flanagan: Okay, if I don't make the chunk size large enough.\\r\\n1:45:03\\r\\nCathal Flanagan: the model will not get enough information that is required. It's technically correct.\\r\\n1:45:09\\r\\nCathal Flanagan: It will find if I say us Gdp.\\r\\n1:45:14\\r\\nCathal Flanagan: and it finds within the Goldman Sachs piece of research. The chunk that says us Gdp is.\\r\\n1:45:18\\r\\nCathal Flanagan: that's a very that's it's done its job. It's done exactly what we want. It's found the most relevant chunk of information\\r\\n1:45:27\\r\\nCathal Flanagan: within the document that will. That is closest. So I'm trying to answer. Remember, it's just trying to find relevant text within the document. Right? So\\r\\n1:45:34\\r\\nCathal Flanagan: it is doing its job from a retrieval perspective.\\r\\n1:45:46\\r\\nCathal Flanagan: So the question is, then how should we set this chunking like, what? Okay? If we know that one word is enough. If we know that 5 words is likely not to be enough.\\r\\n1:45:50\\r\\nCathal Flanagan: is 10 words. Enough?\\r\\n1:45:59\\r\\nCathal Flanagan: Is a hundred is a thousand like? Where? What is the optimal? Right? And there's obviously a trade off.\\r\\n1:46:01\\r\\nCathal Flanagan: Right previously it was, we were very limited\\r\\n1:46:06\\r\\nCathal Flanagan: on how much we could return.\\r\\n1:46:10\\r\\nCathal Flanagan: So we have to be careful, much like memory usage.\\r\\n1:46:13\\r\\nCathal Flanagan: as I mentioned, that's become less and less of a concern as the as the context lengths have expanded.\\r\\n1:46:18\\r\\nJoe Seiwert III: So\\r\\n1:46:25\\r\\nJoe Seiwert III: sorry, Charlie, so it sounds like we would do like default. We should just put everything into one chunk\\r\\n1:46:27\\r\\nJoe Seiwert III: to start with, and if that blows through the context window, then back off.\\r\\n1:46:34\\r\\nCathal Flanagan: Oh, that's very interesting. And it's actually a paper club topic that we should discuss, because there's a lot of, because this is changing quickly. And actually, a lot of folks\\r\\n1:46:39\\r\\nCathal Flanagan: are suggesting exactly that.\\r\\n1:46:48\\r\\nCathal Flanagan: That not exactly what you're describing, because effectively, what you're describing is waiting for an error to occur right, but rather\\r\\n1:46:51\\r\\nCathal Flanagan: using the model to understand\\r\\n1:47:02\\r\\nCathal Flanagan: 1st how we should be routing this. If we think this will be a question that we best answered with a large context or with a retrieval system.\\r\\n1:47:06\\r\\nCathal Flanagan: it's still an area of much like fine tuning versus rag. It's an area of active discussion, active like it's art rather than science. People are trying different things and writing papers about them.\\r\\n1:47:16\\r\\nCathal Flanagan: At the same time we would do assume that you will probably always need rag to some degree, because.\\r\\n1:47:28\\r\\nCathal Flanagan: you know, in the enterprise you're quickly going to get to millions of documents right? And so you still have. Even if you think about going from within a document or finding the most relevant paragraph, you still need to find the most relevant document right.\\r\\n1:47:35\\r\\nCathal Flanagan: and the retrieval systems are not perfect. So therefore we're not just retrieving one chunk.\\r\\n1:47:50\\r\\nCathal Flanagan: We're often and most likely retrieving\\r\\n1:47:58\\r\\nCathal Flanagan: 5, 1020 of the closest chunks\\r\\n1:48:00\\r\\nCathal Flanagan: and giving them. We're kind of giving ourselves a best chance, right? So you know, no way. No system is ever returning just the number one chunk. All systems are returning. Kind of the top. 5. The top 10 chunks.\\r\\n1:48:07\\r\\nCathal Flanagan: Okay.\\r\\n1:48:22\\r\\nJoe Seiwert III: Yeah, thanks. That helps. Yeah.\\r\\n1:48:26\\r\\nCathal Flanagan: Perfect. Okay, this was the code I showed for updating the chunks. This is literally how you do it here. And I encourage you. Try these, play with them. And, you know, go to the extreme on the low side and also on the upside, and kind of see?\\r\\n1:48:29\\r\\nCathal Flanagan: Maybe where you think the the sweet spot is. I think the sweet spot has changed from when these were set up as defaults. Quite frankly, because I'm definitely in favor of larger chunks. Given compared to the constraints we were working within even a few months ago.\\r\\n1:48:43\\r\\nCathal Flanagan: Okay, awesome. I want to show you next how to set up a vector database. So what we were showing you is how to do everything in memory currently. But it's almost certainly that you'll want to actually create a database that persists that lives beyond just this kind of session that we're working within. You're creating a vector store, a vector database that is, other people can call upon within your organization. So to do that\\r\\n1:49:01\\r\\nCathal Flanagan: today, we're going to use one called Chromadb, because it's nice and easy to set up. It's still a SQL. Like database. So it has all the properties, but it's quite a lightweight, and it's open source and free, which is why we like to use it.\\r\\n1:49:29\\r\\nCathal Flanagan: So I'm just going to say creation vector dB,\\r\\n1:49:42\\r\\nCathal Flanagan: and then team is going to explain to us why I'm technically, not a vector database.\\r\\n1:49:49\\r\\nCathal Flanagan: Because people get very upset about this sort of thing.\\r\\n1:49:54\\r\\nCathal Flanagan: Okay, import chroma.\\r\\n1:49:59\\r\\nCathal Flanagan: Bb.\\r\\n1:50:02\\r\\nCathal Flanagan: sorry. I'm not sorry. Sorry. Sorry. I'm not exploding. He wouldn't be just exploding if I had previously used what I was using. Something called fast. Sorry, Teama, I think, should be okay with this.\\r\\n1:50:05\\r\\nCathal Flanagan: All right. Import chromadb. I actually need to install it. That's pretty important. How do I install it?\\r\\n1:50:17\\r\\nCathal Flanagan: I 1st have to pip, install it like many other things.\\r\\n1:50:24\\r\\nCathal Flanagan: So I'm going to add the code here\\r\\n1:50:28\\r\\nCathal Flanagan: and say, Pip, install llama index vector doors from\\r\\n1:50:34\\r\\nCathal Flanagan: okay. And I'm gonna say, import Roma.\\r\\n1:50:54\\r\\nCathal Flanagan: Maybe while that's happening. And I'm gonna delete these. So we don't get an error perfect.\\r\\n1:50:58\\r\\nCathal Flanagan: Okay, I'm also going to import a couple of other libraries that I require.\\r\\n1:51:06\\r\\nCathal Flanagan: Just while we're waiting for this to install.\\r\\n1:51:10\\r\\nCathal Flanagan: I'm going to install the actual promo, the vector store itself.\\r\\n1:51:13\\r\\nCathal Flanagan: So that's just installing.\\r\\n1:51:19\\r\\nCathal Flanagan: I'm going to say from.\\r\\n1:51:22\\r\\nCathal Flanagan: Oh, man, okay, I think that should still work, though. Okay, from llama\\r\\n1:51:27\\r\\nCathal Flanagan: index import vector, or chroma vector, store. I'm actually gonna get rid of last, I already imported. So this is all I'm doing. Vector stores, import chroma vector store. Okay?\\r\\n1:51:37\\r\\nCathal Flanagan: And I'm going to import something. I require call storage context. So from llama index import storage context.\\r\\n1:51:48\\r\\nCathal Flanagan: oh, it's in the core. Hold on a sec, gosh or\\r\\n1:52:06\\r\\nCathal Flanagan: phone app vector one.\\r\\n1:52:17\\r\\nCathal Flanagan: This needs to have a chroma.\\r\\n1:52:27\\r\\nCathal Flanagan: Okay, now, I need to.\\r\\n1:52:35\\r\\nCathal Flanagan: Now I've got everything installed or imported. Now, I need to actually invoke a chroma database client. Okay? So for folks who are familiar with working with databases. Right? This is the same thing as creating it. Just a a connection. Right? So I'm going to say, chroma\\r\\n1:52:39\\r\\nCathal Flanagan: client equals Roma TV dot. Persistent client.\\r\\n1:52:54\\r\\nCathal Flanagan: Okay, I can give it a pop if I wish.\\r\\n1:53:04\\r\\nCathal Flanagan: Okay, so that's creating the kind of the database client itself. If I look at the\\r\\n1:53:07\\r\\nCathal Flanagan: folder here, I see it's created something called chroma. dB, with a chroma dot SQL. Live file\\r\\n1:53:14\\r\\nCathal Flanagan: cool alright. So now I can do stuff. With that\\r\\n1:53:22\\r\\nCathal Flanagan: I can create a collection within that database.\\r\\n1:53:25\\r\\nCathal Flanagan: Okay, so I'm gonna say, chroma collection equals Roma client Josh for Yace collection.\\r\\n1:53:28\\r\\nCathal Flanagan: And I'm going to just call this tech 16.\\r\\n1:53:44\\r\\nCathal Flanagan: Okay?\\r\\n1:53:48\\r\\nCathal Flanagan: So I've all I've done here is, I have created a database.\\r\\n1:53:50\\r\\nCathal Flanagan: I've created a collection. Think of it! It's like a table within the database. And now I actually need to\\r\\n1:53:55\\r\\nCathal Flanagan: kind of like, put stuff in there, right?\\r\\n1:54:01\\r\\nCathal Flanagan: So I can create the vector store vector door\\r\\n1:54:05\\r\\nCathal Flanagan: from my collection. This should be straightforward. I also need to say, store again, this is very much boilerplate. Right?\\r\\n1:54:15\\r\\nCathal Flanagan: Just to kind of get these things set up.\\r\\n1:54:24\\r\\nCathal Flanagan: Okay. But now what comes the interesting thing? But this will also look very similar to what we did previously.\\r\\n1:54:28\\r\\nCathal Flanagan: Right? Which is, I'm actually going to load up the data with one important addition. So this code I'm going to paste here is exactly the same code that we had earlier. Okay, I'm loading the data.\\r\\n1:54:35\\r\\nCathal Flanagan: The important, the important difference is or edge\\r\\n1:54:48\\r\\nCathal Flanagan: the important difference. If I'm passing in this new\\r\\n1:54:54\\r\\nCathal Flanagan: piece of information, I'm saying the storage context that you're using.\\r\\n1:54:59\\r\\nCathal Flanagan: Okay is, I'm calling it storage context. I could call it something completely different. But this is effectively saying, this is what you're going to be using, which is a chroma database.\\r\\n1:55:04\\r\\nCathal Flanagan: Okay, so that's the everything here is exactly the same.\\r\\n1:55:13\\r\\nCathal Flanagan: except for this one additional thing that I'm passing, in which I've set up here. Okay, to be a vector chroma store.\\r\\n1:55:17\\r\\nCathal Flanagan: Okay? But then I'm creating an engine, as I did previously asking the same question, okay.\\r\\n1:55:25\\r\\nCathal Flanagan: gonna load the file. Now it's loading the files into that database.\\r\\n1:55:34\\r\\nCathal Flanagan: Okay?\\r\\n1:55:39\\r\\nCathal Flanagan: And it's going to do a query over them\\r\\n1:55:41\\r\\nCathal Flanagan: a little sore because it's creating the database and running.\\r\\n1:55:53\\r\\nCathal Flanagan: breaking the file apart, putting it into the actual chroma database. Again. These will take a little longer as expected, but then it has the benefit that that database that exists, that you or your colleagues or the people could theoret theoretically use it. If it's within a centralized location.\\r\\n1:55:59\\r\\nCathal Flanagan: See if it gives us a response.\\r\\n1:56:19\\r\\nCathal Flanagan: Okay, while we're waiting for that, I'm going to Martha Stewart this and show you kind of the next section, because I want to inspire you for your your homework a little bit.\\r\\n1:56:24\\r\\nCathal Flanagan: So I want to show you how, in addition to querying\\r\\n1:56:32\\r\\nCathal Flanagan: a vector store, you can also query other data sets on the Internet or other places. We just showed a bunch of them from Linechain. And here I'm going to show you\\r\\n1:56:39\\r\\nCathal Flanagan: the example of creating the same similar type of index. But over web pages.\\r\\n1:56:50\\r\\nCathal Flanagan: So one second.\\r\\n1:57:00\\r\\nCathal Flanagan: I don't know why. It is being so slow tonight. It is being very painful.\\r\\n1:57:12\\r\\nCathal Flanagan: Okay, well, there we go. It's\\r\\n1:57:25\\r\\nCathal Flanagan: it's no, it's failing on us tonight. I will send. I will send you the prepared notebook.\\r\\n1:57:31\\r\\nCathal Flanagan: Which is our Martha Stewart version of this, where it is, in fact.\\r\\n1:57:38\\r\\nCathal Flanagan: up and running, and you can run it yourself into your heart's content. I'm not sure exactly. It's probably just having trouble with the server at the moment. But the web search the web page search is very similar in that. We're installing a web. Reader, here.\\r\\n1:57:43\\r\\nCathal Flanagan: Rice, creating a web. Reader.\\r\\n1:58:00\\r\\nCathal Flanagan: if I scroll down I'll show it to you.\\r\\n1:58:10\\r\\nCathal Flanagan: It installs a lot. Okay, and what you can do here is on the web. Reader, you can basically pass in a website that you want to use. So instead of using a Pdf document.\\r\\n1:58:22\\r\\nCathal Flanagan: are there websites that you want us to index instead.\\r\\n1:58:32\\r\\nCathal Flanagan: Right? So here you can pass in one or many websites that you wanted to actually read in.\\r\\n1:58:36\\r\\nCathal Flanagan: Okay, but it's exactly the same. Once you have the website much like the Pdf document, then you can actually index that or multiple websites themselves\\r\\n1:58:41\\r\\nCathal Flanagan: right? And actually set up a query engine and then actually do the queries directly over the website. Okay? And so it goes to the point on the homework that I want you to explore what's possible from all of the different\\r\\n1:58:52\\r\\nCathal Flanagan: from all of the different systems that are available here in the llama hub that I showed you so pick one or multiple that are interesting to you. Maybe it's spotify. Maybe it's Whatsapp. Maybe it's semantic scholar or many of the others. And instead of using a Pdf document. Use one of those if you want to use Pdf documents, please feel free to do that also. It's totally up to you. Kind of it's your homework. It's whatever you decide to do with it.\\r\\n1:59:07\\r\\nCathal Flanagan: But you know, see what you can build.\\r\\n1:59:32\\r\\nCathal Flanagan: What's the craziest data loader that you can get to work in its simplest form. You can just take it, you know, do it over a Pdf. Of your choice. But I think that most folks here will be capable of extending this to use any of another system that might be of interest to you.\\r\\n1:59:34\\r\\nCathal Flanagan: And so that is due next week before Class\\r\\n1:59:52\\r\\nCathal Flanagan: great. And with that we're actually at time. Next week we're going to go one level further. So tonight we did what was called naive rag. This is kind of building a Chatbot\\r\\n1:59:57\\r\\nCathal Flanagan: type of system over a simple knowledge base. You're going to take this and extend it to other databases or other data sites?\\r\\n2:00:08\\r\\nCathal Flanagan: But then next week we're going to talk about. Okay, how do we really optimize this? How do we kind of add in different elements. And particularly, how do we, you know, really refine the results to make the retrieval as good as it possibly can be, because if that's what we're really focused on in this class, it's, you know, we, we're not going to try and build a better model in Openai where we can have most impact\\r\\n2:00:18\\r\\nCathal Flanagan: is making that retrieval step as good as it possibly can be. So that's where next week we're going to pick up again with advanced rag.\\r\\n2:00:43\\r\\nCathal Flanagan: Excellent! I will stick around for questions. Thank you for those\\r\\n2:00:53\\r\\nCathal Flanagan: who joined us live. If you're watching the recording feel free to catch up in the slack channel on the conversation from tonight. We will also have, I believe, this paper club happening again this weekend and then I will have the office hours of the regular time on next Sunday.\\r\\n2:00:57\\r\\nCathal Flanagan: ash\\r\\n2:01:17\\r\\nCathal Flanagan: one Pm. Eastern 10 Am. Pacific, and I'll actually be in the Middle East, so it'll be 9 Pm. For me. So we'll do that next Sunday for anyone who would like to join and hang out and talk about anything that's on their mind.\\r\\n2:01:18\\r\\nCathal Flanagan: So we'll send out a recap email as usual, and thank you\\r\\n2:01:33\\r\\nCathal Flanagan: for week 3. Look forward to speaking to you all slack.\\r\\n2:01:37\\r\\nDarren Pitts: Cool. Thank you. Take care, guys.\\r\\n2:01:40\\r\\nMario Cadete: Thank you.\\r\\n2:01:43\\r\\nTushar Kothalkar: Thank you. Thank you.\\r\\n2:01:45\\r\\nPranshu Tiwari: Yeah. So so I had a question.\\r\\n2:01:47\\r\\nCathal Flanagan: No, no, we're gonna go by the order that's in the chat here. So Aj.\\r\\n2:01:49\\r\\nAjay Dawar: So, Charlie, is there a way to swap out\\r\\n2:01:57\\r\\nAjay Dawar: the cosine similarity with something else? So let's just say it's a different kind of data, and\\r\\n2:02:02\\r\\nAjay Dawar: or to use a different similarity algorithm?\\r\\n2:02:07\\r\\nAjay Dawar: Or is lama only PIN to cosine similarity and whatever cosine similarity were implemented.\\r\\n2:02:10\\r\\nDima Timofeev: I'm sorry I'm gonna interrupt here, I answered. Thank you for asking these questions here as well. I answered it in our in our channel, and I provided the link to the documentation to the class that you can overwrite and write any function you want.\\r\\n2:02:19\\r\\nCathal Flanagan: There you go, perfect, all right. I might check that out myself. I know that it's very possible, depending on the backend system that you're using. So it's again, it goes to the point of like when you're you. It's not by using a system or what Llama index provides. It's also very much.\\r\\n2:02:35\\r\\nCathal Flanagan: It will be limited by what the retrieval system allows. Right? So remember, we're using proma here.\\r\\n2:02:51\\r\\nCathal Flanagan: You might be using reviation. You might be using opensearch, you might be using pine cone.\\r\\n2:03:00\\r\\nCathal Flanagan: One of the ways they differentiate themselves from one another is the type of retrieval algorithms that they make available as well, okay. So again.\\r\\n2:03:06\\r\\nCathal Flanagan: to pick like what we're doing here, we're limited to some some degree by long index.\\r\\n2:03:17\\r\\nCathal Flanagan: Really, when you're putting this into production, the limitation or the considerations will be on the retrieval\\r\\n2:03:22\\r\\nCathal Flanagan: that is offered by the by the actual kind of cloud offering that you're you're likely using.\\r\\n2:03:29\\r\\nCathal Flanagan: Okay? Who else? Who's next?\\r\\n2:03:41\\r\\nCathal Flanagan: I'm good.\\r\\n2:03:44\\r\\nAndrey Skripkin: Yeah, I guess it's me.\\r\\n2:03:45\\r\\nAndrey Skripkin: I have a question. So I'm I'm looking to build an agent\\r\\n2:03:47\\r\\nAndrey Skripkin: for myself and my family where I can feed like the information about my family, basically like everything from the history of where we live and like addresses occupation like jobs\\r\\n2:03:53\\r\\nAndrey Skripkin: to the credit cards or insurance and stuff like that. And now we've uploaded Pdf. In this in this on this session. But is it possible to\\r\\n2:04:11\\r\\nAndrey Skripkin: to provide and feed the real time kind of a Google sheet or something where I where I stored this data about my life\\r\\n2:04:24\\r\\nAndrey Skripkin: and updated like, I don't know, on a regular basis, and and it has always the the latest information about me.\\r\\n2:04:33\\r\\nAndrey Skripkin: Yeah, to ask the question.\\r\\n2:04:42\\r\\nCathal Flanagan: Okay? So simple answer is, Yes, but again, it goes to some of the limitations. So you have to think about how these systems interact\\r\\n2:04:44\\r\\nCathal Flanagan: with data. Can you use it to retrieve information over Google sheets? You can\\r\\n2:04:55\\r\\nCathal Flanagan: does it work? Well, maybe the systems by design\\r\\n2:05:00\\r\\nCathal Flanagan: tend not to work very well with structured data at the moment. Again, it's getting better. But it's kind of my sense is you might be disappointed.\\r\\n2:05:06\\r\\nCathal Flanagan: So what are your 2 other options here? Simple one is what you're suggesting, which is, you put it all in text, and you, you know you do good retrieval over the text itself.\\r\\n2:05:15\\r\\nCathal Flanagan: Okay, that's kind of I think that would be like how you would do this in the short to medium term.\\r\\n2:05:26\\r\\nCathal Flanagan: If you were to say to me, what's the optimal way of building this?\\r\\n2:05:32\\r\\nCathal Flanagan: Well, what you're describing is a knowledge graph rise and a knowledge graph\\r\\n2:05:37\\r\\nCathal Flanagan: is, and maybe we'll do an office hours on this.\\r\\n2:05:43\\r\\nCathal Flanagan: A knowledge graph is a system. It's a database effectively that is designed for entities.\\r\\n2:05:47\\r\\nCathal Flanagan: entities and their associations. So, for example, the most famous knowledge graph would be the network graph that powers Linkedin or Facebook right\\r\\n2:05:55\\r\\nCathal Flanagan: where it's effectively you, everybody else right?\\r\\n2:06:06\\r\\nCathal Flanagan: A knowledge graph powers, all the search results on the right hand side of Google. When you actually ask about a person, for example, it shows their spouse their date of birth, etc.\\r\\n2:06:09\\r\\nCathal Flanagan: Structuring that type. This type of information is kind of perfect for a knowledge graph. Because you're saying, Okay, for my wife. Find me her birthday.\\r\\n2:06:21\\r\\nCathal Flanagan: Right?\\r\\n2:06:31\\r\\nCathal Flanagan: it's like, if you wanted to say the gold standard. That's how I would actually build it. But it's complicated. 1st of all, you have to stand up a knowledge graph. I could be an entire 8 week semester in itself of how to build good knowledge graphs.\\r\\n2:06:33\\r\\nCathal Flanagan: Neo 4 J. Is kind of the standard if you want to test it out and play with this\\r\\n2:06:52\\r\\nCathal Flanagan: But then also you have to be able to query it with natural language, and that requires you to write\\r\\n2:06:58\\r\\nCathal Flanagan: natural language to something called cipher. Cipher is the language for querying knowledge graphs.\\r\\n2:07:04\\r\\nCathal Flanagan: it is okay, it's not great. It is very sensitive to things like case case sensitivity. And so therefore you might be frustrated with it. So\\r\\n2:07:11\\r\\nCathal Flanagan: knowing, you know, honestly simplest answer right now would be.\\r\\n2:07:25\\r\\nCathal Flanagan: put it all in textual format, and do retrieval over the text.\\r\\n2:07:29\\r\\nAndrey Skripkin: Gotcha. Thank you.\\r\\n2:07:33\\r\\nCathal Flanagan: But then, again, that goes to an actually, that's an interesting example. Because that's what's chunking strategy.\\r\\n2:07:34\\r\\nCathal Flanagan: You would want to be careful about the chunking, and make sure effectively that each chunk is a person. Make it easy.\\r\\n2:07:39\\r\\nAndrey Skripkin: Perfect, appreciate it.\\r\\n2:07:48\\r\\nCathal Flanagan: Cool. All right. Start.\\r\\n2:07:50\\r\\nSarit Arora: Yeah, my question is actually somewhat somewhat related, but a specific question is related to A previous comment that you made, that when you are searching\\r\\n2:07:52\\r\\nSarit Arora: and retrieving from a database, then schema becomes really important, right?\\r\\n2:08:02\\r\\nSarit Arora: But one additional challenge that I think we are facing is that the the the databases could be relational.\\r\\n2:08:08\\r\\nSarit Arora: And\\r\\n2:08:17\\r\\nSarit Arora: it can have multiple paths connecting to each of the tables, right? So even if we have say structured it, and there is a schema. But when you are, say, putting a prompt or a natural language query that you want to kind of run out of that the the danger of not finding the right result is is huge, so are there any guidance on how to even do schema, so that it's readable by the prompts in a better way.\\r\\n2:08:19\\r\\nCathal Flanagan: Yes, Snowflake. Have a lot of detail on this for their cortex. Analyst.\\r\\n2:08:43\\r\\nCathal Flanagan: So it's hard to set up. They have what's called a yaml file and they actually requiring too much, in my opinion. For example, they want to have synonyms defined right? That revenue might also be called sales.\\r\\n2:08:49\\r\\nCathal Flanagan: They also want you to define\\r\\n2:09:03\\r\\nCathal Flanagan: common expressions that you might be using. They have a streamless app or a an app that they're setting up to automatically try and generate some of that for you. But it's still far too manual, in my opinion. So\\r\\n2:09:07\\r\\nCathal Flanagan: you can look at the cortex type, typing cortex analysts.\\r\\n2:09:21\\r\\nCathal Flanagan: yaml, file generator, or something like that. And you'll get a sense for kind of what they require.\\r\\n2:09:25\\r\\nCathal Flanagan: And as far as I'm aware they're kind of furthest along here.\\r\\n2:09:31\\r\\nSarit Arora: Thanks, I think. Looks like there's an opportunity to automate here.\\r\\n2:09:35\\r\\nCathal Flanagan: Yeah, for I was. I've been talking to Snowflake about it. They're well aware of it.\\r\\n2:09:39\\r\\nSarit Arora: Cool. Thank you.\\r\\n2:09:43\\r\\nCathal Flanagan: Sure Mahesh.\\r\\n2:09:45\\r\\nmahesh: Yeah, I have 3 simple questions. For the 1st question is.\\r\\n2:09:46\\r\\nmahesh: see, typically, when we use this Api key, we are built on tokens right like whatever number of tokens we use now, considering the llama index is internally calling. Llm. How do I ensure that llama index is not, you know, burning out my credit card.\\r\\n2:09:51\\r\\nDima Timofeev: Well, and it's very common problem if you worry about like bad intentions the code is open, sourced\\r\\n2:10:12\\r\\nDima Timofeev: so you can check it out, and usually community takes care of it. But for a credit card I will pass it back to Charlie.\\r\\n2:10:21\\r\\nCathal Flanagan: Yeah, so couple of things at different points along the way you can measure. So there's certain libraries. One called token, that can allow you to effectively understand how many tokens are getting passed in, and if you're you could theoretically use that to set breaks or limits.\\r\\n2:10:31\\r\\nCathal Flanagan: I prefer to do it a slightly different way, which is, I'll run everything with a\\r\\n2:10:49\\r\\nCathal Flanagan: lower cost model at 3, 5 model, or something like that which is at this point effectively free, and\\r\\n2:10:55\\r\\nCathal Flanagan: then I will get a sense for it, and then I'll multiply it by 10 or something like that, you know, depending on the differential\\r\\n2:11:02\\r\\nCathal Flanagan: to understand what it might actually be. Introduction. Because you know, the.\\r\\n2:11:09\\r\\nCathal Flanagan: the, the, the system.\\r\\n2:11:18\\r\\nCathal Flanagan: Yeah, we're not fully in, like, we're not fully in control of this. It has a certain amount of agent, particularly when we move to agents like they can decide what they want.\\r\\n2:11:21\\r\\nCathal Flanagan: You kind of do you have to monitor that yourself somewhat manually?\\r\\n2:11:30\\r\\nCathal Flanagan: then you have additional con confusion arises. When then, when you're using reasoning models, and even though you're passing in\\r\\n2:11:37\\r\\nCathal Flanagan: a hundred 1,000 tokens and only getting back 4,000, you'll still get billed for 140,000, because they'll they reserve 30,000 private tokens for reasoning. For example.\\r\\n2:11:46\\r\\nDima Timofeev: And this problem is not unique and not limited to large language models. You probably have seen a lot of memes when people were forgetting to shut down their very expensive gpus on Aws or Google Cloud, or when you had an intern who tried to say, Select star from table with trillions of records in some highly distributed, expensive databases like Redshift or\\r\\n2:11:59\\r\\nDima Timofeev: Google bigquery. So it's coming. It's an economy of cloud services.\\r\\n2:12:24\\r\\nDima Timofeev: And it's so easy to slip down and crash your credit card balance.\\r\\n2:12:30\\r\\nCathal Flanagan: And I can, I can add, plus one on that. I once had an accidental $17,000\\r\\n2:12:36\\r\\nCathal Flanagan: build bigquery for an inefficient query.\\r\\n2:12:44\\r\\nmahesh: Well.\\r\\n2:12:48\\r\\nmahesh: okay, okay. So sorry. I had 2 more questions. So the second question is, so I looks like, just by using llama index, I can build an AI application without having to know a whole lot of long. You know. Llms, is that a fair understanding.\\r\\n2:12:49\\r\\nCathal Flanagan: Yes, that's and many have, and many do.\\r\\n2:13:09\\r\\nCathal Flanagan: so is it perfectly optimized? Is it? As fast as you might want? Is this, you know all the bill, you know. No. But can you build a good Chatbot over a knowledge base with exactly what the information we've shown you today is 100. Yes.\\r\\n2:13:14\\r\\nmahesh: Okay. Now, is it equally comparable? Lama index versus lang chain versus Gen. AI. All these are are these some standard features that they provide, whether you know whether I want to build a Chatbot, or maybe I want to build an agent to do some more sophisticated work.\\r\\n2:13:29\\r\\nCathal Flanagan: Are, we.\\r\\n2:13:49\\r\\nDima Timofeev: Would you mind if I take this.\\r\\n2:13:49\\r\\nCathal Flanagan: Yeah. Man go for it.\\r\\n2:13:51\\r\\nDima Timofeev: So usually somebody, after practicing with multiple tools, will pick their own tool, and people will fight. It's almost like holly wars between different frameworks\\r\\n2:13:53\\r\\nDima Timofeev: and people just eventually. So I am a length chain person, but I usually never admit it in public, because Charlie is Olema Index person, and we never talk about frameworks because we want to stay friends.\\r\\n2:14:04\\r\\nDima Timofeev: So it it goes like this. The All. Those are sophisticated wrappers with a lot of additional functionality. You try different ones. You pick whatever you prefer personally on what is the standard. And if we are talking about specifically, Lemma index and blank chain, which is, which are top 2. So there isn't they like, I would not be surprised if they are like 8 90 plus of all. Lm, wrappers. Lm. Applications.\\r\\n2:14:18\\r\\nDima Timofeev: Surprisingly, they were built by 2 people who were sitting next to each other in the office.\\r\\n2:14:47\\r\\nDima Timofeev: and then they both left and started those 2 companies and went very different directions. So they they were like literally from neighboring teams, and they were sitting a few desks from each other.\\r\\n2:14:52\\r\\nCathal Flanagan: yes, and and I like I used to be a Langchain person. I'm now Lama Index person, because Langchain kept changing where they were effectively how they were structuring their their modules, their code, and every time I would rerun it every second day it was breaking, and so back to developments, but I just didn't like that aspect of it. And I got to know the CEO Lama Index, and I like him a lot which is also part of my bias. Dina. Why do you like long chain? Still.\\r\\n2:15:07\\r\\nmahesh: I heard the same feedback that they keep changing so fast that court seem to break every every other day.\\r\\n2:15:37\\r\\nDima Timofeev: You see, my applications are very small, and usually are just like more tools rather than full scale applications, and I struggle with preset defaults. I struggle with overriding settings. I want something like a like a function call or single tool, or a single override so very narrow stuff. And I really appreciate the simplicity of blank chain to this extent.\\r\\n2:15:44\\r\\nCathal Flanagan: Well, see, then, you would argue, lang lama index is just too much abstractors.\\r\\n2:16:12\\r\\nDima Timofeev: Many things. Yes, and then I don't care about rag. Usually like in 99% of cases when I build something and lama index is rag centric primarily, not so much anymore. But it used to be for years rag Centric and I don't care about rag application. So I just want to have some thin layer around universal Apis, and that is why length chain is my my choice, plus. I think they use\\r\\n2:16:17\\r\\nDima Timofeev: python magic. This piping operator, which is very natural for data processing folks. And I just like to use the same like Linux style data processing data management style operators, which is very easy to reason about personally for me.\\r\\n2:16:43\\r\\nCathal Flanagan: I think, what you're also observing. There is engineer versus data scientist.\\r\\n2:17:00\\r\\nCathal Flanagan: I like the abstraction. Dima hates the abstraction.\\r\\n2:17:04\\r\\nDima Timofeev: Oh!\\r\\n2:17:10\\r\\nCathal Flanagan: Or Mahesh.\\r\\n2:17:11\\r\\nmahesh: Thank you.\\r\\n2:17:13\\r\\nCathal Flanagan: Punch. You.\\r\\n2:17:19\\r\\nPranshu Tiwari: Yeah, thank you. So I would like to go back or you know.\\r\\n2:17:22\\r\\nPranshu Tiwari: go back to the original question, and maybe paraphrase it.\\r\\n2:17:28\\r\\nPranshu Tiwari: I wanted to understand the difference between the last lecture and this lecture.\\r\\n2:17:33\\r\\nPranshu Tiwari: So from the last lecture I understood\\r\\n2:17:41\\r\\nPranshu Tiwari: that when we use lang chain we are using the QK.\\r\\n2:17:44\\r\\nPranshu Tiwari: From the Lm model, and the value from the Pdf document\\r\\n2:17:53\\r\\nPranshu Tiwari: which we have stored in the Google Colab, or where wherever is it\\r\\n2:18:00\\r\\nPranshu Tiwari: and in the in the today's class.\\r\\n2:18:07\\r\\nPranshu Tiwari: What I understand is that the Qaty and the key is is contextualized from the Pdf.\\r\\n2:18:11\\r\\nPranshu Tiwari: But the value is coming from the larger corpus, which is learned from the original Llm.\\r\\n2:18:27\\r\\nPranshu Tiwari: And hence I feel\\r\\n2:18:38\\r\\nPranshu Tiwari: this comes to the original question of Langchain versus Lama Index, though they are just applications. There could be certain cases where we need specific information like the company policy of company of a company. Let's say ABC would be more appropriate\\r\\n2:18:42\\r\\nPranshu Tiwari: coming from a Pdf documentation than from the wider value coming from a larger context because they won't be having the specifics like holidays of that company on in the specific geography.\\r\\n2:19:03\\r\\nPranshu Tiwari: Is my understanding or or paraphrasing right.\\r\\n2:19:21\\r\\nCathal Flanagan: Nope,\\r\\n2:19:26\\r\\nCathal Flanagan: So\\r\\n2:19:28\\r\\nCathal Flanagan: in both cases. So I just want to be like super explicit\\r\\n2:19:33\\r\\nCathal Flanagan: you. The the term learned or learning is a really loaded term in this world, because it means some very specific things. So I want to just again and reemphasize at no point in this process last week or next this week, or I, until until we get to the lecture on fine tuning. Are we ever changing the model? Are we ever\\r\\n2:19:37\\r\\nCathal Flanagan: training the model to do anything are to a new level.\\r\\n2:19:59\\r\\nCathal Flanagan: The difference between last week and this week is really around.\\r\\n2:20:06\\r\\nCathal Flanagan: An intermediate step be introduced. This week. Last week\\r\\n2:20:11\\r\\nCathal Flanagan: we were passing in all of the information and saying, summarize this\\r\\n2:20:15\\r\\nCathal Flanagan: effectively right, or we are chunking up and dividing the information, using the Mapreduce method and saying, Summarize the 1st page, the second page, 3rd page, and then give me an overall summary.\\r\\n2:20:22\\r\\nCathal Flanagan: Okay, it is very much focused on the summarization task.\\r\\n2:20:33\\r\\nCathal Flanagan: and the model at some point sees all of the data.\\r\\n2:20:38\\r\\nCathal Flanagan: whether that's in, you know, piece by piece, or it is the entire thing at once. The model sees all of that document that you want to summarize. Okay.\\r\\n2:20:42\\r\\nCathal Flanagan: today, we were using the example of a single document as a knowledge source. But really, you should also be thinking about this, as it relates to\\r\\n2:20:55\\r\\nCathal Flanagan: possibly tens of thousands of documents.\\r\\n2:21:03\\r\\nCathal Flanagan: And when you have that volume of information, and you want to just search for information over it. How do you do that?\\r\\n2:21:06\\r\\nCathal Flanagan: Well, you have to introduce this intermediate step where you're storing. You're breaking down that information apart in a way that you can be. You can search over it in a very efficient way, find the 5 most relevant paragraphs within that all of those documents, because again, today we do use one document. But again, what I expect in the enterprise is going to be tens of thousands of documents. So amongst tens of thousands of documents.\\r\\n2:21:13\\r\\nCathal Flanagan: how do we find just the 3, most 5 most relevant paragraphs within all of those those pieces of text.\\r\\n2:21:40\\r\\nCathal Flanagan: only grabbing the 5 of 5, 1020 of those\\r\\n2:21:47\\r\\nCathal Flanagan: passing just that information into the model with the original question, and using that\\r\\n2:21:52\\r\\nCathal Flanagan: as that small set of knowledge to answer from this much larger set of knowledge to answer that question\\r\\n2:21:59\\r\\nCathal Flanagan: right? And so the difference between last week and this week is anything we did last week.\\r\\n2:22:06\\r\\nCathal Flanagan: The model was seeing everything this week.\\r\\n2:22:11\\r\\nCathal Flanagan: We're being very, very selective, potentially out of tens of thousands of paragraphs. We're finding 5\\r\\n2:22:15\\r\\nCathal Flanagan: that are then being passed into the model. Does that make sense.\\r\\n2:22:22\\r\\nPranshu Tiwari: No? Yeah, partially.\\r\\n2:22:27\\r\\nPranshu Tiwari: But actually, you mentioned somewhere that the key that the key value is updated\\r\\n2:22:29\\r\\nPranshu Tiwari: because the key comes from the contextual meaning of the 5 items.\\r\\n2:22:36\\r\\nCathal Flanagan: Oh, okay.\\r\\n2:22:42\\r\\nCathal Flanagan: right? I just I I unless I'm mistaken, I didn't use the term key value because T. Again, key value in python have the specific meaning. So I just want to be really careful on term.\\r\\n2:22:43\\r\\nPranshu Tiwari: No, I am using the key value of the Llm. So if, since Llm. Uses query, key and value.\\r\\n2:22:54\\r\\nPranshu Tiwari: so I believe the value is coming from the from the bigger corpus learned by the Llm. But the key is coming from the from the learned learned context which you have told, while in yesterday's while in the last week class.\\r\\n2:23:00\\r\\nPranshu Tiwari: the value is coming from the Pdf.\\r\\n2:23:18\\r\\nPranshu Tiwari: With the query and the key\\r\\n2:23:23\\r\\nPranshu Tiwari: from the original Llm. Original Llm.\\r\\n2:23:26\\r\\nCathal Flanagan: Okay. I see that.\\r\\n2:23:31\\r\\nDima Timofeev: I can help you and address it in the chat, if you would like to. I think there is some terminology shift, and I can help you offline, and we can give a bit more time to other folks.\\r\\n2:23:32\\r\\nPranshu Tiwari: Yeah, yeah.\\r\\n2:23:46\\r\\nDima Timofeev: If you don't. That's fine.\\r\\n2:23:46\\r\\nPranshu Tiwari: Yeah, that's fine. And when I say queue key and value I mean to say, not the query key of the dictionary, but the query key value of the Llm. So that's what I want to say.\\r\\n2:23:47\\r\\nCathal Flanagan: Okay, we need to define what that means, or at least how you think. Please just say to explain what you mean by the key value of the Llm. Because\\r\\n2:23:56\\r\\nCathal Flanagan: I am not. That is not technical terminology that I'm familiar with.\\r\\n2:24:04\\r\\nSunil Samel: If I may, I think Branch is referring to the Qkv. Matrices. The weights that are used in.\\r\\n2:24:09\\r\\nCathal Flanagan: Oh, okay. Okay, okay, I understand. Right?\\r\\n2:24:17\\r\\nPranshu Tiwari: Yeah.\\r\\n2:24:21\\r\\nCathal Flanagan: Okay, let's take it offline.\\r\\n2:24:22\\r\\nCathal Flanagan: Jeremy.\\r\\n2:24:27\\r\\nCathal Flanagan: Nope, okay,\\r\\n2:24:36\\r\\nJeremy McCormick: Sorry I was. I was muted sorry about that. I was wondering about for the rag approach. How well does it do with like semi-structured data like Json, I mean, I have a I have something for work where I have Json, that that actually is like a structured format that describes database tables and columns. I'm wondering could I ask it? Questions like\\r\\n2:24:39\\r\\nJeremy McCormick: about groups of columns or something? And it would would it understand that? Are there techniques I could use to kind of\\r\\n2:25:06\\r\\nJeremy McCormick: get the data in a certain form where it would work better with an Llm. Or is it just? Is this, would it not work? Very well if I just. In other words, I'm talking about loading the actual Json files into let's say, Llama Index. W. Would that work? Well.\\r\\n2:25:14\\r\\nCathal Flanagan: Yes, in fact, when we hold most retrieval systems, we actually are returning Json.\\r\\n2:25:29\\r\\nCathal Flanagan: which is metadata and the text itself. So the model works quite well. It does understand. Json.\\r\\n2:25:37\\r\\nCathal Flanagan: yeah, I think that's perfectly reasonable. It's well structured.\\r\\n2:25:45\\r\\nCathal Flanagan: In fact, the best way that we can sometimes deal with tables nowadays, and tables that are in structured format is actually 1st convert them to Json.\\r\\n2:25:50\\r\\nJeremy McCormick: Oh, actually converting the table data to Json and then having it. Look at the Json. Okay? Probably because it's better able to tokenize it. Then, alright, I'll I'll I'll give. Try.\\r\\n2:26:01\\r\\nCathal Flanagan: Put better structure there. It's not perfect by any means, but it's kind of it's the best that people are doing at the moment when it relates to table data.\\r\\n2:26:11\\r\\nJeremy McCormick: Okay, I'll give it a try. Then. Thanks.\\r\\n2:26:19\\r\\nCathal Flanagan: Yeah. Good one, maybe, for the homework next.\\r\\n2:26:21\\r\\nCathal Flanagan: How do? How do I pronounce your name? Nitran apologies.\\r\\n2:26:28\\r\\nNguyen Khoa: Page 2.\\r\\n2:26:32\\r\\nAjay Dawar: New Year.\\r\\n2:26:34\\r\\nCathal Flanagan: New new one. Okay, cool. Nice to meet you.\\r\\n2:26:35\\r\\nCathal Flanagan: Do you? Wanna give us your question?\\r\\n2:26:42\\r\\nNguyen Khoa: So basically, you know, correct? Because it's yeah.\\r\\n2:26:46\\r\\nCathal Flanagan: Have a question.\\r\\n2:26:53\\r\\nNguyen Khoa: Thank you.\\r\\n2:26:55\\r\\nSunil Samel: He's speaking, but we can't hear him.\\r\\n2:26:57\\r\\nJeremy McCormick: Yeah. Your volume is very, very low.\\r\\n2:26:59\\r\\nCathal Flanagan: Okay, maybe we can either take it offline or you can maybe just increase your microphone and we'll come back again. Kate.\\r\\n2:27:06\\r\\nYankai Su: Yes, so my whole, like a couple of questions related to each other. The 1st one like to simply suppose a company want to build a chat, Chatbot\\r\\n2:27:14\\r\\nYankai Su: to leverage all these documents they have, so help to search, or on top of that, using\\r\\n2:27:26\\r\\nYankai Su: the large number model capabilities. Today you introduce this lamb index.\\r\\n2:27:36\\r\\nYankai Su: so can I consider like to. This is the right way to\\r\\n2:27:43\\r\\nYankai Su: to go to build this Chatbot on top of these companies.\\r\\n2:27:47\\r\\nYankai Su: Document collaboration, double collection.\\r\\n2:27:55\\r\\nCathal Flanagan: Okay. Sorry I'm I'm can you repeat that? I'm not exactly sure what the what the question is.\\r\\n2:28:02\\r\\nYankai Su: Suppose that the company want to build a Chatbot.\\r\\n2:28:07\\r\\nCathal Flanagan: Yep.\\r\\n2:28:13\\r\\nYankai Su: To to help later to query all these documents?\\r\\n2:28:14\\r\\nYankai Su: Oh, so can I consider it today's way the Lama Index.\\r\\n2:28:21\\r\\nYankai Su: It's is the right way to go to build this application.\\r\\n2:28:26\\r\\nCathal Flanagan: Okay. So I think your question is, is, if a company wants to do something in production, Islama index, the right way to go.\\r\\n2:28:33\\r\\nYankai Su: You know.\\r\\n2:28:40\\r\\nCathal Flanagan: Hence, so lama Index is a way to abstract\\r\\n2:28:40\\r\\nCathal Flanagan: and make it easy to build these things. So are people building.\\r\\n2:28:48\\r\\nCathal Flanagan: I Enterprise chat box with Lama Index. Absolutely\\r\\n2:28:51\\r\\nCathal Flanagan: it. Well, I it just depends on the scale and sophistication. So if you had a small engineering team\\r\\n2:28:57\\r\\nCathal Flanagan: yes, I think that's that's the way to go most of this is about the retrieval system.\\r\\n2:29:03\\r\\nCathal Flanagan: right rather than lama index per se. So the technical challenge is not\\r\\n2:29:09\\r\\nCathal Flanagan: kind of moving information back and forth into the model. It's actually, I'm building that in retrieval.\\r\\n2:29:16\\r\\nCathal Flanagan: that retrieval system itself standing that up. It's often a managed service on a cloud provider, for example, like this is very much a data engineering challenge per se\\r\\n2:29:23\\r\\nCathal Flanagan: but\\r\\n2:29:33\\r\\nCathal Flanagan: assuming you get that stood up, and you actually you. You load up your documents and you have pipelines in place to keep them fresh and up to date, etc. Then Llama index is a great tool to\\r\\n2:29:34\\r\\nCathal Flanagan: orchestrate a lot of that and pull it out.\\r\\n2:29:46\\r\\nCathal Flanagan: You see, people, you know using it, I see see people moving away because they basically just wanna do things themselves in very specific ways.\\r\\n2:29:49\\r\\nCathal Flanagan: And they'll just use them index for the loading functionality, for example, rather than like the chat bot itself.\\r\\n2:29:58\\r\\nCathal Flanagan: Because the Chatbot itself ultimately is a pretty simple thing. You're basically calling\\r\\n2:30:06\\r\\nCathal Flanagan: a an Api, which is your retrieval system and getting getting context back and passing it in.\\r\\n2:30:13\\r\\nCathal Flanagan: So I definitely see people using elements of llama index.\\r\\n2:30:21\\r\\nCathal Flanagan: But you know, people pick and choose much like banking. They pick and choose the different parts that are abstractions that they need, and they build some of the rest itself. But\\r\\n2:30:27\\r\\nCathal Flanagan: if you're looking to prototype, or you want to get the 1st version up and running, I would 100% say, llama index is the right way to go.\\r\\n2:30:36\\r\\nYankai Su: Oh, is there some other better choice, or some other good choice other than like going this way?\\r\\n2:30:45\\r\\nCathal Flanagan: Is there any better choices? Well, Langchain is the primary other choice from a framework perspective.\\r\\n2:30:55\\r\\nCathal Flanagan: Otherwise, people just kind of build it itself themselves like they're. These are abstractions. The benefit of most of these are actually just the connectors that they have to make it easy. And, you know, allow you to switch out like chroma. dB, to an elastic to you know something else.\\r\\n2:31:01\\r\\nCathal Flanagan: So it allows you to kind of write the code once, but apply it to many different types of systems and standardize your code. But if people have decided on one retrieval system that they're using one model provider. You know\\r\\n2:31:15\\r\\nCathal Flanagan: there isn't a whole lot of value that a lama index would would add in that particular use case, because it's quite simple to build these systems with that\\r\\n2:31:28\\r\\nCathal Flanagan: framework.\\r\\n2:31:37\\r\\nCathal Flanagan: You know, Buff.\\r\\n2:31:39\\r\\nCathal Flanagan: it's I would suggest it's the right place to begin, particularly as you're prototyping or spinning up. And again, our people are building these and real real production systems with lama index. So it is a very serious framework as well.\\r\\n2:31:42\\r\\nCathal Flanagan: 19 option. In the larger community. We're gonna talk about agents in a few weeks. So I think that's a separate thing of like you would have for simple rag.\\r\\n2:31:56\\r\\nCathal Flanagan: which is most of the ways people of enterprise are using this today. You'll use line chain or llama index.\\r\\n2:32:08\\r\\nCathal Flanagan: We're gonna talk about frameworks for for agents in a few weeks time. That's a much less settled area. There's still many options, and they're all pretty bad and pretty ways. We're gonna talk about the least bad of them, which is called crew AI so again, depending on the workflow, you'll kind of do.\\r\\n2:32:13\\r\\nYankai Su: Okay, yeah. Another question related to slide when I see, of course, only today's these examples, I feel like to what we are using this Lama index.\\r\\n2:32:34\\r\\nYankai Su: It's more like a search engine.\\r\\n2:32:47\\r\\nYankai Su: So the question is like,\\r\\n2:32:52\\r\\nYankai Su: can can I just consider now, like, I just give this Pdf document, and then later, I\\r\\n2:32:56\\r\\nYankai Su: perform this search engine on top of it, or.\\r\\n2:33:04\\r\\nDima Timofeev: If you do, you go.\\r\\n2:33:08\\r\\nYankai Su: Here we're talking about the lamp. Here we are talking about the live language model\\r\\n2:33:09\\r\\nYankai Su: so it, I mean.\\r\\n2:33:15\\r\\nYankai Su: is it like, it's kind of more like a search engine here, or it's in fact, it's largely model powered.\\r\\n2:33:18\\r\\nYankai Su: Well.\\r\\n2:33:27\\r\\nCathal Flanagan: And it goes to the point of what? What are these systems? They're retrieval, augmented generation systems.\\r\\n2:33:28\\r\\nCathal Flanagan: Okay, the keyword retrieval, the thing that we build that we're responsible for that we have the most control over is the retrieval which is a search index.\\r\\n2:33:34\\r\\nCathal Flanagan: Rise.\\r\\n2:33:43\\r\\nCathal Flanagan: That's what we're still responsible for building the language model itself.\\r\\n2:33:45\\r\\nCathal Flanagan: the brain, the lot. You know, the bit that we're passing the information into.\\r\\n2:33:49\\r\\nCathal Flanagan: That's open. AI, that's anthropic. That's the model foundational model providers. Okay? So yes. Like, we are large, largely building with these systems a search index, a really good search index, a really good retrieval system. Again, we're using things that services that other people have built. But like, that's the core of the of\\r\\n2:33:54\\r\\nCathal Flanagan: what we need to build for the enterprise.\\r\\n2:34:17\\r\\nDima Timofeev: And if we take it to the alternate state, and we see that we have entire Internet. And we have Olm on top of this Internet, we will build a multi-billion business which is called perplexity.ai.\\r\\n2:34:21\\r\\nCathal Flanagan: Oh!\\r\\n2:34:35\\r\\nYankai Su: Guess my question is maybe use example. When I think a lot of us have that, the experience we build the website using the\\r\\n2:34:37\\r\\nYankai Su: the Google, provided you can simply build a website. And then also, it will provide you a search engine\\r\\n2:34:49\\r\\nYankai Su: on your website. So you can like associate it based on only your website. Now, whatever building today, using this lamb index, can we consider it something similar? Just like, let the Google website, we can\\r\\n2:34:58\\r\\nYankai Su: provided the search for any function.\\r\\n2:35:15\\r\\nCathal Flanagan: Before.\\r\\n2:35:19\\r\\nCathal Flanagan: It's like most search that's done today, particularly on a website. It's like keyword search right? It's looking.\\r\\n2:35:20\\r\\nYankai Su: And.\\r\\n2:35:27\\r\\nCathal Flanagan: Words that you're searching for. When we do search over these type of systems, it's generally what's called semantic search.\\r\\n2:35:27\\r\\nCathal Flanagan: So think about the word Us. Or U.S.A. If you search for that, people also want you to find United States of America. If that's in the document, they they think about them as the same thing, even though they're spelled very differently.\\r\\n2:35:36\\r\\nCathal Flanagan: Raj. So\\r\\n2:35:49\\r\\nCathal Flanagan: when you're using these type of systems, the way it's different than other systems you might use. For example, workday or outlook, etc.\\r\\n2:35:51\\r\\nCathal Flanagan: is, those systems will generally be using keyword search. Any system we use here is generally using semantic search. So it's a it's similar. But it's a slightly different type of search that's occurring.\\r\\n2:36:00\\r\\nDima Timofeev: As Charlie mentioned previously, we tend to combine those 2 approaches to get the best from 2 worlds and semanticity of words. Why, we suspect that America is the same as the Us. U.S.A. Is learned property. So embedding model is also learning a huge purpose of data, and it gets this knowledge from the training data. So there is no semantical\\r\\n2:36:15\\r\\nDima Timofeev: connection in your training data. It will never learn that America is usually refers as U.S.A, etcetera.\\r\\n2:36:42\\r\\nYankai Su: Oh, I guess in our class of\\r\\n2:36:51\\r\\nYankai Su: maybe we're covered at for refine the model\\r\\n2:36:57\\r\\nYankai Su: right? Maybe it's in some lecture.\\r\\n2:37:02\\r\\nYankai Su: Are we going to cover, like to refine the model, return this terrible thing.\\r\\n2:37:06\\r\\nCathal Flanagan: Sorry you mean, retrain the model I like. So we're going to cover fine tuning of models.\\r\\n2:37:12\\r\\nYankai Su: Only.\\r\\n2:37:18\\r\\nCathal Flanagan: Fine-tuning of models from the perspective of fine-tuning for behavior, not knowledge.\\r\\n2:37:19\\r\\nCathal Flanagan: Because fine-tuning for knowledge has very limited\\r\\n2:37:25\\r\\nCathal Flanagan: impact rag will give you. And we're going to talk about this next week at the beginning of class effectively. We're going to talk about the paper, which shows that you know, for the most benefits that you're going to be able to achieve in these type of systems.\\r\\n2:37:29\\r\\nCathal Flanagan: Tuning the model for knowledge is\\r\\n2:37:44\\r\\nCathal Flanagan: will give you the lower Roi compared to\\r\\n2:37:47\\r\\nCathal Flanagan: building a retrieval augmented generation system. So it's not that it doesn't do anything but like you can see the chart here.\\r\\n2:37:51\\r\\nCathal Flanagan: It's look as rag, only versus fine tune, model or right\\r\\n2:38:02\\r\\nCathal Flanagan: rag is is significantly better and effectively the same as as a fine tune plus rag approach. So\\r\\n2:38:09\\r\\nCathal Flanagan: we are going to talk about fine tuning models, but in a very narrow case we are not. This class is not about building the next Openai model.\\r\\n2:38:18\\r\\nYankai Su: Okay, I my last question. I sorry I take a little bit long regarding like this database. I think some other fellow already asked this question.\\r\\n2:38:28\\r\\nYankai Su: Now we convert her to Json.\\r\\n2:38:38\\r\\nYankai Su: Now, in case if the table is says it's bigger.\\r\\n2:38:42\\r\\nYankai Su: do we have some other better way, or pretty much, that this system\\r\\n2:38:47\\r\\nYankai Su: is the best way for lightning, using lightning model.\\r\\n2:38:54\\r\\nCathal Flanagan: Or for.\\r\\n2:38:57\\r\\nDima Timofeev: You can have the database of any size, you can put as many data as you want. The only physical limitation is the hardware you can afford. It can be petabyte scale vector store with every single unit of information on the planet. If it can fit into a distributed database.\\r\\n2:38:58\\r\\nCathal Flanagan: Wow.\\r\\n2:39:20\\r\\nCathal Flanagan: yeah, I think the question, though, might be, because if there's a conversion from that into Json, is that correct?\\r\\n2:39:21\\r\\nYankai Su: The.\\r\\n2:39:28\\r\\nYankai Su: It's kind of the the table size. Suppose that you have like multiple big tables.\\r\\n2:39:30\\r\\nYankai Su: The very 1st question is to.\\r\\n2:39:39\\r\\nYankai Su: is it the right way. We convert it to Json, and then.\\r\\n2:39:42\\r\\nCathal Flanagan: No, not really.\\r\\n2:39:47\\r\\nYankai Su: Available.\\r\\n2:39:48\\r\\nCathal Flanagan: Was the question from maybe I think with Jeremy earlier around Json as an input was like, you know, as a structure of information. It wasn't really taking a structured database table down to\\r\\n2:39:49\\r\\nCathal Flanagan: like a a large scale down to Jason representation in Json. If you have a very large scale table that you want to ask questions over.\\r\\n2:40:02\\r\\nCathal Flanagan: It is possible today there are connectors to all the major database providers. You should try it.\\r\\n2:40:12\\r\\nYankai Su: Thank you.\\r\\n2:40:18\\r\\nCathal Flanagan: What I'm going to suggest is, if you try it, you will probably be disappointed.\\r\\n2:40:19\\r\\nCathal Flanagan: Why? Because it doesn't fully understand the schema of the table, so you can manually give it the schema in the prompt\\r\\n2:40:25\\r\\nCathal Flanagan: right?\\r\\n2:40:32\\r\\nCathal Flanagan: and you know that will probably do a decent job. I would be surprised if it didn't. But the problem is, when you have 50 tables that are changing all the time.\\r\\n2:40:36\\r\\nCathal Flanagan: How does it? How do you keep that schema up to date refreshed.\\r\\n2:40:44\\r\\nCathal Flanagan: Is that your responsibility? Should that live on the server side? That's where kind of snowflakes, you know, Will.\\r\\n2:40:47\\r\\nCathal Flanagan: That's where they they should be managing that right. You hand off those tasks to them. But can you build this weekend for class next week? A\\r\\n2:40:54\\r\\nCathal Flanagan: system that will retrieve information will write a SQL. Query and retrieve information for you. Yes, you can.\\r\\n2:41:03\\r\\nCathal Flanagan: It's just. The queries tend to. It tends to be very brittle in my experience.\\r\\n2:41:13\\r\\nYankai Su: Yeah, yeah, this week, the the homework. I I did try a little bit about the launching to create.\\r\\n2:41:18\\r\\nYankai Su: I forgot to create a table or something that to hmm.\\r\\n2:41:27\\r\\nYankai Su: So they didn't. But I I don't know. Let's I don't know that, really. That's a good way\\r\\n2:41:34\\r\\nYankai Su: to go know.\\r\\n2:41:40\\r\\nCathal Flanagan: Queries or natural language over just asking questions for structured data is still a very hard problem. At the moment it's it's tractable. You'll get some results, but it's still very much in the prototyping phase rather than the production phase. In my opinion.\\r\\n2:41:42\\r\\nYankai Su: Okay.\\r\\n2:42:00\\r\\nYankai Su: So you mean, like, for database part is still kind of hard.\\r\\n2:42:01\\r\\nCathal Flanagan: Yeah, it's kind of it's just that it's not, doesn't come natural to the model. Remember, the model is set up\\r\\n2:42:05\\r\\nCathal Flanagan: to work well with text information. If you give it a database table, it will try its best. But it's another. We're all. We're all about\\r\\n2:42:10\\r\\nCathal Flanagan: minimizing errors here, and that that's introduced a lot of errors potentially.\\r\\n2:42:20\\r\\nRavi Shankar: It can't. A module trained on relational data sets purely.\\r\\n2:42:27\\r\\nCathal Flanagan: Yes.\\r\\n2:42:34\\r\\nRavi Shankar: Techniques will be different from training on the simple text than relational.\\r\\n2:42:35\\r\\nCathal Flanagan: Because remember, what they're trying to do is predict the next word.\\r\\n2:42:40\\r\\nCathal Flanagan: Rather than so.\\r\\n2:42:46\\r\\nCathal Flanagan: you know, people are trying different methods here like reducing them down to different schemas. We've talked about putting, passing them through to the actual image of a table\\r\\n2:42:49\\r\\nCathal Flanagan: through to a multimodal model is another option. The problem is, everything works until it doesn't, and the error rates are far too high at the moment for kind of both use cases.\\r\\n2:42:59\\r\\nRavi Shankar: I see.\\r\\n2:43:12\\r\\nYankai Su: Yeah, thank, you.\\r\\n2:43:14\\r\\nRavi Shankar: It's an inherent limit of it being a language model that it is not able to understand.\\r\\n2:43:16\\r\\nCathal Flanagan: Exactly. Now. There's a lot of money to be made for folks who unlock this, so there's lots of people trying very hard. So I'm sure we'll see. We continue to see progress all the time. So in a year's time it could be a largely solved problem. But right now. It's kind of in the\\r\\n2:43:22\\r\\nCathal Flanagan: kind of it's it's in the let's try it, but probably not going to work. Phase.\\r\\n2:43:39\\r\\nRavi Shankar: Thank you.\\r\\n2:43:46\\r\\nCathal Flanagan: Aj.\\r\\n2:43:48\\r\\nAjay Dawar: Charlie. 1st of all, thank you for staying over so late. Really appreciate it.\\r\\n2:43:50\\r\\nAjay Dawar: So here's what I understood about chunks. I'm connecting dots. Tell me if I connected the right dots. So, as an example\\r\\n2:43:55\\r\\nAjay Dawar: in the Goldman Sachs.\\r\\n2:44:02\\r\\nAjay Dawar: Pdf. That you gave. If I had a corpus of such documents from all kinds of investment banks all over the world.\\r\\n2:44:05\\r\\nAjay Dawar: And I said, and I wanted to have 2 kinds of questions. One question is.\\r\\n2:44:12\\r\\nAjay Dawar: I just want to know what each of these papers is stating about the the chances of a recession. Right? It's just an information retrieval.\\r\\n2:44:16\\r\\nAjay Dawar: In that case, maybe my chunk size is gonna be smaller relatively.\\r\\n2:44:28\\r\\nAjay Dawar: And let's say, my, another another type of question could be root cause questions like, I want to know from all these papers.\\r\\n2:44:33\\r\\nAjay Dawar: Why, why is someone\\r\\n2:44:40\\r\\nAjay Dawar: saying what they're saying about the chance of the session like? Give me a deep explanation. In that case I might relatively have a larger chunk.\\r\\n2:44:43\\r\\nAjay Dawar: Am I getting that right? Is that the chunk size\\r\\n2:44:51\\r\\nAjay Dawar: connect to these kinds of use case.\\r\\n2:44:56\\r\\nCathal Flanagan: So yes, is the answer, but it's all about depth, and like sequential, like the sequential nature of it.\\r\\n2:44:59\\r\\nCathal Flanagan: When you add one to ask, you know, hey? Compare 2 papers. Compare 2 sources, etc.\\r\\n2:45:08\\r\\nCathal Flanagan: You can do that. It just depends on\\r\\n2:45:17\\r\\nCathal Flanagan: how would we think about this.\\r\\n2:45:22\\r\\nCathal Flanagan: Let's say that you had Morgan Stanley and Goldman Sachs.\\r\\n2:45:25\\r\\nCathal Flanagan: Raj, both in the search index, are in the index.\\r\\n2:45:28\\r\\nCathal Flanagan: How would I actually structure this? I would have metadata.\\r\\n2:45:31\\r\\nCathal Flanagan: which has the name of the broker right?\\r\\n2:45:38\\r\\nCathal Flanagan: And I would say, the 1st thing I would say is, I would do 2 searches instead of one, and I would search over\\r\\n2:45:41\\r\\nCathal Flanagan: data that has the filter applied to Goldman Sachs. The filter applied for just Morgan, Stanley and I would search across them for their opinion about recession. Right? And I would bring those back together. Give them both the model, and say, now answer the question\\r\\n2:45:47\\r\\nCathal Flanagan: right? That is kind of. We're guaranteeing success. We're guaranteeing. When I search over Morgan Stanley. It's just Morgan Stanley data. When I search over Goldman Sachs it's just Goldman Sachs data.\\r\\n2:46:01\\r\\nCathal Flanagan: If we were to do it in the way that we talked about it tonight.\\r\\n2:46:12\\r\\nCathal Flanagan: what would I do? I would effectively, when I'm chunking them up. Remember, because each chunk is a paragraph.\\r\\n2:46:15\\r\\nCathal Flanagan: I would actually append to each chunk\\r\\n2:46:22\\r\\nCathal Flanagan: a piece of information at the beginning, which is alter Goldman Sachs alter Morgan Stanley, and then I would have the the chunk within the text of the chunk of the text, because then, when I'm doing the retrieval.\\r\\n2:46:25\\r\\nCathal Flanagan: I would basically say, what is Goldman Sachs saying about us? Gdp, what is Morgan Stanley? Or you know, I would say, find, find what Goldman Sachs and Morgan Stanley are saying about us. Gdp.\\r\\n2:46:39\\r\\nCathal Flanagan: and then let's say I'm going to grab\\r\\n2:46:50\\r\\nCathal Flanagan: 5 chunks to answer that question.\\r\\n2:46:52\\r\\nCathal Flanagan: Well, I'm going to basic guarantee. I'm likely going to find the correct chunks\\r\\n2:46:55\\r\\nCathal Flanagan: that are related to Gdp outlooks.\\r\\n2:47:01\\r\\nCathal Flanagan: I'm within 2 of the 5,\\r\\n2:47:04\\r\\nCathal Flanagan: one will say Morgan Stanley says this, the other will say, go source Goldman, Sachs.\\r\\n2:47:06\\r\\nCathal Flanagan: and the model will be smart enough to understand. That's Goldman's view. That's Morgan's view. And then I'm putting the 2 together, or it's putting the 2 together. So\\r\\n2:47:12\\r\\nCathal Flanagan: you have to think about kind of the workflow. Right? How you want to implement these.\\r\\n2:47:20\\r\\nCathal Flanagan: Some are easier than others. Right?\\r\\n2:47:24\\r\\nCathal Flanagan: Can you have the system like tonight? You got one shot. You had one search that it was able to do right\\r\\n2:47:28\\r\\nCathal Flanagan: in a production system. Can you do it in a way that it has the ability to call generally would search these as Apis, or what are called functions. We're gonna cover those. In a later class.\\r\\n2:47:36\\r\\nCathal Flanagan: we would put these, this is an Api that this model can make a decision that I can call that that back end multiple times to do multiple searches\\r\\n2:47:47\\r\\nCathal Flanagan: or an agent, you know, an agent, because the light agent will basically allow you to go with level deeper, which is okay. Here's how I'm going to make a plan to answer this question. I'm going to go out and execute it. I'm gonna you know, that, like the deeper the question. The more you move into the necessity for going from simple rag to an agentic type of solution.\\r\\n2:47:57\\r\\nCathal Flanagan: Sure thing, chill.\\r\\n2:48:21\\r\\nJoe Seiwert III: It seems it sort of occurs to me that this is kind of like what notebook Lm does. In other words, you load up a bunch of papers, and then you can query across those papers.\\r\\n2:48:23\\r\\nJoe Seiwert III: it seems very similar to me, at least. At the top level.\\r\\n2:48:34\\r\\nJoe Seiwert III: Does that sound right.\\r\\n2:48:39\\r\\nCathal Flanagan: Yeah, it does. We don't actually know how no book Lm is structured under the hood. But like\\r\\n2:48:40\\r\\nCathal Flanagan: and remember, they have the liberty of it's actually closer to the summarization task, because.\\r\\n2:48:45\\r\\nJoe Seiwert III: Got it.\\r\\n2:48:50\\r\\nCathal Flanagan: They? They have 2 elements to it. If you think about notebook. Lm, as a rag system, you, that's how it actually started. You just load up all of these documents. It chunks them up. And you ask questions over just those documents. That was the original notebook. Lm.\\r\\n2:48:51\\r\\nCathal Flanagan: the functionality I showed last week, and the functionality has become famous for is the podcast\\r\\n2:49:05\\r\\nCathal Flanagan: which is effectively just summarization. It's very good summarization to be clear. But it's summarization over a large purpose of documents.\\r\\n2:49:11\\r\\nJoe Seiwert III: Unre. Thank you. Unrelated question, time series data, are we gonna cover that at all?\\r\\n2:49:21\\r\\nJoe Seiwert III: And it's probably I'm guessing something. For like an office hours.\\r\\n2:49:28\\r\\nCathal Flanagan: Sorry. Say that one more time, Jim.\\r\\n2:49:34\\r\\nJoe Seiwert III: Time series, data.\\r\\n2:49:36\\r\\nCathal Flanagan: Oh, man, you're killing me! Everyone pointed, pointing point.\\r\\n2:49:38\\r\\nJoe Seiwert III: Sorry.\\r\\n2:49:43\\r\\nCathal Flanagan: No, no, no, it's good, it's good. It's good point in time. Data for all these data sources is like\\r\\n2:49:44\\r\\nCathal Flanagan: is very challenging and difficult. Are. What are you thinking about? As it relates to time series specifically.\\r\\n2:49:51\\r\\nJoe Seiwert III: A specific application would be looking across you know, say, a production process that has\\r\\n2:49:57\\r\\nJoe Seiwert III: 25 traces on it, you know, 25 different measurements, let's say.\\r\\n2:50:04\\r\\nCathal Flanagan: I'm sorry you want to do something with us.\\r\\n2:50:10\\r\\nJoe Seiwert III: Yeah, look for patterns. And and\\r\\n2:50:13\\r\\nJoe Seiwert III: yeah, look for patterns and compare it to a previous say, production lot. Those kind of things.\\r\\n2:50:17\\r\\nCathal Flanagan: happy to talk about in office hours. It's definitely more relevant as it relates to the agentic frameworks.\\r\\n2:50:27\\r\\nJoe Seiwert III: Okay.\\r\\n2:50:34\\r\\nCathal Flanagan: Big challenge is like what that would require. Joe is\\r\\n2:50:35\\r\\nCathal Flanagan: quantitative analysis, not qualitative analysis. Right?\\r\\n2:50:40\\r\\nCathal Flanagan: It would require having the model writing code and executing that code. That's entire. That is actually possible. Today, the big chat thing that you'll find here.\\r\\n2:50:44\\r\\nCathal Flanagan: Okay, it creates a time series race.\\r\\n2:50:54\\r\\nCathal Flanagan: How does it then interpret it? Right?\\r\\n2:50:58\\r\\nCathal Flanagan: it's I'll show you some examples if you remind me of, like.\\r\\n2:51:01\\r\\nCathal Flanagan: the model is producing distributions that look like Smiley faces, for example, but failing to recognize, it's actually a smiley face that they've created. So it just goes to like the depth of information on how it can, how it can really understand patterns.\\r\\n2:51:05\\r\\nCathal Flanagan: So progress is being made.\\r\\n2:51:20\\r\\nCathal Flanagan: the multimodality aspects of the models being able to pass in images and have the model kind of reflect on those is a big step forward. It's not so. You're right. It's an office hours thing rather than a main class thing. But you know the Tld orbit is going to be\\r\\n2:51:22\\r\\nCathal Flanagan: largely, technically possible, largely disappointing at the moment.\\r\\n2:51:40\\r\\nJoe Seiwert III: Okay, thanks and thanks again for staying so late.\\r\\n2:51:45\\r\\nCathal Flanagan: Good thing.\\r\\n2:51:49\\r\\nCathal Flanagan: And then last one. Richard. I suspect this might be an advertisement for paperclip.\\r\\n2:51:51\\r\\nRichard Ryan: Well, an advertisement, and and also a plea. I.\\r\\n2:51:57\\r\\nRichard Ryan: We had an excellent suggestion on a paper on Temperatures and Transformers from Dima. But Dema is not available on Saturday, and and could present at some other time, but I was hoping that we could. We could stick to our regular Saturday morning meeting, just because if we keep on changing times, it's gonna get impossible to organize. And I\\r\\n2:52:03\\r\\nRichard Ryan: I'm going to give up. So so is there. A paper was on a few minutes ago, and she put something very interesting. Another kind of overview of\\r\\n2:52:26\\r\\nRichard Ryan: chain of thought, reasoning as it applies across Llms. Up in the Paper Club channel. I was hoping that she could perhaps present Anja. Are you still there?\\r\\n2:52:39\\r\\nRichard Ryan: I think she's left us, unfortunately.\\r\\n2:52:53\\r\\nRichard Ryan: Does does anyone else have any paper that they feel comfortable enough with that they could present on\\r\\n2:52:55\\r\\nRichard Ryan: this Saturday\\r\\n2:53:03\\r\\nRichard Ryan: under a silence.\\r\\n2:53:09\\r\\nRichard Ryan: Alright! Well, I will ask the question again in channel and see if we get an answer, and if not, then we'll just we'll. We'll skip a weekend, which you know is is fine. But I I do think we need to stick to regular time and and have a series of paper.\\r\\n2:53:11\\r\\nRichard Ryan: The second question is just that there had been a lot of interest in having a meetup and I took a poll and meet up Channel Bay Area Meetup Channel, and there seemed to be a consensus that the South Bay was better for most of our participants. I think some of our San Francisco crew actually had a kind of little Mini meet up\\r\\n2:53:30\\r\\nRichard Ryan: the day before. So yesterday which is great, and they just kind of made it happen so my suggestion would be patio, 7 to 9 in Palo Alto. If that is, if that works for people, that would be tomorrow night, the 13, th\\r\\n2:53:54\\r\\nRichard Ryan: 79 at Patio Palo alto for bay area media.\\r\\n2:54:14\\r\\nRichard Ryan: Any checkers?\\r\\n2:54:21\\r\\nRichard Ryan: Yes, no.\\r\\n2:54:23\\r\\nJoe Seiwert III: Thursday later would be better. But yeah, I would say, let's do it in the Channel. A.\\r\\n2:54:26\\r\\nJANE: 1st day.\\r\\n2:54:32\\r\\nJeremy McCormick: Yeah, I think only about 20% of the classes here in here right now.\\r\\n2:54:33\\r\\nRichard Ryan: No, I know, I know, and I don't want to take up. I don't want to take up with valuable class time with with practical, so practical housekeeping. But anyway, alright so. But but people are saying, are people. What I'm hearing is that Thursday does seem to work, and tomorrow. 7 to 9 might be good.\\r\\n2:54:38\\r\\nSunil Singhal: Yeah, I think it. It works for me. I can come over here.\\r\\n2:55:02\\r\\nRichard Ryan: Great terrific. All right. So let's go for that, and we'll we'll try and figure out what we'll be doing about paper. But thank you, and thank you, Charlie, for staying so late.\\r\\n2:55:06\\r\\nCathal Flanagan: No problem. All right. Thanks everyone. So we'll see. Some folks might go for drinks tomorrow night. That's fun. We'll see some people I personally will be flying on Saturday, so I won't be able to join Paper Club. I will see some folks for office hours on Sunday, and then, of course we will have class again this time next week, so otherwise we'll see everyone on slack. Keep the conversation going, and support each other, and thanks everyone. Good night.\\r\\n2:55:17\\r\\nRavi Shankar: Wonderful. Thank you.\\r\\n2:55:43\\r\\nRichard Ryan: Thank you. Thank you. Charles.\\r\\n2:55:44\\r\\nCathal Flanagan: Thank you. Charlie. Safe travels.\\r\\n2:55:46\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"openai\", verbose=True)"
      ],
      "metadata": {
        "id": "hSjVU1Kk5h1P"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"Why do we not use LLMs in enterprise?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEj-vBMI5S0Q",
        "outputId": "e5d9d9e4-4f88-4573-ee72-f52acc334d9b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Why do we not use LLMs in enterprise?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"Why do we not use LLMs in enterprise?\"}\n",
            "Got output: LLMs are not extensively used in enterprise because in scenarios where a single retrieval system or model provider is already established and being utilized, the addition of a LLM like Lama Index may not provide significant value. It is often simpler and more efficient to build systems using existing frameworks like Line Chain or other established options for enterprise needs.\n",
            "========================\n",
            "\n",
            "LLMs (Large Language Models) are not extensively used in enterprise because in scenarios where a single retrieval system or model provider is already established and being utilized, the addition of a LLM like Lama Index may not provide significant value. It is often simpler and more efficient to build systems using existing frameworks like Line Chain or other established options for enterprise needs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"What is Charlie's favorite restaurant?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHO1Ys0J5Ezc",
        "outputId": "88463080-a126-4a57-aff3-2a76768a6aeb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What is Charlie's favorite restaurant?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"What is Charlie's favorite restaurant?\"}\n",
            "Got output: Patio Palo Alto.\n",
            "========================\n",
            "\n",
            "Charlie's favorite restaurant is Patio Palo Alto.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"When is coffee chat? when is paper club? when are the office hours?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY1mqzKa5fK3",
        "outputId": "4110ef86-7859-4f3a-fee3-6b92d14c121e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: When is coffee chat? when is paper club? when are the office hours?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"When is coffee chat?\"}\n",
            "Got output: Coffee chat is held on Saturdays or Sundays, depending on the weekend.\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"When is paper club?\"}\n",
            "Got output: Paper club is scheduled for Saturdays at 10 PM Pacific standard time.\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\": \"When are the office hours?\"}\n",
            "Got output: The office hours are scheduled to be 30 minutes before class, with the possibility of extending to an hour for certain topics not covered in the main syllabus.\n",
            "========================\n",
            "\n",
            "- Coffee chat is held on Saturdays or Sundays, depending on the weekend.\n",
            "- Paper club is scheduled for Saturdays at 10 PM Pacific standard time.\n",
            "- The office hours are scheduled to be 30 minutes before class, with the possibility of extending to an hour for certain topics not covered in the main syllabus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"What is the agenda for our next class?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZh-CauI6Ac5",
        "outputId": "8823004f-eb3c-494f-e0d1-0c5cd1ee823f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What is the agenda for our next class?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"What is the agenda for our next class?\"}\n",
            "Got output: The agenda for the next class includes discussing frameworks, advanced civilization, advanced prompt engineering, introducing Ryke, focusing on Rag, talking about open source models with a specific mention of the Gemma model from Google, discussing agents and agentic behavior using the Crew AI framework, fine-tuning models, evaluating benchmarks, and exploring beyond text with multimodal inputs like image generation and video.\n",
            "========================\n",
            "\n",
            "The agenda for our next class includes:\n",
            "- Discussing frameworks\n",
            "- Advanced civilization\n",
            "- Advanced prompt engineering\n",
            "- Introducing Ryke\n",
            "- Focusing on Rag\n",
            "- Talking about open source models with a specific mention of the Gemma model from Google\n",
            "- Discussing agents and agentic behavior using the Crew AI framework\n",
            "- Fine-tuning models\n",
            "- Evaluating benchmarks\n",
            "- Exploring beyond text with multimodal inputs like image generation and video.\n"
          ]
        }
      ]
    }
  ]
}