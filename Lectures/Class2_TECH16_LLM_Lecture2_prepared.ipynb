{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thisisRMak/2025-tech16-LLM/blob/main/Lectures/Class2_TECH16_LLM_Lecture2_prepared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAsj88npPdRu",
        "outputId": "d8497489-55a6-4759-cca4-78fa68be9356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft0dVY1iOLhv"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "client = OpenAI(api_key=open_ai_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X61OO5zwVZg"
      },
      "source": [
        "# Prompt engineering - few shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYJ9ZnsKVsO3"
      },
      "outputs": [],
      "source": [
        "def chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT-3.5 model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT-3.5-turbo model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT-3.5 model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0.3,\n",
        "        # response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"Below are examples of text messages and their classifications. After studying these examples, please classify the new text message at the end.\n",
        "\n",
        "              Example 1:\n",
        "\n",
        "              Text: \"Can you send me the files by tomorrow? It's not urgent, but I'd like to review them.\"\n",
        "              Classification: Non-Urgent\n",
        "              Example 2:\n",
        "\n",
        "              Text: \"Please review the final report ASAP! We need it ready by our meeting in the morning!\"\n",
        "              Classification: Urgent\n",
        "              Example 3:\n",
        "\n",
        "              Text: \"Let's schedule a meeting for next week to discuss the project. No rush.\"\n",
        "              Classification: Non-Urgent\n",
        "              Example 4:\n",
        "\n",
        "              Text: \"URGENT: The server is down and needs immediate attention!\"\n",
        "              Classification: Urgent\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the following message: {message}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qPxpNEraVsRP",
        "outputId": "a9473a1e-c15d-44c4-c7e8-8e2f895c1e98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Classification: Non-Urgent'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "chat(\"Reminder: Tomorrow's team meeting has been postponed. Please update your calendars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHnNNONhZoTh"
      },
      "source": [
        "### JSON mode and Log Probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV1xvaalaOxw"
      },
      "outputs": [],
      "source": [
        "def chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT-3.5 model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT-3.5-turbo model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT-3.5 model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo-preview\",\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        logprobs=True,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\n",
        "            You are a helpful assistant classifations.\n",
        "\n",
        "            Below are examples of text messages and their classifications. After studying these examples, please classify the new text message at the end.\n",
        "\n",
        "              Example 1:\n",
        "\n",
        "              Text: \"Can you send me the files by tomorrow? It's not urgent, but I'd like to review them.\"\n",
        "              Classification: Non-Urgent\n",
        "              Example 2:\n",
        "\n",
        "              Text: \"Please review the final report ASAP! We need it ready by our meeting in the morning!\"\n",
        "              Classification: Urgent\n",
        "              Example 3:\n",
        "\n",
        "              Text: \"Let's schedule a meeting for next week to discuss the project. No rush.\"\n",
        "              Classification: Non-Urgent\n",
        "              Example 4:\n",
        "\n",
        "              Text: \"URGENT: The server is down and needs immediate attention!\"\n",
        "              Classification: Urgent\n",
        "\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the following message as Non-Urgent or Urgent and return with probability as JSON: {message}\"}\n",
        "        ]\n",
        "    )\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKx6kntUZtT0"
      },
      "outputs": [],
      "source": [
        "response = chat(\"Reminder: Tomorrow's team meeting has been postponed. Please update your calendars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlgy42XIdEjb",
        "outputId": "f4b178e1-54b2-40aa-b16a-ca252fbb175a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Classification\": \"Non-Urgent\",\n",
            "  \"Probability\": 0.95\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmzn4ktYRty4"
      },
      "source": [
        "# Chain of thought reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJR8Sy2vRzcS"
      },
      "outputs": [],
      "source": [
        "def chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo-preview\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\n",
        "            You are a helpful assistant classifations.\n",
        "\n",
        "            Below are examples of questions and how to calculate the answer\n",
        "\n",
        "              Example 1: Arithmetic Problem\n",
        "              Prompt: \"If a shirt costs $20 and there is a 10% discount, how much does the shirt cost after the discount?\"\n",
        "              Chain of Thought Answer:\n",
        "                Calculate the amount of discount: 10% of $20 is $2.\n",
        "                Subtract the discount from the original price: $20 - $2 = $18.\n",
        "                The shirt costs $18 after the discount.\n",
        "\n",
        "              Example 2: Logic Puzzle\n",
        "              Prompt: \"There are four apples and you take away three. How many apples do you have?\"\n",
        "              Chain of Thought Answer:\n",
        "                You start with four apples.\n",
        "                You take away three apples.\n",
        "                After taking three, you now have those three apples.\n",
        "                You have 3 apples\n",
        "\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Answer the following question: {message}\"}\n",
        "        ]\n",
        "    )\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC9PUbrjSZbZ",
        "outputId": "27af3fc5-345f-49cb-e72c-45b314d77c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find out how much CO2 10 trees will absorb in a year, you multiply the amount one tree absorbs by the number of trees.\n",
            "\n",
            "- Each tree absorbs 48 pounds of CO2 a year.\n",
            "- Number of trees: 10.\n",
            "\n",
            "Calculation: 48 pounds/tree × 10 trees = 480 pounds.\n",
            "\n",
            "The 10 trees will absorb 480 pounds of CO2 in a year.\n"
          ]
        }
      ],
      "source": [
        "print(chat(\"If a tree absorbs 48 pounds of CO2 a year, how much CO2 will 10 trees absorb in a year?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6ti4VrlvFWj"
      },
      "source": [
        "# Langchain & summarizing PDF's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE5ZaG8FvOeR",
        "outputId": "ae494b1c-1cfd-4fd5-fdbe-cf77dd3274c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-06 23:49:52--  https://arxiv.org/pdf/2401.16212.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.195.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2401.16212 [following]\n",
            "--2025-02-06 23:49:53--  http://arxiv.org/pdf/2401.16212\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 512852 (501K) [application/pdf]\n",
            "Saving to: ‘2401.16212.pdf’\n",
            "\n",
            "2401.16212.pdf      100%[===================>] 500.83K  1.66MB/s    in 0.3s    \n",
            "\n",
            "2025-02-06 23:49:53 (1.66 MB/s) - ‘2401.16212.pdf’ saved [512852/512852]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get a PDF\n",
        "!wget https://arxiv.org/pdf/2401.16212.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54LWtNYvvOl5",
        "outputId": "861e0be5-ccc3-48cd-ce3c-6747048688d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2401.16212.pdf\tsample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg3xOtPjDwud",
        "outputId": "5d5e27cf-e9ef-42f4-f4a6-533383572f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-06 23:49:54--  https://www.morganstanley.com/im/publication/insights/articles/article_increasingreturns.pdf\n",
            "Resolving www.morganstanley.com (www.morganstanley.com)... 23.220.224.225\n",
            "Connecting to www.morganstanley.com (www.morganstanley.com)|23.220.224.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/pdf]\n",
            "Saving to: ‘article_increasingreturns.pdf’\n",
            "\n",
            "article_increasingr     [ <=>                ] 564.52K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-02-06 23:49:54 (5.96 MB/s) - ‘article_increasingreturns.pdf’ saved [578072]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.morganstanley.com/im/publication/insights/articles/article_increasingreturns.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO0OSBa0QvLL",
        "outputId": "fd806c88-baa2-4743-b953-cb681e42843f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.16 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.17)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting langchain-core<0.4.0,>=0.3.32 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.34-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.4-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.34-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pydantic-settings, dataclasses-json, langchain-core, langchain-openai, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.33\n",
            "    Uninstalling langchain-core-0.3.33:\n",
            "      Successfully uninstalled langchain-core-0.3.33\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.16 langchain-core-0.3.34 langchain-openai-0.3.4 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 pypdf-5.2.0 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community pypdf langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34HMl-vPvUlM"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader # Update import statement\n",
        "\n",
        "loader = PyPDFLoader(\"2401.16212.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmGWqFLIvkqV",
        "outputId": "4f7fe3d1-c602-44af-cec8-ac0e28fc895d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '2401.16212.pdf', 'page': 0, 'page_label': '1'}, page_content='Better Call GPT, Comparing Large Language Models Against Lawyers\\nLAUREN MARTIN, NICK WHITEHOUSE, STEPHANIE YIU, LIZZIE CATTERSON, RIVINDU\\nPERERA, AI Center of Excellence, Onit Inc., New Zealand\\nThis paper presents a groundbreaking comparison between Large Language Models (LLMs) and traditional legal contract review-\\ners—Junior Lawyers and Legal Process Outsourcers (LPOs). We dissect whether LLMs can outperform humans in accuracy, speed,\\nand cost-efficiency during contract review. Our empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers,\\nuncovering that advanced models match or exceed human accuracy in determining legal issues. In speed, LLMs complete reviews in\\nmere seconds, eclipsing the hours required by their human counterparts. Cost-wise, LLMs operate at a fraction of the price, offering a\\nstaggering 99.97 percent reduction in cost over traditional methods. These results are not just statistics—they signal a seismic shift in\\nlegal practice. LLMs stand poised to disrupt the legal industry, enhancing accessibility and efficiency of legal services. Our research\\nasserts that the era of LLM dominance in legal contract review is upon us, challenging the status quo and calling for a reimagined\\nfuture of legal workflows.\\nCCS Concepts: • Computing methodologies→Natural language generation; Information extraction; • Applied computing\\n→Law; Document searching .\\nAdditional Key Words and Phrases: Generative AI, Large Language Models, LegalAI, NLP\\nReference Format:\\nLauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera. 2024. Better Call GPT, Comparing Large Language\\nModels Against Lawyers. 1, 1 (January 2024), 16 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nThe integration of Artificial Intelligence (AI) into the legal sector has opened a new frontier in legal services. However, as\\nof the current state of research, there appears to be a significant gap in exploratory and experimental studies specifically\\naddressing the capabilities of Generative AI and Large Language Models (LLMs) in the context of determination and\\ndiscovery of legal issues. Such studies would be instrumental in understanding how these advanced AI technologies\\nmanage the intricate task of accurately classifying and pinpointing legal matters, a domain traditionally reliant on the\\ndeep, contextual, and specialised knowledge of human legal experts.\\nTo address the identified gap in the research landscape, this study proposes an experimental and exploratory analysis\\nof the performance of LLMs in the legal domain. The research aims to evaluate the capabilities of LLMs contrasting\\ntheir performance against human legal practitioners on high volume real-world legal tasks. These types of high volume\\nlegal tasks are frequently outsourced or pushed to less experienced lawyers, and given the rapid advancements made\\nby LLMs, raises the question of whether LLMs have achieved a level of legal comprehension that is comparable to the\\nquality, accuracy and efficiency of Junior Lawyers or outsourced legal practitioners on such tasks.\\nAuthor’s address: Lauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera, AI Center of Excellence, Onit Inc., Auckland, New\\nZealand, lauren.martin@onit.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\\nof this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on\\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from contact@onit.com.\\n© 2023 Copyright held by the owner/author(s). Publication rights licensed to Onit Inc..\\n1\\narXiv:2401.16212v1  [cs.CY]  24 Jan 2024')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krxv5sE1vij9",
        "outputId": "dddf7175-5dfb-4b45-af9b-a6127f7eb157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_documents': [Document(metadata={'source': '2401.16212.pdf', 'page': 0, 'page_label': '1'}, page_content='Better Call GPT, Comparing Large Language Models Against Lawyers\\nLAUREN MARTIN, NICK WHITEHOUSE, STEPHANIE YIU, LIZZIE CATTERSON, RIVINDU\\nPERERA, AI Center of Excellence, Onit Inc., New Zealand\\nThis paper presents a groundbreaking comparison between Large Language Models (LLMs) and traditional legal contract review-\\ners—Junior Lawyers and Legal Process Outsourcers (LPOs). We dissect whether LLMs can outperform humans in accuracy, speed,\\nand cost-efficiency during contract review. Our empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers,\\nuncovering that advanced models match or exceed human accuracy in determining legal issues. In speed, LLMs complete reviews in\\nmere seconds, eclipsing the hours required by their human counterparts. Cost-wise, LLMs operate at a fraction of the price, offering a\\nstaggering 99.97 percent reduction in cost over traditional methods. These results are not just statistics—they signal a seismic shift in\\nlegal practice. LLMs stand poised to disrupt the legal industry, enhancing accessibility and efficiency of legal services. Our research\\nasserts that the era of LLM dominance in legal contract review is upon us, challenging the status quo and calling for a reimagined\\nfuture of legal workflows.\\nCCS Concepts: • Computing methodologies→Natural language generation; Information extraction; • Applied computing\\n→Law; Document searching .\\nAdditional Key Words and Phrases: Generative AI, Large Language Models, LegalAI, NLP\\nReference Format:\\nLauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera. 2024. Better Call GPT, Comparing Large Language\\nModels Against Lawyers. 1, 1 (January 2024), 16 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nThe integration of Artificial Intelligence (AI) into the legal sector has opened a new frontier in legal services. However, as\\nof the current state of research, there appears to be a significant gap in exploratory and experimental studies specifically\\naddressing the capabilities of Generative AI and Large Language Models (LLMs) in the context of determination and\\ndiscovery of legal issues. Such studies would be instrumental in understanding how these advanced AI technologies\\nmanage the intricate task of accurately classifying and pinpointing legal matters, a domain traditionally reliant on the\\ndeep, contextual, and specialised knowledge of human legal experts.\\nTo address the identified gap in the research landscape, this study proposes an experimental and exploratory analysis\\nof the performance of LLMs in the legal domain. The research aims to evaluate the capabilities of LLMs contrasting\\ntheir performance against human legal practitioners on high volume real-world legal tasks. These types of high volume\\nlegal tasks are frequently outsourced or pushed to less experienced lawyers, and given the rapid advancements made\\nby LLMs, raises the question of whether LLMs have achieved a level of legal comprehension that is comparable to the\\nquality, accuracy and efficiency of Junior Lawyers or outsourced legal practitioners on such tasks.\\nAuthor’s address: Lauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera, AI Center of Excellence, Onit Inc., Auckland, New\\nZealand, lauren.martin@onit.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\\nof this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on\\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from contact@onit.com.\\n© 2023 Copyright held by the owner/author(s). Publication rights licensed to Onit Inc..\\n1\\narXiv:2401.16212v1  [cs.CY]  24 Jan 2024'),\n",
              "  Document(metadata={'source': '2401.16212.pdf', 'page': 1, 'page_label': '2'}, page_content='2 Onit AI Center of Excellence\\nSpecifically, we focus on three primary research questions:\\nDo LLMs outperform Junior Lawyers and Legal Process Outsourcers in determination and location of\\nlegal issues in contracts?\\nThis question aims to assess the precision and recall of LLMs compared to human professionals in determining and\\nlocating legal issues.\\nCan LLMs review contracts faster than Junior Lawyers and Legal Process Outsourcers?\\nIn this question, our focus is on evaluating the efficiency (as measured on a temporal perspective) of LLMs in\\nprocessing and responding to legal queries compared to the time taken by human lawyers.\\nCan LLMs review contracts cheaper than Junior Lawyers and Legal Process Outsourcers?\\nThis question focuses on evaluating the comparative costs between Lawyers and LPOs to determine whether LLMs\\nare more cost effective.\\nThrough this research, we aim to contribute a comprehensive understanding of the potential capabilities and\\nlimitations of LLMs in the legal domain, providing valuable insights for Legal and AI practitioners.\\n2 RELATED WORK\\nThe related work in this area encompasses a broad spectrum of technological advancements in generative AI applied on\\nlegal contract reviews and generation of legal contracts. In this section we will be exploring direct and also indirect\\nadvancements towards application of Natural Language Understanding (NLU) and Natural Language Generation (NLG)\\nin legal domain giving the priority to generative AI approaches.\\nGuha et al. [6] introduce the LegalBench which is a collaborative benchmark for measuring legal reasoning in LLMs.\\nTheir work focuses on establishing a benchmark for evaluating the capabilities of LLMs in legal reasoning. By employing\\nthe Issue, Rule, Application, Conclusion framework (IRAC), which is a widely used method for organising legal analysis,\\nthe authors have initiated a collaborative project aimed at developing a comprehensive and open-source benchmark.\\nThis endeavour emphasises the potential for computational tools to enhance legal practices, especially in administrative\\nand transactional settings. However, the primary objective of LegalBench is not to replace legal professionals with\\ncomputational systems, but to assess the extent to which current models can support and augment legal reasoning.\\nOur research, in contrast, delves into a more specialised commercial domain within legal technology, focusing on\\nthe application of LLMs in determining and locating legal issues in contracts. This specific focus on contract analysis\\nsets our work apart from the broader legal reasoning perspective adopted in LegalBench. While Guha et al. emphasise\\nthe need for collaborative efforts and in developing benchmarks for legal reasoning with open source data, our study\\nprovides a detailed and practical examination of the capabilities of LLMs compared directly with legal practitioners,\\nincluding Junior Lawyers and LPOs. This aspect of direct comparison is particularly significant as it sheds light on the\\npractical effectiveness and efficiency of LLMs in real-world legal tasks, a perspective not deeply explored in LegalBench.\\nThe significance of our work is further accentuated by its emphasis on the efficiency of LLMs in contract review, a\\ncrucial aspect in legal practice where time is a critical factor. Our study not only benchmarks the performance of LLMs\\nagainst human practitioners but also evaluates their efficiency in terms of time and cost, offering valuable insights into\\nthe practical application of these models in legal settings. This focus on both performance and efficiency provides a\\npragmatic perspective that has an immediate applicability in legal tasks such as contract review, which are common\\nand often time intensive.\\nIn conclusion, while LegalBench establishes a foundational framework for evaluating LLMs in legal reasoning, our\\nresearch offers a more focused and application-oriented analysis within the legal field. By comparing the capabilities'),\n",
              "  Document(metadata={'source': '2401.16212.pdf', 'page': 2, 'page_label': '3'}, page_content='Better Call GPT, Comparing Large Language Models Against Lawyers 3\\nand efficiency of LLMs with those of human legal practitioners in the specific context of contract review, our study\\naddresses a critical gap in the current research landscape. This approach not only benchmarks the effectiveness of\\nLLMs in a practical legal task but also provides substantial contributions to the evolving field of legal technology and\\nits applications.\\nTan et al. [9] bring a more practical dimension to the theory oriented approach we noticed in the previous research.\\nAlthough this study does not focus on a deep statistical analysis, it provides some important aspect of practical usage of\\nexisting tools. A key aspect of this research, the comparative analysis between ChatGPT and JusticeBot, provides a\\nnuanced understanding of the strengths and limitations of AI-driven legal tools. The study acknowledges that while\\nChatGPT may not always provide perfectly accurate or reliable information, it offers a powerful and intuitive interface\\nfor general public, a significant consideration in enhancing access to legal information.\\nHowever, Tan et al. also identify potential areas for improvement, particularly in the reliability and depth of the legal\\ninformation provided by ChatGPT. In the legal field, where incorrect information can have serious consequences, these\\naspects are of utmost importance. The research suggests the need for continuous improvement and testing of ChatGPT,\\nespecially in the terms of keeping the model updated with the latest legal development and training it on diverse legal\\nscenarios. Although, Open AI claims that ChatGPT currently performs in the level of a legal graduate, there is no\\nevidence in its performance in various legal scenarios. This is one of the key aspect which we tried to address in our\\nresearch.\\nCallister [3] delves into the philosophical and cognitive implications of AI in law, addressing concerns about the\\nreliability and consistency of AI, and highlighting potential issues such as information hallucination. It also emphasises\\nthe transformative nature of AI in shifting the traditional paradigms of legal research.\\nWhile Callister [3] focus on the philosophical aspect of AI and LLM’s ability to involve in the legal decision making,\\nwe tried to offer a viewpoint of practical capabilities and efficiency of LLM’s in specific legal tasks. We, however, agree\\nwith the cautionary perspective on the broader implications of generative AI for legal epistemology brought to light by\\nCallister [3] which needs to be accounted when using generative AI widely in legal decision making.\\nChoi et al. [4] bring a new dimension to the LLMs applicability in legal domain by focusing on ChatGPT’s applicability\\nin legal education. Although this research does not directly align with our study’s interest in AI’s application in the\\nlegal domain, it brings some new perspectives on how LLMs perform in the legal domain. One of the key findings in\\nthis study is the analysis of limited ability of ChaptGPT to perform complex legal analysis and issue spotting which we\\nmainly focused on our research. Our research also extends this dialogue by comparing LLMs’ performance not just\\nto academic standards (as noticed in [6]), but also against real world legal practitioners, particularly Junior Lawyers\\nand LPO professionals. This comparison offers a novel perspective on the utility of LLMs in practical legal work, a\\ndimension not fully explored in any of the aforementioned research attempts.\\n3 METHODOLOGY\\nThis section focuses on the research methodology covering how data was collected, analysed, prepared, and how the\\nmodels were selected to be included as part of this research.\\n3.1 Overall Design\\nThe experiment was designed to compare Junior Lawyers and LPOs against LLMs using Senior Lawyers as a benchmark,\\nestablishing ground truth for determining and locating legal issues in contracts. This replicates the process lawyers')],\n",
              " 'output_text': 'This paper, titled \"Better Call GPT, Comparing Large Language Models Against Lawyers\" by Lauren Martin et al., from Onit Inc., New Zealand, explores the effectiveness of Large Language Models (LLMs) in legal contract review tasks, comparing their performance with Junior Lawyers and Legal Process Outsourcers (LPOs). The study focuses on accuracy, speed, and cost-efficiency, finding that LLMs can match or exceed human accuracy in identifying legal issues, complete reviews significantly faster, and operate at a substantially lower cost, offering a 99.97% reduction compared to traditional methods. The research indicates a potential shift in legal practice towards LLM dominance, suggesting a future where legal workflows are significantly transformed by AI technology. The study contributes to the understanding of LLM capabilities in the legal domain, highlighting their potential to enhance the accessibility and efficiency of legal services.'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4-turbo-preview\", api_key=open_ai_key)\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "res = chain.invoke(pages[0:3])\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yct4RIQ_R6UA",
        "outputId": "dad077fe-4808-480b-d11b-a409e4988a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This paper, titled \"Better Call GPT, Comparing Large Language Models Against Lawyers\" by Lauren Martin et al., from Onit Inc., New Zealand, explores the effectiveness of Large Language Models (LLMs) in legal contract review tasks, comparing their performance with Junior Lawyers and Legal Process Outsourcers (LPOs). The study focuses on accuracy, speed, and cost-efficiency, finding that LLMs can match or exceed human accuracy in identifying legal issues, complete reviews significantly faster, and operate at a substantially lower cost, offering a 99.97% reduction compared to traditional methods. The research indicates a potential shift in legal practice towards LLM dominance, suggesting a future where legal workflows are significantly transformed by AI technology. The study contributes to the understanding of LLM capabilities in the legal domain, highlighting their potential to enhance the accessibility and efficiency of legal services.\n"
          ]
        }
      ],
      "source": [
        "print(res[\"output_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csayOsg-wG6l"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define prompt\n",
        "prompt_template = \"\"\"Write a concise summary in a maximum of 3 bullets of the following text enclosed within three backticks:\n",
        "```{text}```\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Define LLM chain\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo-preview\", api_key=open_ai_key)\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Define StuffDocumentsChain\n",
        "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
        "\n",
        "res = stuff_chain.invoke(pages[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KEMRXIgRrqh",
        "outputId": "9351ba81-cd59-462a-8f46-dee0521147fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The paper evaluates the performance of Large Language Models (LLMs) against Junior Lawyers and Legal Process Outsourcers (LPOs) in legal contract review, focusing on accuracy, speed, and cost-efficiency, with LLMs showing superior results in all areas.\n",
            "- Empirical analysis reveals LLMs match or exceed human accuracy in identifying legal issues, complete reviews significantly faster, and operate at a 99.97% lower cost than traditional methods.\n",
            "- The findings suggest LLMs have the potential to disrupt the legal industry by enhancing the accessibility and efficiency of legal services, indicating a shift towards LLM dominance in legal contract review.\n"
          ]
        }
      ],
      "source": [
        "print(res[\"output_text\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}